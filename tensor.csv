issue_title,issue_body,created_at,issue_labels
Do we have any way to avoid memory copy in tf.convert_to_tensor ?,"**System information**
- TensorFlow version (use command below): 2.4.1

**Describe the current behavior**

Code:

```python
@tf.function
def inference(model, image):
    return model(image)


def test_model():
    model = tf.keras.Sequential([tf.keras.layers.Conv2D(filters=3, kernel_size=3)])

    np_image = np.random.randn(256, 256, 256, 3)
    model(np_image)
    # passing a python object to tf.function is not a recommend way, so this is slow.
    inference(model, np_image)

    tf_image = tf.convert_to_tensor(np_image)
    inference(model, tf_image)

    total_time = 0
    for _ in range(10):
        np_image = np.random.randn(256, 256, 256, 3)
        start_time = time.time()
        model(np_image)
        total_time += time.time() - start_time
    print(""inference by model: {0} s"".format(total_time))

    total_time = 0
    for _ in range(10):
        np_image = np.random.randn(256, 256, 256, 3)
        start_time = time.time()
        inference(model, np_image)
        total_time += time.time() - start_time
    print(""inference by np_image: {0} s"".format(total_time))

    total_time = 0
    for _ in range(10):
        np_image = np.random.randn(256, 256, 256, 3)
        tf_image = tf.convert_to_tensor(np_image)
        start_time = time.time()
        inference(model, tf_image)
        total_time += time.time() - start_time
    print(""inference by tf_image: {0} s"".format(total_time))
```

The result:

```txt
inference by model: 2.4521803855895996 s
inference by np_image: 2.472818374633789 s
inference by tf_image: 0.007699012756347656 s, convert time: 2.4546449184417725 s
```

My question is:

1. What does  `tf.convert_to_tensor` do ? Does it copy cpu data from numpy to tf.Tensor and also copy it to GPU ?
2. How can I avoid memory copy when using tf.convert_to_tensor ? In my case I will not modify the input image at all, so it's safe for me to make a memory view for the numpy data to tf.Tensor.
I can restrict the numpy data to be continuous and C order if it is needed.

From official docs I can find that it's recommended to create data from tf.data.Dataset which is already a tf.Tensor. But in my case we cannot use tf.data.Dataset API because I want to use some data which is mutable when training.

The alternative is to produce tf.Tensor by multiprocessing and push it to main process for training, but it will dump and reload the tf.Tensor object when using multiprocessing.Queue.
And I cannot run `tf.queue.FIFOQueue` in multiprocessing. Is this a good way to sharing tensor between different processes? (For numpy, we can use memmap to share numpy data.)

**Describe the expected behavior**

My expected behavior is that the convert time is very small.
",2022-01-06 09:17:23+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.4"
Higher validation and test loss and lower accuracy using tf.data.dataset with tfrecords than using numpy,"**System information**
- OS Platform and Distribution : Kaggle and Colab gpu/tpu
- TensorFlow version (use command below): 2.4 on Kaggle, 2.7 on Colab
- GPU model and memory:tesla p100, colab tpu, kaggle tpu

Hi, I'm training and testing an LSTM network using kaggle notebooks. 
To improve time performance I have converted my numpy dataset to TFRecords, but the results are very differents respects results obtained with numpy dataset. I noticed this behavior either on GPU and TPU on Kaggle and Google Colab. 
I want to specify that the hyperparametrs and the dataset order is the same with numpy method and td.data.
I repeated the tests on differents file of the dataset many times but each time the validation and test accuracy are lower  and validation and test loss are higher using tf.data rsepect to using numpy.

With tf.data:
![accuracy](https://user-images.githubusercontent.com/48318112/148437400-86932751-279e-4ae3-96de-2a9c7c3589e3.png)
![loss](https://user-images.githubusercontent.com/48318112/148437401-a0529b6b-71f7-4d56-8604-86914565cc2f.png)

With numpy dataset:
![accuracy](https://user-images.githubusercontent.com/48318112/148437518-2eb912f8-5f37-496a-a0e6-a393b9b64d2a.png)
![loss](https://user-images.githubusercontent.com/48318112/148437522-f79ecc38-6681-42df-91ab-57fb00c0afdc.png)

Execution with tf.data:
```
Epoch 1/18
11929/11929 [==============================] - 323s 26ms/step - loss: 0.1282 - binary_accuracy: 0.9558 - auc: 0.8314 - precision: 0.7457 - recall: 0.6855 - val_loss: 1.6054 - val_binary_accuracy: 0.6283 - val_auc: 0.6912 - val_precision: 0.9549 - val_recall: 0.0453
2022-01-06 15:14:59.141498: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 146116, Output num: 3
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1641482099.138121196"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 146116, Output num: 3"",""grpc_status"":3}
Epoch 2/18
11929/11929 [==============================] - 303s 25ms/step - loss: 0.0450 - binary_accuracy: 0.9862 - auc: 0.8506 - precision: 0.8013 - recall: 0.7681 - val_loss: 1.4894 - val_binary_accuracy: 0.7084 - val_auc: 0.8023 - val_precision: 0.9523 - val_recall: 0.2626
Epoch 3/18
11929/11929 [==============================] - 307s 26ms/step - loss: 0.0294 - binary_accuracy: 0.9915 - auc: 0.8516 - precision: 0.8214 - recall: 0.8078 - val_loss: 1.4176 - val_binary_accuracy: 0.7219 - val_auc: 0.8154 - val_precision: 0.9737 - val_recall: 0.2922
Epoch 4/18
11929/11929 [==============================] - 308s 26ms/step - loss: 0.0186 - binary_accuracy: 0.9947 - auc: 0.8534 - precision: 0.8326 - recall: 0.8298 - val_loss: 1.2668 - val_binary_accuracy: 0.7506 - val_auc: 0.8195 - val_precision: 0.9890 - val_recall: 0.3619
Epoch 5/18
11929/11929 [==============================] - 306s 26ms/step - loss: 0.0142 - binary_accuracy: 0.9961 - auc: 0.8542 - precision: 0.8387 - recall: 0.8378 - val_loss: 1.2077 - val_binary_accuracy: 0.7710 - val_auc: 0.8313 - val_precision: 0.9954 - val_recall: 0.4125
Epoch 6/18
11929/11929 [==============================] - 304s 25ms/step - loss: 0.0126 - binary_accuracy: 0.9965 - auc: 0.8548 - precision: 0.8432 - recall: 0.8397 - val_loss: 1.1289 - val_binary_accuracy: 0.7836 - val_auc: 0.8471 - val_precision: 0.9974 - val_recall: 0.4442
Epoch 7/18
11929/11929 [==============================] - 303s 25ms/step - loss: 0.0094 - binary_accuracy: 0.9975 - auc: 0.8555 - precision: 0.8470 - recall: 0.8455 - val_loss: 0.9592 - val_binary_accuracy: 0.8196 - val_auc: 0.8885 - val_precision: 0.9983 - val_recall: 0.5365
Epoch 8/18
11929/11929 [==============================] - 304s 26ms/step - loss: 0.0085 - binary_accuracy: 0.9978 - auc: 0.8559 - precision: 0.8478 - recall: 0.8463 - val_loss: 0.7648 - val_binary_accuracy: 0.8320 - val_auc: 0.9281 - val_precision: 0.9983 - val_recall: 0.5686
Epoch 9/18
11929/11929 [==============================] - 305s 26ms/step - loss: 0.0070 - binary_accuracy: 0.9982 - auc: 0.8561 - precision: 0.8498 - recall: 0.8481 - val_loss: 0.6531 - val_binary_accuracy: 0.8376 - val_auc: 0.9435 - val_precision: 0.9986 - val_recall: 0.5828
Epoch 10/18
11929/11929 [==============================] - 304s 25ms/step - loss: 0.0062 - binary_accuracy: 0.9984 - auc: 0.8565 - precision: 0.8506 - recall: 0.8491 - val_loss: 0.6175 - val_binary_accuracy: 0.8421 - val_auc: 0.9462 - val_precision: 0.9987 - val_recall: 0.5943
Epoch 11/18
11929/11929 [==============================] - 307s 26ms/step - loss: 0.0048 - binary_accuracy: 0.9987 - auc: 0.8567 - precision: 0.8514 - recall: 0.8501 - val_loss: 0.5576 - val_binary_accuracy: 0.8626 - val_auc: 0.9529 - val_precision: 0.9990 - val_recall: 0.6470
Epoch 12/18
11929/11929 [==============================] - 309s 26ms/step - loss: 0.0044 - binary_accuracy: 0.9988 - auc: 0.8568 - precision: 0.8520 - recall: 0.8514 - val_loss: 0.4787 - val_binary_accuracy: 0.8744 - val_auc: 0.9641 - val_precision: 0.9992 - val_recall: 0.6773
Epoch 13/18
11929/11929 [==============================] - 308s 26ms/step - loss: 0.0040 - binary_accuracy: 0.9989 - auc: 0.8568 - precision: 0.8526 - recall: 0.8521 - val_loss: 0.4774 - val_binary_accuracy: 0.8818 - val_auc: 0.9635 - val_precision: 0.9991 - val_recall: 0.6965
Epoch 14/18
11929/11929 [==============================] - 306s 26ms/step - loss: 0.0036 - binary_accuracy: 0.9990 - auc: 0.8568 - precision: 0.8528 - recall: 0.8522 - val_loss: 0.4594 - val_binary_accuracy: 0.8837 - val_auc: 0.9663 - val_precision: 0.9992 - val_recall: 0.7011
Epoch 15/18
11929/11929 [==============================] - 307s 26ms/step - loss: 0.0035 - binary_accuracy: 0.9990 - auc: 0.8568 - precision: 0.8529 - recall: 0.8517 - val_loss: 0.4588 - val_binary_accuracy: 0.8946 - val_auc: 0.9615 - val_precision: 0.9992 - val_recall: 0.7294
Epoch 16/18
11929/11929 [==============================] - 305s 26ms/step - loss: 0.0031 - binary_accuracy: 0.9991 - auc: 0.8568 - precision: 0.8534 - recall: 0.8529 - val_loss: 0.3980 - val_binary_accuracy: 0.9048 - val_auc: 0.9642 - val_precision: 0.9994 - val_recall: 0.7553
Epoch 17/18
11929/11929 [==============================] - 304s 26ms/step - loss: 0.0030 - binary_accuracy: 0.9991 - auc: 0.8569 - precision: 0.8532 - recall: 0.8528 - val_loss: 0.3670 - val_binary_accuracy: 0.9098 - val_auc: 0.9685 - val_precision: 0.9993 - val_recall: 0.7683
Epoch 18/18
11929/11929 [==============================] - 306s 26ms/step - loss: 0.0028 - binary_accuracy: 0.9992 - auc: 0.8569 - precision: 0.8533 - recall: 0.8526 - val_loss: 0.3948 - val_binary_accuracy: 0.9091 - val_auc: 0.9655 - val_precision: 0.9992 - val_recall: 0.7667
3497/3497 [==============================] - 83s 23ms/step - loss: 0.4007 - binary_accuracy: 0.9079 - auc: 0.9650 - precision: 0.9993 - recall: 0.7645
2022-01-06 16:43:10.007313: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1641487390.006754880"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3"",""grpc_status"":3}
```
Only with Kaggle TPU i have the following error as you can see just after first epoch validation and during testing, but the training run correctly and the program don't crash:

```
W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1641487390.006754880"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3"",""grpc_status"":3}
```
This problem occour also without this error on Kaggle GPU and Colab, so this error is specific on platform TPU Kaggle and is not related with the problem.

In the following link you can find my code with tfrecord write and read and all the process to get the dataset from tfrecord, model creation, fitting and testing:
[link code](https://colab.research.google.com/drive/1goQZL3xRU2vkBos04PZkWL2o1qHv8qul?usp=sharing)

Seems like to with tf.data the training start with lower validation accuracy value and increase more slowly compared to numpy dataset, so with a lot of epochs maybe also with tf.data can reach numpy accuracy level, is this comportament normal using tf.data or not?
",2022-01-06 19:34:18+00:00,"stat:awaiting response, type:bug, stale, comp:keras, TF 2.7"
Why is tensorflow2.6 so much slower than pytorch1.10.0+cu113?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow installed from binary:
- TensorFlow version --2.6:
- Python 3.8:
- GPU model R3090 and memory 24G:

tensorflow 2.6 code：
```
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras import preprocessing
from tensorflow.keras.applications.vgg19 import VGG19

print(tf.__version__)

model = VGG19(weights='imagenet')
img = preprocessing.image.load_img('008.jpg', target_size=(224, 224))
x = preprocessing.image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x /= 255.
for _ in range(5):
    t1 = time.time()
    preds = model.predict(x)
    print(preds.shape)
    t2 = time.time()
    print(t2 - t1)
```

output:
```
2.6.0
(1, 1000)
2.8845417499542236
(1, 1000)
0.044057369232177734
(1, 1000)
0.027636051177978516
(1, 1000)
0.04877758026123047
(1, 1000)
0.027869701385498047
```

pytorch code:
```
import torch
import torchvision
import time
import thop

print(torch.__version__)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

model = torchvision.models.vgg19(pretrained=False)
model.eval().to(device)
inputs = torch.rand((1, 3, 224, 224)).to(device)
with torch.no_grad():
    for _ in range(5):
        t1 = time.time()
        outputs = model(inputs)
        print(outputs.shape)
        t2 = time.time()
        print(t2 - t1)
```

output:
```
1.10.0+cu113
torch.Size([1, 1000])
0.0020911693572998047
torch.Size([1, 1000])
0.0019617080688476562
torch.Size([1, 1000])
0.001947164535522461
torch.Size([1, 1000])
0.001924753189086914
torch.Size([1, 1000])
0.0019559860229492188
```
Is this my problem or the tensorflow problem?",2022-01-08 06:54:08+00:00,"comp:apis, type:performance, 2.6.0"
why does tensorflow use multi threads to launch gpu kernels?,"I noticed that when I launch kernel to the same GPU with multi threads, cudaLaunchKernel  api will get much slower. 
But seems it's the case in tensorflow. why not use a single thread to communicate with GPU?

Here is my test，when thread_num is larger, cuda api cost will be huge.
![image](https://user-images.githubusercontent.com/26128514/149446536-06d6117a-d348-4b0d-a2a8-8ebf81299c5a.png)

`#include <thread>
#include <vector>

using namespace std;

bool running = true;
const int thread_num = 20;
const int launch_num = 25;

__global__ void task_kernel(int x) {
    int ans = 1024;
    for(int i = 0; i < x; i++) {
        if(ans > 0) {
            ans *= (x + 2);
        }
    }
}

void task(int x) {
    while(running) {
        for(int i = 0; i < launch_num; i++) {
            task_kernel<<<1, 1>>>(x);
        }
        this_thread::sleep_for(1ms);
    }
}

void test() {
    vector<thread> threads;
    for(int i = 0; i < thread_num; i++) {
        thread t(task, i);
        threads.push_back(std::move(t));
    }
    this_thread::sleep_for(100ms);
    running = false;
    for(auto& t: threads) {
        t.join();
    }
}

int main() {
    cudaFree(0);
    test();
    return 0;
}`",2022-01-14 03:23:55+00:00,"stat:awaiting response, stale, comp:gpu, type:performance, TF 2.7"
JIT Compile too slow ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version: 3.7
- Bazel version (if compiling from source): 4.4.4
- GCC/Compiler version (if compiling from source): 7.3.1
- CUDA/cuDNN version: cuda 11
- GPU model and memory: yes 80GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/33950866/149608623-f2e6ed39-b587-4015-afbd-954bc491ba06.png)
I run 1+1=2, TF cost 30mins...
It really compile 30 mins every times.

**Describe the expected behavior**
I don't want to wait 30mins ...

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
turn off ```TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.```

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

#@tf.function(experimental_compile=False,autograph=True)
#def zz_sm():
#  # I don't want to use xla .
#  input_a = tf.random.uniform([1024000], dtype=tf.int32, maxval=1024000)
#  uniq_a, uniq_idx = tf.unique(input_a)
#  uniq_a_shape = tf.shape(uniq_a)
#  #uniq_a_shape_print = tf.Print(uniq_a_shape, [uniq_a_shape], ""zz:"", 10, 10)
#  source_a = tf.gather(uniq_a, uniq_idx)
#  source_a = source_a + source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  source_a = source_a * source_a
#  return source_a
#
#source_a = zz_sm()
with tf.xla.experimental.jit_scope(compile_ops=False):
  input_a = tf.random.uniform([1024000], dtype=tf.int32, maxval=1024000)
  uniq_a, uniq_idx = tf.unique(input_a)
  uniq_a_shape = tf.shape(uniq_a)
  #uniq_a_shape_print = tf.Print(uniq_a_shape, [uniq_a_shape], ""zz:"", 10, 10)
  source_a = tf.gather(uniq_a, uniq_idx)
  source_a = source_a + source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a
  source_a = source_a * source_a

#  I want to use xla .
res_a_1 = source_a * source_a
res_a_2 = res_a_1 + res_a_1
res_a_3 = res_a_2 + res_a_2
res_a_4 = res_a_3 + res_a_3
res_a_5 = res_a_4 + res_a_4
res_a_6 = res_a_5 + res_a_5


session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True,
                             log_device_placement=True)
#session_config.graph_options.rewrite_options.disable_meta_optimizer=True
session_config.graph_options.optimizer_options.global_jit_level = tf.compat.v1.OptimizerOptions.ON_1


with tf.compat.v1.Session(config = session_config) as sess:
    for i in range(16384) :
      _ = sess.run([res_a_6]);
      #print(res)
```
",2022-01-15 04:27:14+00:00,"stat:awaiting response, type:bug, stale, comp:core, TF 2.7"
tf.config.LogicalDeviceConfiguration memory_limit does not really work ?,"**System information**
- Custom code 
- Linux Ubuntu 20.04 (tested also 18.04)
- Computer (not mobile device) 
- TensorFlow installed from pip install:
- TensorFlow version 2.7 (tested also 2.4,2.5,2.6,2.8)
- Python version: 3.7 (tested also 3.8)
- Bazel version (if compiling from source): unbuntu 20.04 no self compil
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2.2/8.2.0.1 
- GPU model and memory: NVIDIA RTX3090 24GB (tested also RTX2060 anX GTX1660) Driver 460.x

Hello I successfully ran the profiler tool on ma classification model to profile the maximum memory usage. Because I want to use different CNN on a same GPU. But I'm really baffled by the results of the profiler. Let me explain  

I have a NVIDIA RTX3090 with 24GB memory so for my small CNN I set 512 memory limit in my code before all use with this code :  
` tf.config.set_logical_device_configuration(gpus[0],[tf.config.LogicalDeviceConfiguration(memory_limit=512)])`


It seems to work because of the tensorflow logs
`2022-01-19 16:24:13.615890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with **512 MB memory:**  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:2d:00.0, compute capability: 8.6`

Nvidia-smi shows that GPU use 419MiB are used
![smi first](https://user-images.githubusercontent.com/35316806/150172707-7ff0eef5-6feb-4915-9662-724b16356254.png)

Then I start a batch to make the inference on the classification model with batch size = 1
and tensorboard shows that the model use about 100MiB
![tensorboard](https://user-images.githubusercontent.com/35316806/150173173-823b1989-6174-4b99-a9aa-5fe272ba9b6e.png)

so theoretically I could have set a small memory limit (under 512) but .. here is the real use of the memory given by Nvidia-smi is 1869MiB !
![nvidiaamemoryused](https://user-images.githubusercontent.com/35316806/150173725-d1d8943d-9518-43b2-9607-ef0acfcbc3ec.png)

I tested the code in Tensorflow 2.4,2.5,2.6,2.7 and 2.8. With different CUDA CUDNN but it is the same. Memory limit seems to be applied at Tensorflow level but not at the real GPU memory ( Nvidia level ).
Did I miss something ? It would be very usefull to be able to manage the memory of a model !",2022-01-19 16:43:08+00:00,"stat:awaiting response, type:bug, stale, comp:apis, TF 2.7"
Cause: 'arguments' object has no attribute 'posonlyargs',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 professional 19042.1466
- TensorFlow installed from (source or binary):pip install 
- TensorFlow version (use command below):v2.7.0-rc1-69-gc256c071bb2 , 2.7.0
- Python version:3.7
- CUDA/cuDNN version: CUDA:11.4.0_471.11,cuDNN:8.3.2.44
- GPU model and memory:GTX 970,4G memory

**Describe the current behavior**

I use SARSA Lambda mode in CPU mode, 10 rounds per second, and in GPU DQN mode, 1 round 100 seconds, GPU utilization is less than 1%， and the prompt:
```
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FD428A85E8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```
I don't know if the slow performance is related to this hint.

**Describe the expected behavior**
I expect performance not lower than SARSA Lambda mode.

**Standalone code to reproduce the issue**
[https://github.com/jaried/Tensorflow/blob/main/MountainCar.py](https://github.com/jaried/Tensorflow/blob/main/MountainCar.py)

**Other info / logs** Include any logs or source code that would be helpful to
prof file:
[https://github.com/jaried/Tensorflow/blob/main/question.prof](https://github.com/jaried/Tensorflow/blob/main/question.prof)
",2022-01-23 17:31:41+00:00,"stat:awaiting tensorflower, comp:autograph, type:performance, TF 2.7"
LSTM slow to calibrate with large observation period,"I am reporting an issue in the calibration speed of LSTM models with large observation period even though none of my resource is saturated (CPU, GPU, RAM, SSD)

**System information**
- OS Platform and Distribution: Win11
- TensorFlow installed from: `pip install tensorflow`
- TensorFlow version: 2.7 / GPU
- Python version: 3.9
- CUDA/cuDNN version: cuDNN 8201 (installed via `pip install tensorflow`), CUDA 11.4
- GPU model and memory: Geforce GTX 3060 4 Go / RAM 32 Go / CPU AMD Ryzen 5 / SSD

**Describe the current behavior**
None of the machine resource is saturated following LSTM calibration
`print(tf.config.list_physical_devices(""GPU""))` shows that the calibration runs on the GPU as expected.

**Describe the expected behavior**
Any of the machine resource should be saturated

**Standalone code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

print(tf.config.list_physical_devices(""GPU""))

if __name__ == ""__main__"":

    epoch = 10
    batch_size = 2000
    number_output = 3
    number_features = 5
    backward = 992
    number_node = 40

    def train_gen():
        for i in range(1000):
            yield np.random.random((batch_size, backward, number_features)),\
                  np.random.random((batch_size, number_output))

    train_dataset = tf.data.Dataset.from_generator(train_gen, output_types=(tf.float64, tf.float64))
    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

    drop_ratio = 0.2
    model = Sequential([layers.Input(shape=(backward, number_features))])
    model.add(layers.LSTM(number_node, return_sequences=False))
    model.add(layers.Dropout(drop_ratio))
    model.add(layers.Dense(number_output, activation='sigmoid'))
    model.compile(optimizer='adam', loss='mse')
    model.summary()

    history = model.fit(train_dataset, epochs=epoch)
```


**Other info / logs**
![image](https://user-images.githubusercontent.com/25941523/151064911-8f1ba15f-16d5-4b63-ad9b-ffb19eb9302e.png)

",2022-01-25 21:48:36+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.7"
Why is inference time so slow when using tensorflow.compat.v1?,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source(pip install)
- TensorFlow version (use command below): TFv1(1.13.1 / 1.14.0 /1.15.4), TFv2(2.7.0)
- Python version: Python 3.6(TFv1), Python 3.7(TFv2)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.3 / cuDNN 8.2.1
- GPU model and memory: GTX 1080ti 11G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: 1.13.1 / 1.14.0 / 1.15.4
2. TF 2.0: 2.7.0

**Describe the current behavior**
Inference time is slow when loading and inferring models using tf.compat.v1.
I tested 4 cases about one model.
the model is one of tensorflow object detection model zoo and trained using own dataset(fine tuning). 

1 case) inference using TF 1.13.1 -> inference time : 0.05
2 case) inference using TF 1.14.0 -> inference time : 0.05
3 case) inference using TF 1.15.4 -> inference time : 0.27
4 case) inference using TF 2.7.0 -> inference time : 0.27

Looking at the above results, I think version of tf.compat.v1 is 1.15.4.
Can I change version of tf.compat.v1 from 1.15 -> 1.14?

Our company solution is using tensorflow 1.13 to load and infer models. And I am migrating to tf2 to support RTX 3000 series.

**Describe the expected behavior**

Expected result,
1 case) inference using TF 1.13.1 -> inference time : 0.05
2 case) inference using TF 1.14.0 -> inference time : 0.05
3 case) inference using TF 1.15.4 -> inference time : 0.05
4 case) inference using TF 2.7.0 -> inference time : 0.05


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
",2022-02-04 06:01:39+00:00,"stat:awaiting response, comp:apis, type:performance, TF 2.7"
Memory leak after model.fit is called in tf 2.7 and 2.8 and training does not start,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: custom code
- OS Platform and Distribution: Ubuntu 18 (google colab)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7, 2.8, 2.9.0-dev20220203
- Python version: 3.7, 3.9, 3.10
- CUDA/cuDNN version: Build cuda_11.1.TC455_06.29190527_0
- GPU model and memory: varies

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I'm using my yolo [implementation](https://github.com/alternativebug/yolo-tf2) which used to work fine on tensorflow versions prior to 2.5. I tried recently training yolo3 on a small dataset (which uses `tf.keras.Model.fit`). Here's a colab [notebook](https://colab.research.google.com/drive/18jCTQajjgBO2bmKekaI5frrluJ2CX4NN?usp=sharing) which you can use to reproduce the issue. Shortly after `model.fit` is called, the following 2 messages keep repeating in no particular order:

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)

and

    INFO:tensorflow:Assets written to: ram://eefa3127-ad7d-4445-a186-75fd8f0b81e1/assets

They keep repeating, memory usage keeps growing and eventually a memory crash occurs. (which doesn't happen in earlier tensorflow versions <= 2.5). You can verify so using this other [notebook](https://colab.research.google.com/drive/1a3RAhVA3pCTQj2FyGY2df7vum27pj69a?usp=sharing) which uses tensorflow 2.5 instead, things should go perfectly fine and training goes as expected. I also tried installing tensorflow 2.8 instead of colab's default version (2.7) and the issue persists.

**Describe the expected behavior**

I'm expecting to see the following immediately after `model.fit` is called:

    Epoch 1/100

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes with enough info
- Briefly describe your candidate solution(if contributing): will describe one when I have one.

**Standalone code to reproduce the issue**
* [Notebook](https://colab.research.google.com/drive/18jCTQajjgBO2bmKekaI5frrluJ2CX4NN?usp=sharing) with error
* [Notebook](https://colab.research.google.com/drive/1a3RAhVA3pCTQj2FyGY2df7vum27pj69a?usp=sharing) without error

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Here's the output containing problems (notebook 1):

    2022-02-07 05:52:00,476 yolo_tf2.utils.common.activate_gpu +325: INFO     [260] GPU activated
    2022-02-07 05:52:00,477 yolo_tf2.utils.common.train +468: INFO     [260] Starting training ...
    2022-02-07 05:52:04,293 yolo_tf2.utils.common.create_models +447: INFO     [260] Training and inference models created
    2022-02-07 05:52:04,295 yolo_tf2.utils.common.wrapper +64: INFO     [260] create_models execution time: 3.8118433569999866 seconds
    2022-02-07 05:52:04,301 yolo_tf2.utils.common.create_new_dataset +366: INFO     [260] Generating new dataset ...
    2022-02-07 05:52:07,014 yolo_tf2.utils.common.adjust_non_voc_csv +184: INFO     [260] Adjustment from existing received 10107 labels containing 16 classes
    2022-02-07 05:52:07,022 yolo_tf2.utils.common.adjust_non_voc_csv +187: INFO     [260] Added prefix to images: /content/yolo-data/images
    Parsed labels:
    Car               3153
    Pedestrian        1418
    Palm Tree         1379
    Traffic Lights    1269
    Street Sign       1109
    Street Lamp        995
    Road Block         363
    Flag               124
    Trash Can           90
    Minivan             68
    Fire Hydrant        52
    Bus                 43
    Pickup Truck        20
    Bicycle             17
    Delivery Truck       4
    Motorcycle           3
    Name: object_name, dtype: int64
    2022-02-07 05:52:09,513 yolo_tf2.utils.common.save_fig +33: INFO     [260] Saved figure /content/output/plots/Relative width and height for 10107 boxes..png
    /usr/local/lib/python3.7/dist-packages/yolo_tf2/utils/dataset_handlers.py:209: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
      groups = np.array(data.groupby('image_path'))
    Processing beverly_hills_train.tfrecord
    Building example: 406/411 ... Beverly_hills184.jpg 99% completed2022-02-07 05:52:12,922 yolo_tf2.utils.common.save_tfr +227: INFO     [260] Saved training TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    Building example: 411/411 ... Beverly_hills365.jpg 100% completed
    Processing beverly_hills_test.tfrecord
    Building example: 31/46 ... Beverly_hills335.jpg 67% completed2022-02-07 05:52:13,175 yolo_tf2.utils.common.save_tfr +229: INFO     [260] Saved validation TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    2022-02-07 05:52:13,271 yolo_tf2.utils.common.read_tfr +263: INFO     [260] Read TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    Building example: 46/46 ... Beverly_hills186.jpg 100% completed
    2022-02-07 05:52:18,892 yolo_tf2.utils.common.read_tfr +263: INFO     [260] Read TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    2022-02-07 05:52:50.575910: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
    INFO:tensorflow:Assets written to: ram://eefa3127-ad7d-4445-a186-75fd8f0b81e1/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://cbe6d5a4-5322-494b-ba91-3fd34131cdd9/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://f15f3f25-9adb-4eb0-aa0d-83fa874bc74e/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://86dd6f5f-4416-4465-99c0-928fd88e8a93/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://ca08220f-cabc-4017-96d3-383557342388/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://0f634207-e822-4d6c-a805-3cfeab37532f/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://a971d021-3da4-402a-a004-4ae4aa67148a/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://31d72fdf-1ce6-4131-a7e6-f6444747e9c9/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://dac323b6-591a-481c-bbe6-85bb82bef38c/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://99b029f7-11d1-40f2-b459-fd1d8dca5ba1/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)
    INFO:tensorflow:Assets written to: ram://210489fb-0895-4769-8be3-effd01d92695/assets
    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      layer_config = serialize_layer_fn(layer)

Here's the output without the problem (notebook 2):

    2022-02-07 06:09:53.125735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
    2022-02-07 06:09:55.370728: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
    2022-02-07 06:09:55,387 yolo_tf2.utils.common.train +468: INFO     [269] Starting training ...
    2022-02-07 06:09:55.387211: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
    2022-02-07 06:09:55.387252: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (de0312867ce7): /proc/driver/nvidia/version does not exist
    2022-02-07 06:09:55.427963: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
    2022-02-07 06:10:00,078 yolo_tf2.utils.common.create_models +447: INFO     [269] Training and inference models created
    2022-02-07 06:10:00,080 yolo_tf2.utils.common.wrapper +64: INFO     [269] create_models execution time: 4.689235652999997 seconds
    2022-02-07 06:10:00,081 yolo_tf2.utils.common.create_new_dataset +366: INFO     [269] Generating new dataset ...
    2022-02-07 06:10:02,572 yolo_tf2.utils.common.adjust_non_voc_csv +184: INFO     [269] Adjustment from existing received 10107 labels containing 16 classes
    2022-02-07 06:10:02,574 yolo_tf2.utils.common.adjust_non_voc_csv +187: INFO     [269] Added prefix to images: /content/yolo-data/images
    Parsed labels:
    Car               3153
    Pedestrian        1418
    Palm Tree         1379
    Traffic Lights    1269
    Street Sign       1109
    Street Lamp        995
    Road Block         363
    Flag               124
    Trash Can           90
    Minivan             68
    Fire Hydrant        52
    Bus                 43
    Pickup Truck        20
    Bicycle             17
    Delivery Truck       4
    Motorcycle           3
    Name: object_name, dtype: int64
    2022-02-07 06:10:04,900 yolo_tf2.utils.common.save_fig +33: INFO     [269] Saved figure /content/output/plots/Relative width and height for 10107 boxes..png
    /usr/local/lib/python3.7/dist-packages/yolo_tf2/utils/dataset_handlers.py:209: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
      groups = np.array(data.groupby('image_path'))
    Processing beverly_hills_train.tfrecord
    Building example: 392/411 ... Beverly_hills294.jpg 95% completed2022-02-07 06:10:10,341 yolo_tf2.utils.common.save_tfr +227: INFO     [269] Saved training TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    Building example: 411/411 ... Beverly_hills94.jpg 100% completed
    Processing beverly_hills_test.tfrecord
    Building example: 25/46 ... Beverly_hills334.jpg 54% completed2022-02-07 06:10:10,730 yolo_tf2.utils.common.save_tfr +229: INFO     [269] Saved validation TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    Building example: 46/46 ... Beverly_hills251.jpg 100% completed
    2022-02-07 06:10:10,843 yolo_tf2.utils.common.read_tfr +263: INFO     [269] Read TFRecord: /content/data/tfrecords/beverly_hills_train.tfrecord
    2022-02-07 06:10:15,264 yolo_tf2.utils.common.read_tfr +263: INFO     [269] Read TFRecord: /content/data/tfrecords/beverly_hills_test.tfrecord
    2022-02-07 06:10:15.676352: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.
    2022-02-07 06:10:15.676423: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.
    2022-02-07 06:10:15.701051: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      category=CustomMaskWarning)
    2022-02-07 06:10:17.064324: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
    2022-02-07 06:10:17.081408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz
    Epoch 1/100
          1/Unknown - 40s 40s/step - loss: 7333.2617 - layer_205_lambda_loss: 403.8862 - layer_230_lambda_loss: 1509.9465 - layer_255_lambda_loss: 5407.68902022-02-07 06:10:59.974130: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.
    2022-02-07 06:10:59.974196: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.
          2/Unknown - 50s 11s/step - loss: 7819.7124 - layer_205_lambda_loss: 697.4546 - layer_230_lambda_loss: 1647.7856 - layer_255_lambda_loss: 5462.71582022-02-07 06:11:10.059899: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.
    2022-02-07 06:11:10.088821: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.
    2022-02-07 06:11:10.133747: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10
    2022-02-07 06:11:10.157875: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.trace.json.gz
    2022-02-07 06:11:10.189438: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10
    2022-02-07 06:11:10.189678: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.memory_profile.json.gz
    2022-02-07 06:11:10.192796: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10Dumped tool data for xplane.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.xplane.pb
    Dumped tool data for overview_page.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.overview_page.pb
    Dumped tool data for input_pipeline.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.input_pipeline.pb
    Dumped tool data for tensorflow_stats.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.tensorflow_stats.pb
    Dumped tool data for kernel_stats.pb to /content/data/tfrecords/train/plugins/profile/2022_02_07_06_11_10/de0312867ce7.kernel_stats.pb
    
         15/Unknown - 181s 10s/step - loss: 3493.4009 - layer_205_lambda_loss: 232.7220 - layer_230_lambda_loss: 629.6332 - layer_255_lambda_loss: 2618.9722
",2022-02-07 06:19:15+00:00,"stat:awaiting response, type:support, comp:keras, TF 2.8"
tflite with hexagon delegate get poor performance while running mobilenet_quant_v1_224.tflite on Snapdragon 845,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MeiZu 16th, Snapdragon 845
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch, commit hash : 851b83d0ef9659bb83b4f8c56708fb045fe276dc
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): ndk 20

After push all libhexagon_*.so to phone, I run benchmark_model with option ""--use_hexagon=true --hexagon_profiling=true"", then get results below
![image](https://user-images.githubusercontent.com/19945545/152995700-7ecb5c09-a39a-483e-bd54-6870c3fb471e.png)

It seem that symbol of remote_handle_control not found, and the performance just so poor which even almost reach 60ms

",2022-02-08 13:26:01+00:00,"stat:awaiting response, type:performance, TFLiteHexagonDelegate, TF 2.8"
high GPU memory usage with tf.data.Dataset.from_tensor_slices,"System information
- OS Platform and Distribution: Linux 5.13.0-28-generic
- TensorFlow installed from: source
- TensorFlow version: 2.8.0
- Python version: 3.8.10
- Installed using: conda
- CUDA version: 11.5
- GPU model and memory: NVIDIA GeForce RTX 3090 24 GB
- CPU model: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz

When I run the following code on Jupyter Notebook, it uses 22651 MB GPU memory when viewed with `nvtop`:
```
import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
```

It also prints the following message:
```
2022-02-16 11:55:26.100430: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-16 11:55:26.576039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22232 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6
```

I found other issues with similar messages, but they did not have the same memory issue. 
",2022-02-16 11:18:21+00:00,"type:build/install, subtype: ubuntu/linux, TF 2.8"
Internal error `Blas xGEMV launch failed` on Tensorflow v2.8.0 for the same block of codes that runs perfectly well on Tensorflow v2.4.1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0
- Python version: 3.8
- Bazel version (if compiling from source): N.A.
- GCC/Compiler version (if compiling from source): N.A.
- CUDA/cuDNN version: CUDA 11.2.1
- GPU model and memory: Tesla T4 / 16 GB

**Describe the current behavior**
Running a block of code with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1 returns an internal error `Blas xGEMV launch failed` when it runs perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0.

**Describe the expected behavior**
Return the same output as Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): N.A.

**Standalone code to reproduce the issue**

The following block of code works perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0, but not with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1.
```
import tensorflow as tf
empty_image = tf.zeros(shape=[1280, 1280, 3], dtype=tf.float32)
gray_image = tf.image.rgb_to_grayscale(empty_image)
```

An important point to note is that when I reduce the `shape` of `empty_image` to `[512, 512, 3]`, there is no issue. However, I believe this is not a device memory issue as I can reproduce this with GeForce RTX 2080 Ti 11 GB as well as Tesla T4 16 GB.

**Other info / logs** 

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/miniconda3/envs/docrec/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ubuntu/miniconda3/envs/docrec/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 7186, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,1638400,3], b.shape=[1,3,1], m=1638400, n=1, k=3 [Op:MatMul]
```

",2022-02-21 01:08:35+00:00,"type:bug, type:build/install, TF 2.8"
SavedModel way slower than h5 in loading,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.8.0
- Python version: 3.7.7
- Bazel version (if compiling from source): 5.0.0
- GCC/Compiler version (if compiling from source): 7.3
- CUDA/cuDNN version: 11.2/8.1
- GPU model and memory: T4


I have found some issues about this, but I didn't find any solution/explanation about it. Loading a SavedModel is way slower than loading the same model in h5df. Is there any way to speed this up? I find hard to justiy the use SavedModel (even if it is the preferred formar) if it takes 4x-5x longer to load in my code.

![image](https://user-images.githubusercontent.com/61322372/155090488-110b9206-7b85-4b95-8df1-586bf17de3d5.png)

",2022-02-22 08:15:53+00:00,"stat:awaiting response, stale, type:performance, TF 2.8"
tf.saved_model.save is very slow if a tf.function contains an onnx model,"I can successfully convert a pretrained Pytorch model into the onnx format and export to a Tensorflow proto file. I can load this file in Tensorflow, add some preprocessing/postprocessing steps and pack them into a `tf.function`. Everything works fine. Now it is very slow if I want to use `tf.saved_model.save` to save this new tf.function.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.8.0
- Python version: 3.7.11
- CUDA/cuDNN version: 11.3
- GPU model and memory: 2 NV TESLA K80 GPUs, 2x12GB 

**Describe the current behavior**
It took 500+ sec to save a simple resnet18, 7000+ sec to save a resnet34, and more than 40 hours to save a resnet50 in the following toy example.
Not sure why this process is so slow or I did something wrong.

**Describe the expected behavior**
Hopefully it could be faster.

**Standalone code to reproduce the issue**
```python
import torch
from torchvision.models import resnet18
import torch.onnx

import tensorflow as tf

import onnx
from onnx_tf.backend import prepare

# load a pytorch model
torch_model = resnet18(pretrained=True, progress=False)
# save to onnx
save_dst = './torch_resnet18.onnx'
x = torch.rand(1, 3, 224, 224)
torch.onnx.export(
    torch_model, 
    x, 
    save_dst, 
    verbose=True,
    opset_version=12,
    export_params=True, 
    input_names = ['input'], 
    output_names = ['output'], 
    dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}},
    )
# load and save to tf graph
onnx_model = onnx.load(save_dst)
tf_model = prepare(onnx_model)
tf_model.export_graph('tf_resnet18')
#%% modify
class TestModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.t_model = tf.saved_model.load('tf_resnet18')
        self.f = self.t_model.signatures['serving_default']

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None, 256, 256, 3], dtype=tf.float32),
    ])
    def __call__(self, x):
        x = tf.image.resize(x, (224, 224))
        x = tf.transpose(x, [0, 3, 1, 2])
        return self.f(x)

m = TestModule()
x = tf.random.uniform((1, 256, 256, 3))
y = m(x)
print(y)
# it works fine above
tf.saved_model.save(m, 'tf_new')
# the last line takes a lot of time...
```
**Note**
It seems the program spent a lot of time on:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1959
which leads to a lower level implementation:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1084",2022-02-28 15:16:02+00:00,"stat:awaiting response, type:performance, comp:tf.function, TF 2.8"
MultiWorkerMirroredStrategy reinitializes reduce operations multiple times at start and also from time to time.,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4 
- GPU model and memory: V100 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Fyi: I use a custom training loop. 
On a multi worker setup with `MultiWorkerMirroredStrategy` I get this output for the first 3 train steps and they are also super slow:
```
[08/03/22 16:50:11] [tensorflow] INFO : Collective all_reduce tensors: 236 all_reduces, num_devices = 8, group_size = 48, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 48, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1
``` 

Then the training runs as expected but it seems that the above init/retracing can happen randomly in the middle of the training again with the same logs. So an intermediate step after 1000 iterations can again take minutes (the more workers/devices the longer)
The step at which this happens is not the same for multiple runs, that being the reason why I say randomly.

Could this be related to non-constant input sizes (number of points in a pointcloud? They are of course padded within a batch) Do I need to globally pad here?

**Describe the expected behavior**
Only initialize the Collective all_reduce tensors once and especially not in the middle of training.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
",2022-03-08 17:02:15+00:00,"stat:awaiting response, comp:dist-strat, type:performance, TF 2.7"
Incorrect calculation of 2nd derivative of a determinant of a matrix,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Monterey.  Also seen on Linux.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0 (also seen in 2.6.X)
- Python version: 3.8.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A, but also seen in cuda11.4
- GPU model and memory: CPU, but also seen in A100 40GB/80GB

**Describe the current behavior**

The calculation of the second derivative of a determinant is incorrect. 

It is a little challenging to describe the full issue in markdown.  I've created a standalone notebook to reproduce the issue.  I compare the TF gradients through a determinant against both finite differences and a custom (albeit slow) determinant implementation.  The TF gradients for the determinant of a matrix, let's call them G, agree with both finite differences and the custom op.  The second derivative, dG/dx, aka the Hessian of the determinant of a matrix, is badly incorrect.  Using autodifferentiation on the custom determinant operation twice, however, agrees well with finite differences methods.

Full reproducer available in this notebook (standalone) here: https://github.com/Nuclear-Physics-with-Machine-Learning/MLQM/blob/spin/examples/2nd%20Derivative%20of%20Determinant%20of%20a%20matrix.ipynb

**Describe the expected behavior**

The calculation of the second derivative of a determinant should be correct.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):  If we can track down the bug, and it's something i can fix, I could.  I haven't modified yet.
- Briefly describe your candidate solution(if contributing):  I believe the operations registered for the backprop through a determinant of a matrix must be missing something, and this could be fixed.

**Standalone code to reproduce the issue**

https://github.com/Nuclear-Physics-with-Machine-Learning/MLQM/blob/spin/examples/2nd%20Derivative%20of%20Determinant%20of%20a%20matrix.ipynb

",2022-03-14 18:05:42+00:00,"stat:awaiting tensorflower, type:bug, comp:ops, TF 2.8"
Generalise `xla::Map` to functions over arbitrary shapes,"**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Currently `xla::Map` only allow functions from shape [] to []. I'd like to use functions with arbitrary shapes. For example, apply a function with shapes [2, 3] -> [5] to a tensor [100, 4, 2, 3] to get a [100, 4, 5].

I've not yet thought about how it would work when mapping multiple input tensors at once.

**Will this change the current api? How?**

It would extend `xla::Map`, either internally, or as an overload. It's possible additional information would need to be passed to a more general implementation of map, in which an overload may be preferable to maintain backwards compatibility.

**Who will benefit with this feature?**

Anyone using XLA who'd like to apply a function over sections of a tensor. It would effectively be an alternative to (some portion of) broadcasting, allowing people to use functions that don't allow leading dimensions to tensors with leading dimensions.

It is particularly useful for me as I'm working with dependent types and adding leading dimensions is non-trivial (and verbose) in type signatures.

**Any Other info.**

It is already possible to do this by indexing into the tensor and iteratively applying the function to the contents, then concatenating the results, but I expect this is significantly slower than could be achieved within XLA. I have considered using `xla::While` for this but I still expect the slicing and concatenation would still come with a significant performance cost.",2022-03-15 21:25:46+00:00,"stat:awaiting tensorflower, type:feature, comp:xla"
can it possible improve the tf serving performance,"Now I train a recommend nn model offline ,and then predict it by tf serving on cpu online machine. There are 8 cores i just applied, and found it cost slow when predict it. More than 0.4% it costs 100ms when predict. The batch size request is 50. There are 167 one hot features and 3 full-connected layers.And the usage of cpu is also slow, it is only 20% usage.
How can i analyze the bottleneck of serving ,and can it possible to reduce the time cost ratio by adjust some parameters?

i have tried many ways followed this links https://www.tensorflow.org/tfx/serving/performance ,but it hardly worked.
Tf serving can't support many sparse features so well?",2022-03-21 15:21:13+00:00,"stat:awaiting response, type:others"
undefined references on tflite x86_64 cross compiled with NDK Android toolchain,"
**System information**
- OS Platform and Distribution: Linux Ubuntu 21.04
- TensorFlow version: 2.8.0
- GCC/Compiler version: Clang 9.0.8, from Android NDK 21.3.6528147


**Describe the problem**

I'm able to build `libtensorflow-lite.a` for Android x86_64, but when I try to link the library I get
```
error: undefined reference to 'xnn_setup_runtime'
```
If I build the lib without XNN delegate support, I got an error on NNAPI. If also disable NNAPI, I got undefined reference errors for ryu related functions. It seems like that all the delegate symbols in the built static library are undefined.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

to build  `libtensorflow-lite.a`:
```
mkdir build
cd build
cmake -DCMAKE_TOOLCHAIN_FILE=~/Android/Sdk/ndk/21.3.6528147/build/cmake/android.toolchain.cmake -DANDROID_ABI=x86_64  ../tensorflow/lite
make -j12
```
later linked using CMake on an Android project with `target_link_libraries(...)`.



**Any other info / logs**

output of `nm -C libtensorflow-lite.a | grep xnn`:
```
0000000000000000 T tflite::xnnpack::DequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int, double)
0000000000000000 T tflite::xnnpack::DequantizeFloat16(unsigned short const*, float*, unsigned long)
0000000000000000 T tflite::xnnpack::PerChannelDequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int const*, float const*, int)
xnnpack_delegate.cc.o:
                 U xnn_create_runtime_v2
                 U xnn_create_subgraph
                 U xnn_define_abs
                 U xnn_define_add2
                 U xnn_define_argmax_pooling_2d
                 U xnn_define_average_pooling_2d
                 U xnn_define_bankers_rounding
                 U xnn_define_ceiling
                 U xnn_define_channelwise_quantized_tensor_value
                 U xnn_define_clamp
                 U xnn_define_convert
                 U xnn_define_convolution_2d
                 U xnn_define_deconvolution_2d
                 U xnn_define_depth_to_space
                 U xnn_define_depthwise_convolution_2d
                 U xnn_define_divide
                 U xnn_define_elu
                 U xnn_define_floor
                 U xnn_define_fully_connected
                 U xnn_define_global_average_pooling_2d
                 U xnn_define_hardswish
                 U xnn_define_leaky_relu
                 U xnn_define_maximum2
                 U xnn_define_max_pooling_2d
                 U xnn_define_minimum2
                 U xnn_define_multiply2
                 U xnn_define_negate
                 U xnn_define_prelu
                 U xnn_define_quantized_tensor_value
                 U xnn_define_sigmoid
                 U xnn_define_softmax
                 U xnn_define_square
                 U xnn_define_squared_difference
                 U xnn_define_square_root
                 U xnn_define_static_constant_pad
                 U xnn_define_static_reshape
                 U xnn_define_static_resize_bilinear_2d
                 U xnn_define_subtract
                 U xnn_define_tensor_value
                 U xnn_define_unpooling_2d
                 U xnn_delete_runtime
                 U xnn_delete_subgraph
                 U xnn_initialize
                 U xnn_invoke_runtime
                 U xnn_setup_runtime
0000000000000000 b guard variable for tflite::xnnpack::(anonymous namespace)::Delegate::Delegate(TfLiteXNNPackDelegateOptions const*)::s_logged
0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphFree(TfLiteContext*, void*)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphInit(TfLiteContext*, char const*, unsigned long)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphInvoke(TfLiteContext*, TfLiteNode*)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::SubgraphPrepare(TfLiteContext*, TfLiteNode*)
0000000000000000 d tflite::xnnpack::(anonymous namespace)::kSubgraphRegistration
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReluNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, float, float, std::__ndk1::vector<unsigned int, std::__ndk1::allocator<unsigned int> > const&)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckPoolingParams(TfLiteContext*, TfLitePoolParams const*, int)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckMediaPipePoolParams(TfLiteContext*, TfLitePoolParams const*, int)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckTensorQInt8OrQUInt8Type(tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteTensor const&, int, int)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CalculateTransposeConvPaddings(TfLiteContext*, TfLitePadding, int, int, int, int, int, int, int, int, int, int, int, int*, int*, int*, int*, int*, int*)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckTensorFloat32OrQCInt8Type(tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteTensor const&, int, int, int)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::CheckTensorFloat32OrQUInt8Type(tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteTensor const&, int, int)
0000000000000000 t tflite::xnnpack::(anonymous namespace)::Subgraph::VisitNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteRegistration*, TfLiteNode*, int, std::__ndk1::unordered_set<int, std::__ndk1::hash<int>, std::__ndk1::equal_to<int>, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<unsigned int, std::__ndk1::allocator<unsigned int> > const&)
                 U tflite::xnnpack::DequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int, double)
                 U tflite::xnnpack::DequantizeFloat16(unsigned short const*, float*, unsigned long)
                 U tflite::xnnpack::PerChannelDequantizeInt8(signed char const*, float*, tflite::RuntimeShape const&, int const*, float const*, int)
0000000000000000 t bool std::__ndk1::__insertion_sort_incomplete<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)
0000000000000000 t void std::__ndk1::__sort<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)
0000000000000000 W void std::__ndk1::vector<xnn_external_value, std::__ndk1::allocator<xnn_external_value> >::__push_back_slow_path<xnn_external_value const&>(xnn_external_value const&)
0000000000000000 t unsigned int std::__ndk1::__sort3<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)
0000000000000000 t unsigned int std::__ndk1::__sort4<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)
0000000000000000 t unsigned int std::__ndk1::__sort5<tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&, int*>(int*, int*, int*, int*, int*, tflite::xnnpack::(anonymous namespace)::Delegate::PrepareOpsToDelegate(TfLiteContext*)::$_0&)
0000000000000000 b tflite::xnnpack::(anonymous namespace)::Delegate::Delegate(TfLiteXNNPackDelegateOptions const*)::s_logged
tflite_with_xnnpack_optional.cc.o:
```

",2022-03-22 18:48:46+00:00,"stat:awaiting response, type:build/install, stale, comp:lite, TF 2.8"
TF.data.dataset.cache(path) still uses memory despite the cache file path is given in tf2.8,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf2.8.0
- Python version:
python 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
CUDA v11.2. cuDNN v8.1
- GPU model and memory:
K80, gMem=16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I came across a weird problem when I read TFrecords files from S3 through tf.dataset and cached them to my local path. Here is my reading code

    filenames=['s3s:path1', ''s3s:path2']
    dataset = tf.data.TFRecordDataset(filenames, compression_type=""GZIP"")
    parsed_dataset = (
        dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)
        .map(decode, num_parallel_calls=tf.data.AUTOTUNE)
        .cache(cache_file_path)
        .prefetch(tf.data.AUTOTUNE)
    )
It’s very strange that cache() still uses the internal memory which results in OOM. Here is the memory usage I printed via callback during training.

2022-03-08T22:19:40.154191003Z ...Training: end of batch 15700; got log keys: ['loss', 'copc', 'auc']
2022-03-08T22:19:40.159188560Z totalmemor: 59.958843GB
2022-03-08T22:19:40.159223737Z availablememory: 8.418320GB
2022-03-08T22:19:40.159250296Z usedmemory: 50.959393GB
2022-03-08T22:19:40.159257814Z percentof used memory: 86.000000
2022-03-08T22:19:40.159263710Z freememory:1.072124GB
2022-03-08T22:19:47.752077011Z Tue Mar  8 22:19:47 UTC 2022	job-submitter:	job run error: signal: killed

**Describe the expected behavior**
I have tested the code on TF2.3 which has no such an issue, but TF2.8 have such an OOM issue. It's supposed that when the path is given, cache(path) should use files to cache instead of memory.


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
",2022-03-24 01:59:37+00:00,"stat:awaiting response, type:bug, stale, comp:data, TF 2.8"
Calling `model.compile()` multiple times leads to memory leak,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04, macOS 12.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): `pip install tensorflow==2.8.0`
- TensorFlow version (use command below): 2.8.0
- Python version: 3.9.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: - 
- GPU model and memory:- 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

[Stackoverflow](https://stackoverflow.com/posts/71633201/timeline)

I have tried resetting the state of the optimizer by re-compiling the model. Then I found that it could lead to a memory leak.

reproducible code attached.

**Describe the expected behavior**

Since there are no active variables that could reach the old generated model graph. The old graph in RAM should be freed.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): -

**Standalone code to reproduce the issue**


```python
import tensorflow as tf
import numpy as np
import objgraph

m = tf.keras.models.Sequential([tf.keras.layers.InputLayer(input_shape=(20,)), tf.keras.layers.Dense(10),tf.keras.layers.Dense(10),tf.keras.layers.Dense(10),tf.keras.layers.Dense(10)])
objgraph.show_growth()

for i in range(100):
    tf.keras.backend.clear_session()
    m.compile('adam', loss='mse')
    data = np.arange(32*20).reshape(32, 20)
    labels = np.zeros(32)
    results = m.fit(data, labels, epochs=10)
    objgraph.show_growth()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
",2022-03-28 01:24:42+00:00,"stat:awaiting response, type:bug, stale, comp:keras, TF 2.8"
GRU performance severely degraded inside tf.function with Apple m1 chip,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS Monterey 12.3, Metal device set to: Apple M1 Pro
- TensorFlow installed from (source or binary):
binary
- TensorFlow version:
tensorflow-deps           2.7.0                
tensorflow-macos          2.8.0              
tensorflow-metal          0.4.0 
- Python version:
3.9.12
- GPU model and memory:
 Apple M1 Pro

**Describe the current behavior**

I run this simple code with a GRU layer with a `tf.function` decorator:

```
import tensorflow as tf
from time import time

a = tf.random.truncated_normal([4, 4, 4])
layer = tf.keras.layers.GRU(4) 

@tf.function
def f(a):
    return layer(a)

start = time()
for _ in range(1000):
    with tf.GradientTape() as tape:
        b = f(a)
print(str(time() - start), ""seconds"")
```
its much slower (~5-10x times) than running in the eager mode. However, this bug only shows up for recurrent layers. When using Dense, the `tf.function` mode is faster than the eager mode as expected. The issue also disappeared outside `tf.GradientTape()`.
I only encountered this problem in my Apple Macbook Pro with M1 chip. I tried it on a linux machine and it's ok.

**Describe the expected behavior**
`tf.function` should be faster (at least not several times slower) than the eager mode.

**Standalone code to reproduce the issue**
It cannot be reproduced on a linux machine, so no Colab notebook is available.

**Other info / logs** 

FYI the code above runs with the warning message as follows:

> 2022-03-31 14:40:34.462151: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-03-31 14:40:34.463604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
2022-03-31 14:40:34.480917: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.
2022-03-31 14:40:34.487243: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] tfg_optimizer{} failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.
	when importing GraphDef to MLIR module in GrapplerHook
2022-03-31 14:40:34.488903: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.
2022-03-31 14:40:34.494395: W tensorflow/core/common_runtime/process_function_library_runtime.cc:932] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node gru_partitionedcall_10_RetVal was passed float from gru/PartitionedCall:12 incompatible with expected variant.
2022-03-31 14:40:34.508855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.",2022-03-31 19:41:40+00:00,"stat:awaiting response, type:bug, stale, type:performance, comp:tf.function, TF 2.8"
Memory Leak in Example/Tutorial Documentation,"## URL(s) with the issue:
https://www.tensorflow.org/tensorboard/image_summaries#logging_arbitrary_image_data

## Description of issue (what needs changing):
Listed example needs to be corrected as to avoid memory leaks.

### Clear description
It appears that the example is creating `tf.image.decode_png`  and `tf.expand_dims` layers within `plot_to_image` which is being repeatedly called by a `tf.keras.callbacks.LambdaCallback`.  These layers are filling VRAM or system RAM, depending on whether or not the example is running on GPU or not.  ",2022-04-04 18:40:22+00:00,"type:docs-bug, stat:awaiting response, stale"
High memory usage while training with tflite_model_maker,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.8.0
- Python version: 3.7.13
- CUDA/cuDNN version: CUDA 11.1
- GPU model and memory: Tesla K80 (Colab GPU) 24 GB GDDR5

**Describe the current behavior**

No errors, however upon increasing `batch_size` past 1 or 2 an OOM error is incurred. Even when keeping `batch_size` to 1, OOM occurs when using more than 1 epoch. When using 1 `batch_size` and 1 `epoch`, after execution of epochs, the memory usage grows until eventually crashing. 

**Describe the expected behavior**

No errors, the training ends without crashing.

**Standalone code to reproduce the issue**
[colab](https://colab.research.google.com/drive/1bMpah_j9tyyK_cg3uL0KuFv8BBO3k4Iu?usp=sharing)",2022-04-05 16:06:22+00:00,"stat:awaiting tensorflower, type:support, comp:lite, TF 2.8"
2.7.0: memory leak in TFLite's tflite::Interpreter::Invoke(),"Seeing a memory leak in tflite::Interpreter::Invoke().
The leak is observed while running our software, wrapping TFLite 2.7.0 (built from source at that tag), on iOS 15.4 (EDIT: confirmed still leaking with 2.7.1 and 15.4.1), and using CoreML delegate.

The following leaks seem to occur roughly with every call to Invoke:

80 bytes chunk with the following stack:
```
class_createInstance		
__CFAllocateObject		
__NSSetI_new		
-[NSSet initWithArray:range:copyItems:]		
+[NSSet setWithArray:]		
-[MLDictionaryFeatureProvider featureNames]		
0x11e19b664		
0x11e14b44c		
tflite::Subgraph::Invoke()		
tflite::Interpreter::Invoke()	
```

48 bytes chunk with the following stack:
```
class_createInstance		
__CFAllocateObject		
__NSSetI_new		
-[NSSet initWithArray:range:copyItems:]		
+[NSSet setWithArray:]		
-[MLDictionaryFeatureProvider featureNames]		
0x11e19b664		
0x11e14b44c		
tflite::Subgraph::Invoke()		
tflite::Interpreter::Invoke()
```

16 bytes chunk with the following stack:
```
_CFCreateArrayStorage		
-[NSDictionary allKeys]		
-[MLDictionaryFeatureProvider featureNames]		
0x11e19b664		
0x11e14b44c		
tflite::Subgraph::Invoke()		
tflite::Interpreter::Invoke()	
```

32 bytes chunk with the following stack 
```
_objc_rootAllocWithZone		
objc_alloc_init		
-[NSTaggedPointerString UTF8String]		
0x11e19afd0		
-[MLNeuralNetworkEngine verifyInputs:error:]		
-[MLNeuralNetworkEngine evaluateInputs:options:error:]		
__62-[MLNeuralNetworkEngine predictionFromFeatures:options:error:]_block_invoke		
0x10342e7bc		
_dispatch_lane_barrier_sync_invoke_and_complete		
-[MLNeuralNetworkEngine predictionFromFeatures:options:error:]		
0x11e19b714		
0x11e14b44c		
tflite::Subgraph::Invoke()		
tflite::Interpreter::Invoke()	
```

Last time I went hunting for memory leaks, we were using 2.5.0, and there had been no leak there.",2022-04-06 00:46:56+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TF 2.7"
"XNNPack delegate support for quantized models, no latency improvement","

I built tensorflow with  --define tflite_with_xnnpack=true --define xnn_enable_qs8=true to have acceleration for quantized models. But I don't gain any improvement in the latency of the quantized models compared to when I build tensorflow with --define tflite_with_xnnpack=true alone. I tested both in windows and linux. I am testing on desktop with intel CPU.  Is there anything I am doing wrong? I tested with trained models too. I used the following code to quantize the model.:

```
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import *

from os import listdir
from os.path import isfile, join

def representative_dataset():
    folder = '.\\test'
    onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]  #you can use random arrays I guess
    for file in onlyfiles:
        data = np.load(join(folder, file))
        data = np.expand_dims(data, axis=0)
        yield [tf.dtypes.cast(data, tf.float32)]

def SOC(input_tensor, classNumer=1, epsilonBN=1e-3):
    a = Input(shape=input_tensor)
    x = Conv2D(16, (3, 3), padding='same', use_bias=False, name=""Conv_1"")(a)
    x = BatchNormalization(epsilon=epsilonBN, name=""BN_1"")(x)
    x=Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2), name=""MaxPool_1"")(x)

    x = Conv2D(32, (3, 3), padding='same', use_bias=False, name=""Conv_2"")(x)
    x = BatchNormalization(epsilon=epsilonBN, name=""BN_2"")(x)
    x=Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2), name=""MaxPool_2"")(x)

    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name=""Conv_3"")(x)
    x = BatchNormalization(epsilon=epsilonBN, name=""BN_3"")(x)
    x=Activation('relu')(x) 
    x = MaxPooling2D(pool_size=(2, 2), name=""MaxPool_3"")(x)

    x = Conv2D(128, (3, 3), padding='same', use_bias=False, name=""Conv_4"")(x)
    x = BatchNormalization(epsilon=epsilonBN, name=""BN_4"")(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2), name=""MaxPool_4"")(x)

    x = Flatten()(x)
    x = Dense(classNumer, kernel_initializer='uniform', name=""Dense"")(x)
    model = tf.keras.Model(inputs=a, outputs=x)
    return model

new_model = SOC((32,32,5))
converter = tf.lite.TFLiteConverter.from_keras_model(new_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
tflite_quant_model = converter.convert()

with open('qmodel.tflite', 'wb') as f:
    f.write(tflite_quant_model)
````

**System information**
- OS: Windows 10 Pro
- Desktop, Intel(R) Core(TM) i7-9700K CPU 
- Tensorflow 2.9 built from source (tested with tensoflow 2.8 built from source on linux too)
- python 3.10
- Bazel 5.1.0
- Did not include gpu support on the tensorflow build in windows

",2022-04-07 04:09:38+00:00,"stat:awaiting response, stale, type:performance, comp:lite-xnnpack, TF 2.8"
Alom,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2022-04-07 21:39:07+00:00,"stat:awaiting response, type:others, TFLiteConverter, TF 2.3"
imdeu.htmi,"### 1. System information

git adt- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github gtr SHA, if built comdlim t):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2022-04-07 21:51:42+00:00,"type:others, TFLiteConverter, TF 2.3"
[Memory Issue]: Dst tensor is not initialized,"Summary: I believe this issue is fired by Memory Issue with GPU on Tensorflow 2.8 and Tensorflow-GPU 2.8.

Configuration:
- Python 3.8.12
- GPU: NVIDIA Quadro P1000 Notebook 4Gb (InUse: ~ 3Gb)
- RAM: 16 Gb
- OS: Windows 10 Pro
Config 1: Tensorflow: 2.8 - Tensorflow-GPU: 2.8 - TF-Addons: 0.16.1 - Keras: 2.8
Config 2: Tensorflow: 2.5.x - Tensorflow-GPU: 2.5.x - TF-Addons: 0.15

Description: The following model was just a super-cheap model using only Dense, Add, and Concatenation with only 8M+ parameters, but the warning warns me about memory overflow and stop my training on PyCharm

Total behaviour: 
(Batch Size: 256)
- I have been training this model (8M+) and two heavier versions (48M+, and 55M+) on Tensorflow 2.3.x and 2.5.x but Tensorflow did not raise to me either warning and error and the model still worked fine. The model received four input with `dtype='uint8'`, totally 14930 features per vector using 256 batch size with Adam optimizer.
- But since I upgraded my Tensorflow to 2.8, this issue arised on both models, even I have pruned my model to 8M+ parameters with 13674 features per vector only (same dtype=uint8). 
- In the same code, I always used `from tensorflow.keras import ...` for both versions: 2.5- and 2.8

The memory log is displayed in PyCharm 2021.3.3
[memory_p07-15.pdf](https://github.com/tensorflow/tensorflow/files/8485684/memory_p07-15.pdf)

Behaviour Prediction: I believed this is raised in version 2.6 due to the introduction of API shifting. Please check on that. Also, the PyCharm could not track the code + autocompletion when Tensorflow >= 2.6 (It worked only with Tensorflow 2.3 - 2.5*).
* I did not try installing previous version of Tensorflow ",2022-04-14 01:50:01+00:00,"stat:awaiting response, type:bug, stale, comp:ops, TF 2.8"
ValueError: Input 0 of node model/prune_low_magnitude_conv2d/AssignVariableOp was passed float from model/prune_low_magnitude_conv2d/Mul/ReadVariableOp/resource:0 incompatible with expected resource.,"### 1. System information

- OS Platform and Distribution:
centos 7
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
pip package， tensorflow2.6,  tensorflow-model-optimization==0.7.2

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

`
            self.model = create_model(args['networks'])

            pruning_params = {
                'pruning_schedule':
                    PolynomialDecay(
                        initial_sparsity=0.0,
                        final_sparsity=0.50,
                        begin_step=0,
                        end_step=500000000,
                        frequency=100),
            }

            self.model = prune.prune_low_magnitude(
                self.model, **pruning_params)

        self.lg.info('Create model successfully! Params: [{:.2f}]K'.format(self.model.count_params()/1e3))
        self.lg.info('Create model successfully! Params: [{:.2f}]'.format(self.model.count_params()))

        self.train_data = train_data
        self.val_data = val_data
        self.writer = writer

        self.optimizer = tf.keras.optimizers.Adam(lr=self.opt['lr'])
        lr_scheduler = LearningRateScheduler(self.scheduler)
        epoch_end_call = Epoch_End_Callback(self.val_data, self.train_data, self.lg, self.writer, args['paths'], self.opt['val_step'], state=self.state)
        self.callback = [lr_scheduler, epoch_end_call]

        self.prune_callbacks = [
            lr_scheduler,
            epoch_end_call,
            pruning_callbacks.UpdatePruningStep(),
            pruning_callbacks.PruningSummaries(log_dir=""./logs""),
        ]

    def train(self):
        if self.resume == False:
            self.model.compile(optimizer=self.optimizer, loss=self.opt['loss'])
        # history = self.model.fit(self.train_data, epochs=self.opt['epochs'], workers=self.opt['workers'], callbacks=self.callback, initial_epoch=self.state['current_epoch']+1)
        history = self.model.fit(self.train_data, epochs=self.opt['epochs'], workers=self.opt['workers'], callbacks=self.prune_callbacks, initial_epoch=self.state['current_epoch']+1)

        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        converter._enable_tflite_resource_variables = True
        converter.optimizations = {tf.lite.Optimize.EXPERIMENTAL_SPARSITY, tf.lite.Optimize.DEFAULT}
        tflite_model = converter.convert()

        tflite_model_path = './logs/model.tflite'
        print('model is saved to {}'.format(tflite_model_path))
        with open(tflite_model_path, 'wb') as f:
            f.write(tflite_model)


`



### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

`
/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  category=CustomMaskWarning)
Traceback (most recent call last):
  File ""train.py"", line 68, in <module>
    solver.train()
  File ""/home/usrname/SR/TF/mymodel_03/solvers/solver.py"", line 122, in train
    tflite_model = converter.convert()
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 729, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 715, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1123, in convert
    self._freeze_keras_model())
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py"", line 218, in wrapper
    raise error from None  # Re-throws the exception.
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py"", line 208, in wrapper
    return func(*args, **kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1080, in _freeze_keras_model
    self._funcs[0], lower_control_flow=False))
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1235, in convert_variables_to_constants_v2_as_graph
    converted_input_indices)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1080, in _construct_concrete_function
    new_output_names)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 650, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 628, in wrap_function
    collections={}),
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 87, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 93, in wrapped
    return fn(*args, **kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 648, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 549, in new_func
    return func(*args, **kwargs)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 0 of node model/prune_low_magnitude_conv2d/AssignVariableOp was passed float from model/prune_low_magnitude_conv2d/Mul/ReadVariableOp/resource:0 incompatible with expected resource.
`

### 4. bug
When I use tfmot to try to sparse a model and convert the model to tflite format, it reports an error：
`
ValueError: Input 0 of node model/prune_low_magnitude_conv2d/AssignVariableOp was passed float from model/prune_low_magnitude_conv2d/Mul/ReadVariableOp/resource:0 incompatible with expected resource.
`

",2022-04-17 11:47:09+00:00,"stat:awaiting response, type:bug, stale, TFLiteConverter, TF 2.8"
The callbacks in the training functions in kernels_experimental.cc unnecessarily transfer ownership of the tensors to the caller,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Currently, the callbacks in the `kernels_experimental.cc` file transfer ownership of the tensors to the caller. Although we explain this behavior in the header, this is not intuitive because the tensors don't need to outlive the callback. A better solution would be to automatically free the tensors as soon as the callback returns, thus avoiding instances where callers could mistakenly omit to free the memory themselves.


### Standlone code to reproduce the issue

```c++
void callback(TF_OpKernelContext* ctx, TF_Tensor* source, TF_Tensor* dest) {
  // We are not releasing the memory for `source` and `dest, so memory leak occurs
}

TF_AssignVariable(ctx, 0, 1, callback, status);
```


### Relevant log output

_No response_</details>",2022-04-19 02:12:12+00:00,"stat:awaiting response, type:bug, stale, TF 2.9"
Tensorflow 2.8.0 Limiting GPU memory growth fails.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.3.1 cuDNN 8.2.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
My GPU card is Nivida RTX 2080Ti. Follow the GPU usage instruction on the website, I want to limit my gpu memory growth during a HMC test and use the code as follows:
'''
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
  except RuntimeError as e:
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

# from colabtools import adhoc_import
from contextlib import ExitStack

ADHOC = True
CLIENT = 'fig-export-fig_tree-change-451-3e0a679e9746'

import tensorflow_probability.substrates.jax as tfp
from fun_mc import using_jax as fun_mc
tfd = tfp.distributions



dist = tfd.Normal(0., 1.)
n_chains = 1024
n_super_chains = 4
n_steps = 100
n_sub_chains = n_chains // n_super_chains

def target_log_prob_fn(x):
  return dist.log_prob(x), ()


def kernel(hmc_state, seed):
  hmc_seed, seed = jax.random.split(seed)
  hmc_state, hmc_extra = fun_mc.hamiltonian_monte_carlo_step(
      hmc_state,
      target_log_prob_fn=target_log_prob_fn,
      step_size=0.5,
      num_integrator_steps=1,
      seed=hmc_seed)
  return (hmc_state, seed), (hmc_state.state, hmc_extra.is_accepted)



init_x=dist.sample([n_chains], seed=jax.random.PRNGKey(1))

_, (chain, is_accepted) = fun_mc.trace((fun_mc.hamiltonian_monte_carlo_init(init_x,
    target_log_prob_fn), jax.random.PRNGKey(1)), kernel, 1000)

init_x2 = dist.sample([n_super_chains], seed=jax.random.PRNGKey(3))
init_x2 = jnp.repeat(init_x2, n_sub_chains)
#init_x2 = dist.sample([num_chains], seed=jax.random.PRNGKey(3))
init_x2 = init_x2.reshape([n_super_chains, n_sub_chains])

_, (chain2, is_accepted2) = fun_mc.trace((fun_mc.hamiltonian_monte_carlo_init(init_x2,
    target_log_prob_fn), jax.random.PRNGKey(3)), kernel, 1000)

chain = jnp.concatenate([init_x[jnp.newaxis], chain], 0)
chain2 = jnp.concatenate([init_x2[jnp.newaxis], chain2], 0)
'''
```


### Standalone code to reproduce the issue

```shell
However, I find it failed to control my gpu usage and the code use all the memory in GPU 0. I want to know how to make the limitation works. Thank you !
```


### Relevant log output

_No response_</details>",2022-04-22 06:58:18+00:00,"stat:awaiting response, type:support, stale, comp:gpu, TF 2.8"
Why does tflite using CoreML delegate not accelerate on small models?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
mobilnetv2, tflite with CoreML slower than on GPU
```


### Standalone code to reproduce the issue

```shell
n ocode
```


### Relevant log output

_No response_</details>",2022-04-26 03:34:03+00:00,"stat:awaiting response, stale, comp:lite, type:performance, TF 2.8"
Unroll factor for keras.layers.RNN or performance fix for TensorArray / while_loop.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04


### CUDA/cuDNN version

11.5

### GPU model and memory

RTX 2080Ti

</details>

### Feature description


`keras.layers.RNN` supports unrolling, which is great for performance, unless the unrolled time dimension is too long. The XLA compiler takes a ton of time processing a fully unrolled loop (e.g. 240 iterations). Every major compiler supports unrolling a loop for a given factor. The `unroll` argument from `keras.layers.RNN` could instead take an integer telling how many iterations it should unroll, and insert a while loop around the remaining factor. Using an unroll factor of 32 would do miracles for most long time-dimension datasets.

XLA speedups are huge for my custom RNN Cell, so I'd like to use it, but now I'm waiting 5 minutes for XLA/ptxas to finish processing the unrolled loop. On the other hand, if I don't unroll, the while_loop introduces a TensorArray struct takes a tremendous amount of time every time-iteration of the RNN as there is a cuMemcpyD2H of 4 bytes happening between GPU and CPU. The whole CUDA driver pipeline stalls for this 4-byte copy. Performance drops by a factor of 2 to 5 because of this. I don't know if the 4-byte copy is an unintentional performance bug in TensorArray/tf.while_loop or not. But if it's not, I think the unroll factor is a good compromise.",2022-04-30 12:18:57+00:00,"stat:awaiting response, type:feature, comp:keras, type:performance"
A problem in the example for pix2pix,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type
Others


### Current Behavior?

```shell
On this site:
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb

The output of the generator is tanh, but the image it is compared to is [0,...,1].  
This means half the dynamic range of the activation is wasted.  
This is going to slow down learning.

Where it says this:
last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='tanh') 

It might be better to say this:
last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='sigmoid')
```


### Standalone code to reproduce the issue

```shell
#test the discriminator

img_in = inp/255.
print(np.max(img_in))
print(np.min(img_in))

plt.imshow(img_in)
plt.colorbar()
plt.title(""Input image Left"")
plt.show()

gen_out = tf.keras.backend.squeeze(gen_output,0)
print(np.max(gen_out))
print(np.min(gen_out))
plt.imshow(gen_out)
plt.colorbar()
plt.title(""Input image Right"")
plt.show()

#put image pair through the discriminator
disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)

#show the discriminator output
# NOTE: the size is MUCH smaller than the input image.
plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')
plt.title(""Discriminator Output (Note the axes scales)"")
plt.colorbar()
```


### Relevant log output

```shell
#this is the actual image range
1.0
0.0

#this is the warning it gives for the need to clip
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).

#this is the actual range of the generator output
1.0
-1.0

<matplotlib.colorbar.Colorbar at 0x1c286147310>
```
</details>",2022-05-04 12:52:31+00:00,"type:docs-bug, awaiting PR merge"
TfLite for Microcontrollers giving hybrid error,"I converted my keras .h5 file to a quantized tflite in order to run on the new stm32f746   but when I run it I get an error saying ""Hybrid Models are not supported on TFLite Micro.""

I'm not sure why my model is appearing as hybrid; the code I used to convert is below:

`      model = keras.models.load_model('2D_2CHAN_32_T1_model_2021-09-17_11-27-55.h5')
       converter = tf.lite.TFLiteConverter.from_keras_model(model)
       converter.optimizations = [tf.lite.Optimize.DEFAULT]
       converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
       tflite_model = converter.convert()
       source_text, header_text = convert_bytes_to_c_source(tflite_model,""best_2Duint8"")
           with  open('best_2Duint8.h',  'w')  as  file:
                 file.write(header_text)
           with  open('best_2Duint8.cc',  'w')  as  file:
                 file.write(source_text)`

If I didn't do the quantization, I could have used the input of Float32 to get the right result, but the model was too big and too slow. If I quantify it, I get the same error whether I change the input to uint8_t or uint16_t.

Error message:../tensorflow/tensorflow/lite/micro/kernels/conv.cc Hybrid models are not supported on TFLite Micro. Node CONV_2D (number 4) failed to invoke with status 1

I'd appreciate if someone could guide me if I'm doing something wrong or if there is a better way to convert it.",2022-05-05 02:33:53+00:00,"stat:awaiting response, type:support, comp:micro, TF 2.8"
File size & memory constantly increasing for keras models with StringLookup layer after reloading & resaving,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 12.3.1
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: binary from pip
-   **TensorFlow version (use command below)**: 2.8.0
-   **Python version**: 3.8.3
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: Intel UHD Graphics 630 1536MB
-   **Exact command to reproduce**: 1) pip install psutil 2) python test.py train 3) python test.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
When a keras Model has a StringLookup layer, and saved as SaveModel format, the file size and the memory usage keeps increasing after reloading & resaving.

I have created an MVP in the following code. To assess the memory usage, I used psutil package so please install it if you do not have it.

Running `python test.py train` will create a very simple model with only a StringLookup layer and a Dense layer, and save it.

And then you can run `python test.py` many times to see the file size and the memory usage every time. On my laptop, file size increased by roughly 4K/run and memory usage increased by 3MB/run. No matter how many times you run it, they never converge.

Interestingly, if the model is saved as H5 format this issue is gone.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The script has been attached:
[test.py.zip](https://github.com/tensorflow/tensorflow/files/8685088/test.py.zip)

The `build_model()` function creates a model with a StringLookup layer and a Dense layer.

The `train_model()` function trains the model with a randomly generated array of size (16, 800000) as input.

The `save_model()` function saves the model in SaveModel format, and prints the file & memory usage.

The `load_model()` function loads it.

Running `python test.py train` will train the model and save it.

Running `python test.py` will reload and resave the model. Run it enough times to see the increasing trend.

Pasting the code of test.py for easier reading as well:

```python
import numpy as np
import os
import psutil
import sys
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, StringLookup, Dense
from tensorflow.keras import Model


vocabulary = list('ABCDEFGHIJKLMN')
seq_length = 800000
batch_size = 16


def build_model():
    # Build a simple model with only the StringLookup and activation
    input_layer = Input(shape=(seq_length,), dtype='string', name='input')

    output_layer = StringLookup(
        vocabulary=vocabulary,
        mask_token='',
        output_mode='multi_hot',
        name='string_lookup'
    )(input_layer)
    
    output_layer = Dense(1, activation='sigmoid')(output_layer)
    return Model(inputs=input_layer, outputs=output_layer)


def train_model(model):
    # Train the model with batch_size x seq_length batches
    x_arr = np.array([np.random.choice(vocabulary, size=(seq_length,)) for _ in range(batch_size)])
    y_arr = np.random.choice([0, 1], size=(batch_size,))

    model = build_model()
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='AUC')
    model.fit(x_arr, y_arr, batch_size=1, epochs=1)

    return model


def save_model(model):
    # Save the model and print the saved model file size & memory usage
    tf.keras.models.save_model(model, 'models/test/')
    print(os.path.getsize('models/test/saved_model.pb'), psutil.Process().memory_info().rss)

    
def load_model():
    return tf.keras.models.load_model('models/test/')


if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == 'train':
        save_model(train_model(build_model()))
    else:
        save_model(load_model())
```",2022-05-13 07:47:01+00:00,"stat:awaiting response, type:support, comp:keras, TF 2.8"
[Feature Request] Default Keras callback for timing the training loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The current Keras progress bar callback displays training latency in time/step. The current recommendation is to disable the progress bar in production workflows.

This poses 2 problems:

1. The metric time/step is not batch size agnostic. While tuning batch size to fit the H/W, metrics represented in terms of samples allow for better head to head comparison. Currently, an additional step of converting step to samples is required.

2. Production workflows that require continuous training will benefit greatly from being able to monitor training latency. A deterioration in training speed could lead to spike in training costs.

I propose:

1. Reporting an additional metric for training latency based on samples.

2. A new verbosity setting that allows for reporting the training latency metrics without the progress bar. This could even be part of an existing verbosity.

I would be happy to create a PR for these changes.
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>",2022-05-16 02:19:24+00:00,"stat:awaiting response, type:feature, comp:keras"
How to reduce CPU usage of TensorFlowLite,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tflite 2.6.1

### Custom Code

No

### OS Platform and Distribution

yocto Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

3.7.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, I have a question about how to reduce CPU usage of TensorFlowLite.

I have 100% CPU utilization when running on ARM.
I want to lower it to about 50% because other processes will not work.
Is there a solution to this problem?
It is already running on 1 core, so the method of reducing the number of cores used will not solve the problem.
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>",2022-05-18 07:36:22+00:00,"stat:awaiting response, stale, comp:lite, type:performance, 2.6.0"
tensorflow 1 is much slower than 2 with tf.data.Dataset.from_tensor_slices,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 1.15.0 vs tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 16

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It was very slow when I used tf.data.Dataset.from_tensor_slices() in a tf1 environment to create a dataset, but very quickly with tf2. Whether this is an optimization of tf2. If you have to be in a tf1 environment, is there a good alternative?
```


### Standalone code to reproduce the issue

```shell
df = pd.read_csv(train_file, header=None, names=TRAIN_DATA_COLUMNS)
features = collections.OrderedDict()
for col in CONTINUOUS_COLUMNS:
    features[col] = df[col].to_numpy().astype(np.float32)
    print(features[col].dtype)

for col in CATEGORICAL_COLUMNS:
    features[col] = df[col].to_numpy()
    print(features[col].dtype)
labels = df[LABEL_COLUMN].to_numpy()
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
```


### Relevant log output

_No response_</details>",2022-05-24 02:09:22+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.8"
BUG: Memory leak bugs due to new reference passed to non-stealing APIs (static analyzer reports),"## Function `PyDict_SetItemString` and `PyDict_SetItem`

`PyDict_SetItemString` does not steal a reference from its third argument.
`PyDict_SetItem` does not steal a reference from its last two arguments.

* Internal Report ID: 7311bd 
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L145

* Internal Report ID: 0bf563
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L147

* Internal Report ID: 107a6c
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L149

* Internal Report ID: 336e4b
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L659

* Internal Report ID: f40c5f 
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L663

* Internal Report ID: 253804
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L233

* Internal Report ID: 9b7aa7 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1057

* Internal Report ID: 66dcb9
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1058

* Internal Report ID: 327452 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1093

* Internal Report ID: c80650
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1094

* Internal Report ID: 271ec8 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L141
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L161

* Internal Report ID: c41b64
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L650
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L666
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L667
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L668

* Internal Report ID: 7193e8 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L651
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L658
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L666

* Internal Report ID: 9dcdd9 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L652
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L662
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L667

## Other functions

* Internal Report ID: 6a12cf (`PyObject_GetAttr`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1018

* Internal Report ID: 918a9a (`PyBytes_AsString`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/lib/core/ndarray_tensor.cc#L90

* Internal Report ID: fffdc2 (`PyCapsule_GetPointer`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/lib/core/py_func.cc#L201

* Internal Report ID: 34df8e (`PyCapsule_GetPointer`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/lib/core/py_func.cc#L240 

* Internal Report ID: 506234 (`PyCapsule_GetPointer`)
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L3663
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L3666

* Internal Report ID: 9ede7a (`PyType_FromSpecWithBases`)
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/eee70a365f5deb9c78e63945756f856fb96847cb/tensorflow/python/eager/pywrap_tensor.cc#L1054
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/eee70a365f5deb9c78e63945756f856fb96847cb/tensorflow/python/eager/pywrap_tensor.cc#L1055
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/eee70a365f5deb9c78e63945756f856fb96847cb/tensorflow/python/eager/pywrap_tensor.cc#L1085


## Many usages as the `self` PyObject or a C pointer.

* Internal Report ID: 8235b2 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L276
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L277
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L278


* Internal Report ID: bf3201
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/python_api_parameter_converter_wrapper.cc#L33
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/python_api_parameter_converter_wrapper.cc#L34
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/python_api_parameter_converter_wrapper.cc#L35

* Internal Report ID: b665b7
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L505 
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L507
Other usages in the callee function `ParseStringValue`:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L324
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L327
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L332
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L334 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L343

* Internal Report ID: 3eadfd
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L715
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L717
Other usages in the callee function `ParseStringValue`: 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L324
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L327
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L332
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L334
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L343

* Internal Report ID: 9e7c5b
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L197
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L201
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L215
Other usages in the callee function `CallFunc`:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L158
",2022-05-24 07:56:14+00:00,"stat:awaiting response, type:bug, stale, comp:lite"
Memory leak in TfLiteModelCreateFromFile() in tflite v2.8.0 GNU Linux x86-64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc version 8.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I see memory leak from TfLiteModelCreateFromFile().
```


### Standalone code to reproduce the issue

```shell
When I compile below code
#############################
#include <stdio.h>
#include ""c_api.h""

int main()
{
    TfLiteModel* model = NULL;
    model = TfLiteModelCreateFromFile(""model.tflite"");
    if (model == NULL)
    {
        printf(""Model opening failed\n"");
    }
    TfLiteModelDelete(model);
        return 0;
}

#############################

using 
g++ app.c -I <path to c_api.h> -L <path to libtensorflowlite_c.so >-ltensorflowlite_c
```


### Relevant log output

```shell
valgrind --leak-check=full --track-origins=yes ./a.out

HEAP SUMMARY:
==24315==     in use at exit: 8 bytes in 1 blocks
==24315==   total heap usage: 6 allocs, 5 frees, 72,824 bytes allocated
==24315==
==24315== LEAK SUMMARY:
==24315==    definitely lost: 0 bytes in 0 blocks
==24315==    indirectly lost: 0 bytes in 0 blocks
==24315==      possibly lost: 0 bytes in 0 blocks
==24315==    still reachable: 8 bytes in 1 blocks
==24315==         suppressed: 0 bytes in 0 blocks


It looks like object StderrReporter is not freed.

 ErrorReporter* DefaultErrorReporter() {
  static StderrReporter* error_reporter = new StderrReporter;
  return error_reporter;
}
```
</details>",2022-05-24 08:15:42+00:00,"stat:awaiting tensorflower, comp:lite, type:performance, TF 2.8"
"Dataset from generator is far slower than from tensor slices, anything I can improve?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 1.15.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I use `tf.data.Dataset.from_tensor_slices` and `tf.data.Dataset.from_generator` to create the same dataset, I observe when fetching batches, from_generator is much slower than from_tensor_slices (more than 10x slower).
Also the performance of from_generator greatly drops when the number of yield elements increases. (See the details below).

I am using tf1.15, but I tested tf2.8, seems the performance of generator is even worse...

Wondering is it reasonable that Dataset from generator is far slower than from tensor slices? And want some help on how to improve the performance using the generator. If I want to have a dataset with 10 elements for each records, how can I get better performance?

Thanks so much in advance!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import time

tf.compat.v1.enable_eager_execution(
    config=None, device_policy=None, execution_mode=None
)

size = 100000
data = np.random.rand(size)

def get_one():
    i = 0
    while i < size:
        yield tuple([data[i]]*10)
        i += 1

# dataset = tf.data.Dataset.from_generator(get_one, output_types=tuple([tf.float32]*10))
dataset = tf.data.Dataset.from_tensor_slices(tuple([data]*10))
dataset = dataset.batch(512)

i = 0
total_time = 0
start = time.time()
for sample in dataset:
    # Performing a training step
    end = time.time()
    used = end - start
    total_time += used
    print(""Get batch time: "", used)
    i += 1
    start = time.time()
print(""Average get batch time: "", total_time / i)
```


### Relevant log output

When using from_tensor_slices, average get batch time:  0.001715s
When using from_generator, average get batch time:  0.10747s [0.14395s for tf2.8]

If I change the generator function as follows and re-run the above program:
```shell
def get_one():
    i = 0
    while i < size:
        for j in range(10):
            res = data[i]
        yield res
        i += 1
```
Namely if I get the value 10 times in the generator, but only yield one element, the performance is much faster compared with yield 10 features: average get batch time:  0.02014s, even though it is still much slower than from_tensor_slices. Why is that? I suppose the computation is the same, the only difference is how many elements I output? (BTW if I use tf2.8, in this case, it is 0.3935s, unreasonable... don't know why...)
",2022-05-25 13:12:18+00:00,"stat:awaiting response, comp:data, type:performance, TF 2.8"
New released protobuf +v3.20 causes an error when importing ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.7.1

### Custom Code

No

### OS Platform and Distribution

Linux 20.04

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After the new protobuf release and installing tensorflow an error appears when importing it.
https://pypi.org/project/protobuf/4.21.0/
```


### Standalone code to reproduce the issue

```shell
After installing tensorflow 2.7.1 running a simple `import tensorflow`
```


### Relevant log output

```shell
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).
```
</details>",2022-05-26 08:38:22+00:00,type:bug
Potential race condition in nsync library that causes SIGSEGV or deadlock,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.4

### Custom Code

No

### OS Platform and Distribution

Linux Redhat 7.6

### Mobile device

_No response_

### Python version

3.7

### Bazel version

3.1.0

### GCC/Compiler version

7

### CUDA/cuDNN version

11.0/8.0.2

### GPU model and memory

_No response_

### Current Behaviour?
Our TensorFlow application which uses [nsync](https://github.com/google/nsync) for mutex/cv implementation failed because of SIGSEGV. The issue can be reproduced during thread pool destruction with thread-local [TraceMeRecorder::ThreadLocalRecorderWrapper](https://github.com/tensorflow/tensorflow/blob/efa65ea9f1fa3d0baac82366f4405f2470797e04/tensorflow/core/profiler/backends/cpu/traceme_recorder.cc#L282) used in each thread. After adding logs into nsync library, we are able to identify the root cause.

#### Root cause
Destruction order of thread-local variables is not deterministic in [POSIX](https://linux.die.net/man/3/pthread_key_create). Mutex or conditional variable in TensorFlow is a thread local object. If another thread local object e.g. per-thread TraceMe recorder use a lock in its destructor, the lock can already be destructed and the behavior is undefined.

#### Details
For example, the following logs show two threads(0x7f4118748700 and 0x7f4100718700) share the same nsync waiter object (0x7f465c2c2290) and cause SIGSEGV. A nsync waiter object is a thread local object used by nsync to implement a lock. First, the waiter_destroy i.e. lock destructor was called on thread 0x7f4118748700 and the waiter 0x7f465c2c2290 had been recycled into free_waiters. At this moment, the other thread 0x7f4100718700 enter and take the waiter from free_waiters. Then, the old thread 0x7f4118748700 tries to take a lock again as the other thread-local object use a lock in its destructor. Because the thread-local state is not cleaned up in the old thread 0x7f4118748700, it assumes the waiter object is still reserved by itself. It ends up with two threads hold the same waiter object. The object was freed twice and failed the assertion in nsync_waiter_free_.

```shell
[nsync] waiter_destroy(internal/common.c#L150) tid 0x7f4118748700, w 0x7f465c2c2290
[nsync] nsync_waiter_new_(internal/common.c#L182) tid 0x7f4100718700, w 0x7f465c2c2290
[nsync] nsync_waiter_new_(internal/common.c#L171) tid 0x7f4118748700, tw 0x7f465c2c2290
[nsync] nsync_waiter_new_((internal/common.c#L206) tid 0x7f4100718700, w 0x7f465c2c2290
[nsync] nsync_mu_lock_slow_(internal/mu.c#L102) tid 0x7f4118748700, mu 0x12482b40, waiter 0x7f465c2c2290
[nsync] nsync_mu_lock_slow_(internal/mu.c#L71) tid 0x7f4100718700, mu 0x7f4ae4593f30, waiter 0x7f465c2c2290
[nsync] nsync_mu_lock_slow_(internal/mu.c#L71) tid 0x7f4118748700, mu 0x12482b40, waiter 0x7f465c2c2290
# A fatal error has been detected by the Java Runtime Environment:
#
# SIGSEGV (0xb) at pc=0x00007f4af0e74eda, pid=6, tid=0x00007f4118748700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_172-b11) (build 1.8.0_172-b11)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.172-b11 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C [pywrap_tensorflow_internal.so+0xc8cbeda] nsync::nsync_waiter_free(nsync::waiter*)+0xa
#
# Core dump written. Default location: /opt/code-fetcher-system/src/core or core.6
#
# An error report file with more information is saved as:
# /opt/code-fetcher-system/src/hs_err_pid6.log
```
Deadlock can also happen when the waiter is moved to another mutex before it got notified. For example, two threads above were locked on two different mutex (0x12482b40 and 0x7f4ae4593f30). In this case, the waiter semaphore is decremented twice but only incremented once.

P.S. The thread-local variable that access locks in its destructor in TensorFlow is the per-thread TraceMeRecorder::ThreadLocalRecorderWrapper used in TensorFlow profiler.

### Standalone code to reproduce the issue

N/A

The issue was fixed in nsync upstream. See [PR](https://github.com/google/nsync/pull/12) for more details. We need help from Google to release a new nsync version and upgrade nsync version in TensorFlow. Thank you so much!

### Relevant log output

_No response_</details>",2022-05-26 21:07:09+00:00,"type:bug, comp:core, TF 2.4, awaiting PR merge"
Leak when using Metal delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.3

### Custom Code

Yes

### OS Platform and Distribution

iOS 15.5

### Mobile device

iPad

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Whenever we enable Metal delegate, a fast memory leak is observed. Using just CoreML delegate, and the same exact app/code, the memory use is steady.

Most of the leaked memory looks like

_objc_rootAllocWithZone		
-[MTLDebugCommandBuffer computeCommandEncoder]		
0x102775ec8		
0x11379ec3c		
tflite::Subgraph::Invoke()		
tflite::Interpreter::Invoke()		
```

The app is a Swift app, that calls into native C/C++ code. Native C++ threads then run inference calling into TFLite C API.

Any insight into what can be tried is appreciated!
```


### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>",2022-05-27 15:59:17+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TF 2.7"
XLA performance drop when use --xla_dump_to  flag,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.0-rc0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Linux Ubuntu 18.04

### Python version

3.8.8

### Bazel version

5.0.0

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

11.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When use --xla_dump_to，the performance drop to 50% of original TensorFlow runtime

Environment: Tesla T4


1. python interpreter
Command: python3 test_mlp.py
Average time per iteration: 1.379ms

2. XLA auto clustering
Command: TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit"" python3 test_mlp.py
Average time per iteration: 1.395ms

3. XLA auto clustering with --xla_dump_to
Command: XLA_FLAGS=""--xla_dump_to=/tmp/test_mlp"" TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit"" python3 test_mlp.py
Average time per iteration: 2.52ms
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import time
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()

num_iter = 20
weights = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1), name=""weights"")
bias = tf.Variable(tf.random_normal([1, 3]), name=""bias"")
w = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))

x = tf.placeholder(tf.float32, shape=(None, 2), name='x')
labels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')

mul = tf.matmul(x, weights, name=""matmul"")
pred = tf.add(mul, bias, name=""add"")
logits = tf.nn.relu(pred, name=""Relu"")
loss = tf.nn.softmax_cross_entropy_with_logits(
             labels=labels, logits=logits, name=""cross_entropy""
             )
optimizer = tf.train.AdamOptimizer(1e-3)
opt = optimizer.minimize(loss)

bs = 20480
avg_time = 0
warmup = 2
with tf.Session() as sess:
    sess.run(tf.initializers.global_variables())
    for i in range(num_iter):
        start = time.time()
        _, res_loss = sess.run([opt, loss],
                feed_dict={x: np.random.rand(bs, 2),
                           labels: np.random.randint(0, 1, (bs, 1))})
        end = time.time()
        if i >= warmup:
            avg_time += end - start
        print(""loss: "", res_loss, "", duration: "", (end - start) * 1000, "" (ms)"")

    print(""avg duration: "", (avg_time / (num_iter - warmup)) * 1000, "" (ms)"")
```


### Relevant log output

```shell
# XLA_FLAGS=""--xla_dump_to=/tmp/test_mlp"" TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit"" python3 test_mlp.py
2022-06-14 16:03:10.484916: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From /root/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

2022-06-14 16:03:11.907873: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-14 16:03:13.115903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13793 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5
2022-06-14 16:03:13.117128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13793 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:af:00.0, compute capability: 7.5
2022-06-14 16:03:13.125430: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-06-14 16:03:13.397154: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8ad400de60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-14 16:03:13.397189: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2022-06-14 16:03:13.397195: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5
2022-06-14 16:03:13.549700: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-06-14 16:03:13.599047: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  1600.0151634216309  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  3.8709640502929688  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.8984546661376953  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.435445785522461  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.755403518676758  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.538919448852539  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.468109130859375  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.3980140686035156  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.3653507232666016  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.3200511932373047  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.5146007537841797  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.523183822631836  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.428770065307617  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.515077590942383  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.5157928466796875  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.872943878173828  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.4328231811523438  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.4433135986328125  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.4378299713134766  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.5124549865722656  (ms)
avg duration:  2.520918846130371  (ms)
```
</details>",2022-06-14 08:04:06+00:00,"stat:awaiting tensorflower, type:bug, comp:xla, TF 2.9"
Estimator Training Data Reading Incorrect Values,"  ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.5+

### Custom Code

No

### OS Platform and Distribution

Mac and Linux

### Python version

3.8


### Current Behaviour?

Starting with tensorflow 2.5, I've encountered my training pipeline having wrong values for certain intermediate calculation. It looks like the data pipeline is reading wrong value in tf.train.Example. I made toy dataset with 1 feature (all -5) and 1 label (always 1) and I print the label, but instead it shows value of the feature.

I've been able to reproduce it down to small estimator training loop although it's still ~50 lines. The model here is trivial fake model as focus is on the value of label tensor passed to model_fn. Dataset pipeline is short but still several steps. If I remove some steps the bug disappears. Also batching uses,

`dataset = dataset.batch(batch_size, drop_remainder=True)`

The drop_remainder is required for the bug. Changing that somehow changes how the data is read.

In the model_fn, minor what should be equivalent variations of this line effect whether bug happens.

`tf.stack([features[key] for key in label_names])`

It's also not specific only to tf.stack. tf.reduce_mean also leads to negative values even though features[key] is always positive. One key element is that this list has same tensor multiple times because label is repeated across different model heads. If label_names is unique the bug disappears.

I've tested this on 2.4, 2.5, 2.6, 2.7, 2.8, 2.9. All of 2.5+ consistently show same wrong behavior for value of intermediate printed. I am working on bisecting to exact commit, but building tensorflow is slow. So far I've narrowed it down to bug being introduced in between March 18 commit [fd020fff99a0](https://github.com/tensorflow/tensorflow/commit/fd020fff99a0ad5e0293e43a77df54dd9efedce7) and March 24 commit [83a8259081](https://github.com/tensorflow/tensorflow/commits/b9e31e669e454b185a100802b5d6cb1a6f59d74b?before=b9e31e669e454b185a100802b5d6cb1a6f59d74b+140&branch=b9e31e669e454b185a100802b5d6cb1a6f59d74b&qualified_name=b9e31e669e454b185a100802b5d6cb1a6f59d74b).

I've also tested on both my laptop (mac) and a linux gcp VM. All tests were done with CPU only. They show same behavior.

My suspicion is some memory aliasing is happening when non unique tensor appears for some of these operations. I'm confused why this is estimator specific. Maybe it's possible to reproduce this without estimator, but I've been unsuccessful with that.


### Standalone code to reproduce the issue

Here's a colab link, https://colab.research.google.com/drive/11-7vyU4cc_3BGXudUEuT12bbVS-JJQPH?usp=sharing It is fully self contained with only dependency being tensorflow. If you run notebook in different tensorflow versions <=2.4 vs >=2.5 it produces different behavior.


### Relevant log output

If you run notebook on a good tensorflow version (2.4) you will see 1 for label. If you run it on any bad version (2.5+) -5 is result.
",2022-06-16 01:53:45+00:00,"stat:awaiting response, type:bug, stale, comp:ops, TF 2.9"
[RNN] LSTM after conversion to TFLite not run on GPU. ,"### 1. System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow 2.5, tensorflow 2.6, tensorflow 2.9, tf-nightly
- Mobile device: Huawei Mate 10 Pro with Android 10.


### 2. Code
Provide code to help us reproduce your issues using one of the following options:
You can see the attched files. 


### 3. Failure after conversion
In most cases I can complete the conversion to TFLite, with and without Quantization.


### 4. Any other info / logs
The problem exists after the conversion; I am currently unable to understand if LSTM or GRU layers are actually supported on GPU and NNAPI_GPU Delegates. I test my models using the BenchmarkTools through the Andoid apk, setting the --use_gpu or use_nnapi flags to True. 

From my tests, I was able to find two different cases:

1) When I set in conversion as supported_ops  tf.lite.OpsSet.TFLITE_BUILTINS e tf.lite.OpsSet.SELECT_TF_OPS which leads to 

UNPACK: Operation is not supported.
06-16 17:31:52.176 27077 27077 E tflite  : 294 operations will run on the GPU, and the remaining 296 operations will run on the CPU.
06-16 17:31:52.177 27077 27077 I tflite  : Replacing 294 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions.
06-16 17:31:53.098 27077 27077 I tflite  : Initialized OpenCL-based API.
06-16 17:31:53.242 27077 27077 I tflite  : Created 1 GPU delegate kernels.
06-16 17:31:53.245 27077 27077 I tflite  : Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.

2) When I set only tf.lite.OpsSet.SELECT_TF_OPS in supported_ops, which leads to
Created TensorFlow Lite delegate for select TF ops.
06-16 17:39:13.016 27417 27417 I tflite  : TfLiteFlexDelegate delegate: 609 nodes delegated out of 609 nodes with 1 partitions.
06-16 17:39:13.016 27417 27417 I tflite  : Replacing 609 node(s) with delegate (TfLiteFlexDelegate) node, yielding 1 partitions.
06-16 17:39:13.152 27417 27417 I tflite  : Created TensorFlow Lite delegate for GPU.
06-16 17:39:13.152 27417 27417 I tflite  : GPU delegate created.
06-16 17:39:13.178 27417 27417 E tflite  : Following operations are not supported by GPU delegate:
06-16 17:39:13.178 27417 27417 E tflite  : DELEGATE TfLiteFlexDelegate: Operation is not supported.
06-16 17:39:13.178 27417 27417 E tflite  : No operations will run on the GPU, and all 1 operations will run on the CPU.
06-16 17:39:13.178 27417 27417 I tflite  : Created 0 GPU delegate kernels.
06-16 17:39:13.198 27417 27417 I tflite  : Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.

In any case, however, I note that indeed, the values for the Inference (avg) time are extremely higher than in the case in which I use the CPU directly, without either of the two delegates.
This is probably due to a Fallback to CPU, which results in worse performance.

I have tested all possible settings configurations in conversion as you can see below, even with tensorflow 2.9 and tf-nightly, but I couldn't do better.
   
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])  
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_data_gen
    converter.experimental_new_converter = True
    converter.experimental_new_quantizer = True
    converter.experimental_enable_resource_variables = True
    converter.allow_custom_ops = True
    converter.target_spec.supported_types = [tf.float16]
tf.lite.OpsSet.TFLITE_BUILTINS]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.inference_input_type = [tf.float32]
    converter.inference_output_type = [tf.float32]
    tflite_quantized_model = converter.convert()
    

The ops that cause problems during the performance of the Benchmark are as follows:

06-16 17:31:54.781 27077 27077 I tflite  :                       RESHAPE            0.019           0.017         0.112%          0.112%         0.000              1       [sequential/lstm/transpose1]:0
06-16 17:31:54.781 27077 27077 I tflite  :                        UNPACK            0.032           0.029         0.199%          0.311%         0.000              1       [sequential/lstm/unstack, sequential/lstm/unstack1, sequential/lstm/unstack2, sequential/lstm/unstack3, sequential/lstm/unstack4, sequential/lstm/unstack5, sequential/lstm/unstack6, sequential/lstm/unstack7, sequential/lstm/unstack8, sequential/lstm/unstack9, sequential/lstm/unstack10, sequential/lstm/unstack11, sequential/lstm/unstack12, sequential/lstm/unstack13, sequential/lstm/unstack14, sequential/lstm/unstack15, sequential/lstm/unstack16, sequential/lstm/unstack17, sequential/lstm/unstack18, sequential/lstm/unstack19, sequential/lstm/unstack20]:1

although as indicated in this page the unpack operator is supported through TF Ops
https://www.tensorflow.org/lite/guide/op_select_allowlist

and the Reshape Layer is supported on GPU as here
https://www.tensorflow.org/lite/performance/gpu_advanced

Also, on this page it is indicated that LSTM is supported on GPU with float16, but also specifying the type in the conevrsion
converter.target_spec.supported_types = [tf.float16]

Also, on this page
https://www.tensorflow.org/lite/performance/gpu_advanced
it is indicated that LSTM is supported on GPU with float16

converter.target_spec.supported_types = [tf.float16]

Also specifying the type in the conversion, I didn't get any changes.


Basically, can you tell me if RNN can run on GPU and NNAPI-GPU on TFLite. If so can you provide me with support to do so. I have already taken a long time, with no solution or support.",2022-06-16 16:03:57+00:00,"stat:awaiting response, stale, comp:lite, type:performance, TFLiteGpuDelegate, TFLiteNNAPIDelegate, TF 2.9"
[TF 1.x] Cannot print tensors whose operations have been executed from modules in anotehr file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.2.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.01

### Mobile device

_No response_

### Python version

3.6.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?
Apparently, using TF `1.x` modules imported from another files output tensors which can't be `tf.Print`ed. Everything uses `compat.v1`, base version specified above.

- There's **no** `stdout`, and scripts are executed from the terminal so `stderr` should be visible. The corresponding string 'message' alongside with `tf.print`/`tf.Print` command is also not printed

- with no `stdout`, the program would simply *hang* until CTRL+Z killed. Using `sess.close()` and other variations do not help at all.

### Standalone code to reproduce the issue

This is a snippet of the code I'm using:

```py
import batching #module from another file
import nls #same as above

sess = tf.compat.v1.Session()

with sess.as_default():
    #Executing custom ops
    data_obj = nls.MyDataset('train')
    inp, out = batching.batch_inputs(data_obj, train=False, batch_size=1, num_preprocess_threads=8, num_readers=4)
    #ops computation finished

    a = out[0] #out is a LIST of 4 tensors - none of them can be printed
    print_op = tf.print(""\n---\ntensors:"", a, output_stream=sys.stdout)
    
    with tf.control_dependencies([print_op]): #dummy operation to maintain graph
      d_tensor = a * 1
    sess.run(d_tensor)

sess.close() #This seems to be largely irrelevant, changing no behavior whatsoever
```

vanilla `print()`-ing tensors leads to their shapes and types, but not the contents. So the tensors are well formed atleast - just not accessible for some reason.

These computations aren't intermediary between layers - its simply vanilla operations to *preprocess* tensors from a `TFRecord` and output the data for future training in the model. That stage hasn't been implemented yet.


### Relevant log output

```shell
2022-06-21 12:43:08.150767: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
/opt/awesome/data/sample_tfrecs IN DATA_FILES
Glob Files Done...
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/batching.py:156: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
using non random shuffle queue
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/dataset.py:80: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/data_providers/nexar_large_speed.py:817: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/batching.py:242: batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
4  batch_joins, each of them capacity is,  2  instances  Warning: using this might be quite slow!
buffer queue capacity is:  4  batches

---------------------------
Freezes here, no further stdout/stderr - nor the corresponding message with tf.print
---------------------------
```
</details>",2022-06-21 12:51:49+00:00,"stat:awaiting response, type:support, stale, comp:ops, TF 2.2"
Poor performance of tensorflow higher versions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.4.1, 2.3.1, 2.2.0, 2.1.0, 2.0.0, 1.15.2 , 1.14.0, 1.13.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.7.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?
The detailed experiment results is as follows:
Version | Memory 
-- | -- 
2.7.0 | 142
2.4.1 | 118
2.3.1 | 114
2.2.0 | 108
2.1.0 | 110
1.15.2 | 106
1.14.0 | 104
1.13.1 | 92

According to the above experiment results, when TensorFlow version is higher than 1.13.1, the performance of the model is worse.


### Standalone code to reproduce the issue
[train.csv](https://github.com/tensorflow/tensorflow/files/8997439/train.csv)
[test.csv](https://github.com/tensorflow/tensorflow/files/8997441/test.csv)
```shell
import numpy as np 
import pandas as pd 
import tracemalloc
tracemalloc.start()
train = pd.read_csv(""train.csv"")
test = pd.read_csv(""test.csv"")

train_labels = train.iloc[:,-3:]
features = pd.concat([train.iloc[:,1:-3], test.iloc[:,1:]])

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_features =scaler.fit_transform(features)
scaled_train = scaled_features[:train.shape[0]]
scaled_test = scaled_features[train.shape[0]:]
print(scaled_train.shape, scaled_test.shape)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_train, train_labels, random_state = 42, test_size = 0.3)
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
    layers.Dense(16, activation='relu', input_shape=[8]),
    layers.Dense(64, activation='relu'),
    layers.Dense(64, activation = 'relu'),
    layers.Dense(32, activation = 'relu'),
    layers.Dense(3),
])
model.compile( optimizer = ""adam"", loss = ""mae"")
history = model.fit( x_train, y_train, validation_data=(x_test, y_test), batch_size = 256, epochs = 50, verbose = False)
hstry_df = pd.DataFrame(history.history)

submissions = model.predict(scaled_test)

submission_df = pd.DataFrame(submissions, columns = train_labels.columns)
submission_df['date_time'] = test['date_time']
submission_df.to_csv(""submissions.csv"", index=False)


current3, peak3 = tracemalloc.get_traced_memory()
print(""Get_dummies memory usage is {"",current3 /1024/1024,""}MB; Peak memory was :{"",peak3 / 1024/1024,""}MB"")
```


### Relevant log output

_No response_</details>",2022-06-28 03:31:11+00:00,"stat:awaiting response, stale, comp:apis, type:performance, TF 2.7"
Can we get PocketFFT ported to Tensorflow?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Is it possible to integrate PocketFFT when using a CPU into Tensorflow, which functions like tf.signal.stft and tf.signal.inverse_stft can leverage? 

Currently, Tensorflow's FFT uses EigenFFT, which is almost 3x slower than Numpy and Jax, which use PocketFFT.

There are heaps more details here: https://github.com/tensorflow/tensorflow/issues/6541

I'm sure many projects would benefit from this investment considering much of what's done for speech and music these days use STFT data.
```


### Standalone code to reproduce the issue

```shell
print(""seconds (lower is better):"")
print(f""Tensorflow {tf.__version__}"", timeit.timeit('X = tf.signal.rfft(x)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))
print(f""Tensorflow {tf.__version__}, double precision"", timeit.timeit('X = tf.cast(tf.signal.rfft(tf.cast(x, tf.float64)), tf.complex64)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))
print(""Numpy: "", timeit.timeit('X = numpy.fft.rfft(x)', setup='import numpy.fft; import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))
print(""Jax: "", timeit.timeit('jnp.fft.rfft(x).block_until_ready()', setup='import jax.numpy as jnp; import tensorflow as tf; x = tf.random.normal([50000, 512]).numpy()', number=10))
```


### Relevant log output

```shell
seconds (lower is better):
Tensorflow 2.9.1 5.495112890999991
Tensorflow 2.9.1, double precision 7.629201937000033
Numpy:  2.1803204349999987
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
Jax:  1.4081462569999985
```
</details>",2022-07-06 02:42:38+00:00,"stat:awaiting tensorflower, type:feature, comp:apis, comp:ops, TF 2.9"
Int-8 converter scales static model parameters incorrectly,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1 (tensorflow-estimator 2.9.0)

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

https://colab.research.google.com/drive/1HN3tZCEeK79YPFT85udXf7FybBo6-PYf

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

After INT-8 conversion, the models parameters are scaled incorrectly (and unnecessarily) and model performance drops significantly.
The colab notebook compares a default conversion and a 8-bit quantized conversion and shows that the output changes and difference between classes can no longer be made. The same code produces the same issues with the upper mentioned TF installation and in the default colab notebook configuration.

The model consists of a simple custom layer that compares the input bounding box with an internal hardcoded bounding box. Only minimum, maximum and subtract operations are used. 
When converting the Keras sequential model to a default .tflite model (not quantized), the output is exactly as expected. Converting it with 8-bit quantization the model parameters are scaled in a way that clear, non-borderline cases are predicted incorrectly. 

I understand that scaling might be necessary for large models with diverse inputs, but in this case I would like have full control over what changes are made to the model during conversion, since it is a very simple model and the possible input is very predictable and narrowly defined.
Inputs and model parameters both range from 0-100, for which no scaling is needed and no overflow could possibly happen. This is (I hope) reflected in my representative dataset, which seems to be a necessary input for 8-bit integer quantization.

I do not mind the exact numerical output of the model, but the output of the model should be different for overlapping and non-overlapping bounding boxes, which is only given for the default converted tflite model.

To sum up, how can I convert or write a model for 8bit integers, in a way that I have direct control over the model parameters.
",2022-07-12 15:00:03+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TFLiteConverter, ModelOptimizationToolkit, TF 2.11"
"label_image:undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.1.1

### GCC/Compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I built the label_image example, it just didn't work like:
[1/1] Linking CXX executable test_tensorflow
FAILED: test_tensorflow 
: && /usr/bin/c++ -O3 -DNDEBUG  CMakeFiles/test_tensorflow.dir/main.cpp.o -o test_tensorflow -L/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow   -L/usr/local/lib -Wl,-rpath,/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow:/usr/local/lib:/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow/lite  /usr/local/lib/libabsl_bad_any_cast_impl.a  /usr/local/lib/libabsl_bad_optional_access.a  /usr/local/lib/libabsl_bad_variant_access.a  /usr/local/lib/libabsl_base.a  /usr/local/lib/libabsl_city.a  /usr/local/lib/libabsl_civil_time.a  /usr/local/lib/libabsl_cord.a  /usr/local/lib/libabsl_cord_internal.a  /usr/local/lib/libabsl_cordz_functions.a  /usr/local/lib/libabsl_cordz_handle.a  /usr/local/lib/libabsl_cordz_info.a  /usr/local/lib/libabsl_cordz_sample_token.a  /usr/local/lib/libabsl_debugging_internal.a  /usr/local/lib/libabsl_demangle_internal.a  /usr/local/lib/libabsl_examine_stack.a  /usr/local/lib/libabsl_exponential_biased.a  /usr/local/lib/libabsl_failure_signal_handler.a  /usr/local/lib/libabsl_flags.a  /usr/local/lib/libabsl_flags_commandlineflag.a  /usr/local/lib/libabsl_flags_commandlineflag_internal.a  /usr/local/lib/libabsl_flags_config.a  /usr/local/lib/libabsl_flags_internal.a  /usr/local/lib/libabsl_flags_marshalling.a  /usr/local/lib/libabsl_flags_parse.a  /usr/local/lib/libabsl_flags_private_handle_accessor.a  /usr/local/lib/libabsl_flags_program_name.a  /usr/local/lib/libabsl_flags_reflection.a  /usr/local/lib/libabsl_flags_usage.a  /usr/local/lib/libabsl_flags_usage_internal.a  /usr/local/lib/libabsl_graphcycles_internal.a  /usr/local/lib/libabsl_hash.a  /usr/local/lib/libabsl_hashtablez_sampler.a  /usr/local/lib/libabsl_int128.a  /usr/local/lib/libabsl_leak_check.a  /usr/local/lib/libabsl_leak_check_disable.a  /usr/local/lib/libabsl_log_severity.a  /usr/local/lib/libabsl_low_level_hash.a  /usr/local/lib/libabsl_malloc_internal.a  /usr/local/lib/libabsl_periodic_sampler.a  /usr/local/lib/libabsl_random_distributions.a  /usr/local/lib/libabsl_random_internal_distribution_test_util.a  /usr/local/lib/libabsl_random_internal_platform.a  /usr/local/lib/libabsl_random_internal_pool_urbg.a  /usr/local/lib/libabsl_random_internal_randen.a  /usr/local/lib/libabsl_random_internal_randen_hwaes.a  /usr/local/lib/libabsl_random_internal_randen_hwaes_impl.a  /usr/local/lib/libabsl_random_internal_randen_slow.a  /usr/local/lib/libabsl_random_internal_seed_material.a  /usr/local/lib/libabsl_random_seed_gen_exception.a  /usr/local/lib/libabsl_random_seed_sequences.a  /usr/local/lib/libabsl_raw_hash_set.a  /usr/local/lib/libabsl_raw_logging_internal.a  /usr/local/lib/libabsl_scoped_set_env.a  /usr/local/lib/libabsl_spinlock_wait.a  /usr/local/lib/libabsl_stacktrace.a  /usr/local/lib/libabsl_status.a  /usr/local/lib/libabsl_statusor.a  /usr/local/lib/libabsl_str_format_internal.a  /usr/local/lib/libabsl_strerror.a  /usr/local/lib/libabsl_strings.a  /usr/local/lib/libabsl_strings_internal.a  /usr/local/lib/libabsl_symbolize.a  /usr/local/lib/libabsl_synchronization.a  /usr/local/lib/libabsl_throw_delegate.a  /usr/local/lib/libabsl_time.a  /usr/local/lib/libabsl_time_zone.a  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_cc.so  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_framework.so.2  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//lite/libtensorflowlite.so && :
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':
main.cpp:(.text+0x2b7b): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bc1): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bf3): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':
main.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x3b4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long)':
main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x96b): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x98a): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `main':
main.cpp:(.text.startup+0x6ab): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text.startup+0x87f): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```


### Standalone code to reproduce the issue

```shell
if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str(), file_name.size()),
                                       tensorflow::StringPiece("".png"", std::string("".png"").size()))) {
        image_reader = DecodePng(root.WithOpName(""png_reader""), file_reader,
                                 DecodePng::Channels(wanted_channels));
    } else if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str()),
                                              tensorflow::StringPiece("".gif""))) {
        // gif decoder returns 4-D tensor, remove the first dim
        image_reader =
                Squeeze(root.WithOpName(""squeeze_first_dim""),
                        DecodeGif(root.WithOpName(""gif_reader""), file_reader));
    } else if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str()),
                                              tensorflow::StringPiece("".bmp""))) {
        image_reader = DecodeBmp(root.WithOpName(""bmp_reader""), file_reader);
    } else {
        // Assume if it's neither a PNG nor a GIF then it must be a JPEG.
        image_reader = DecodeJpeg(root.WithOpName(""jpeg_reader""), file_reader,
                                  DecodeJpeg::Channels(wanted_channels));
    }
```


### Relevant log output

```shell
[1/1] Linking CXX executable test_tensorflow
FAILED: test_tensorflow 
: && /usr/bin/c++ -O3 -DNDEBUG  CMakeFiles/test_tensorflow.dir/main.cpp.o -o test_tensorflow -L/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow   -L/usr/local/lib -Wl,-rpath,/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow:/usr/local/lib:/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow/lite  /usr/local/lib/libabsl_bad_any_cast_impl.a  /usr/local/lib/libabsl_bad_optional_access.a  /usr/local/lib/libabsl_bad_variant_access.a  /usr/local/lib/libabsl_base.a  /usr/local/lib/libabsl_city.a  /usr/local/lib/libabsl_civil_time.a  /usr/local/lib/libabsl_cord.a  /usr/local/lib/libabsl_cord_internal.a  /usr/local/lib/libabsl_cordz_functions.a  /usr/local/lib/libabsl_cordz_handle.a  /usr/local/lib/libabsl_cordz_info.a  /usr/local/lib/libabsl_cordz_sample_token.a  /usr/local/lib/libabsl_debugging_internal.a  /usr/local/lib/libabsl_demangle_internal.a  /usr/local/lib/libabsl_examine_stack.a  /usr/local/lib/libabsl_exponential_biased.a  /usr/local/lib/libabsl_failure_signal_handler.a  /usr/local/lib/libabsl_flags.a  /usr/local/lib/libabsl_flags_commandlineflag.a  /usr/local/lib/libabsl_flags_commandlineflag_internal.a  /usr/local/lib/libabsl_flags_config.a  /usr/local/lib/libabsl_flags_internal.a  /usr/local/lib/libabsl_flags_marshalling.a  /usr/local/lib/libabsl_flags_parse.a  /usr/local/lib/libabsl_flags_private_handle_accessor.a  /usr/local/lib/libabsl_flags_program_name.a  /usr/local/lib/libabsl_flags_reflection.a  /usr/local/lib/libabsl_flags_usage.a  /usr/local/lib/libabsl_flags_usage_internal.a  /usr/local/lib/libabsl_graphcycles_internal.a  /usr/local/lib/libabsl_hash.a  /usr/local/lib/libabsl_hashtablez_sampler.a  /usr/local/lib/libabsl_int128.a  /usr/local/lib/libabsl_leak_check.a  /usr/local/lib/libabsl_leak_check_disable.a  /usr/local/lib/libabsl_log_severity.a  /usr/local/lib/libabsl_low_level_hash.a  /usr/local/lib/libabsl_malloc_internal.a  /usr/local/lib/libabsl_periodic_sampler.a  /usr/local/lib/libabsl_random_distributions.a  /usr/local/lib/libabsl_random_internal_distribution_test_util.a  /usr/local/lib/libabsl_random_internal_platform.a  /usr/local/lib/libabsl_random_internal_pool_urbg.a  /usr/local/lib/libabsl_random_internal_randen.a  /usr/local/lib/libabsl_random_internal_randen_hwaes.a  /usr/local/lib/libabsl_random_internal_randen_hwaes_impl.a  /usr/local/lib/libabsl_random_internal_randen_slow.a  /usr/local/lib/libabsl_random_internal_seed_material.a  /usr/local/lib/libabsl_random_seed_gen_exception.a  /usr/local/lib/libabsl_random_seed_sequences.a  /usr/local/lib/libabsl_raw_hash_set.a  /usr/local/lib/libabsl_raw_logging_internal.a  /usr/local/lib/libabsl_scoped_set_env.a  /usr/local/lib/libabsl_spinlock_wait.a  /usr/local/lib/libabsl_stacktrace.a  /usr/local/lib/libabsl_status.a  /usr/local/lib/libabsl_statusor.a  /usr/local/lib/libabsl_str_format_internal.a  /usr/local/lib/libabsl_strerror.a  /usr/local/lib/libabsl_strings.a  /usr/local/lib/libabsl_strings_internal.a  /usr/local/lib/libabsl_symbolize.a  /usr/local/lib/libabsl_synchronization.a  /usr/local/lib/libabsl_throw_delegate.a  /usr/local/lib/libabsl_time.a  /usr/local/lib/libabsl_time_zone.a  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_cc.so  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_framework.so.2  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//lite/libtensorflowlite.so && :
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':
main.cpp:(.text+0x2b7b): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bc1): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bf3): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':
main.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x3b4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long)':
main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x96b): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x98a): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `main':
main.cpp:(.text.startup+0x6ab): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text.startup+0x87f): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```
</details>",2022-07-14 01:40:54+00:00,"stat:awaiting response, type:bug, comp:runtime, TF 2.9"
GPU much slower than CPU for text processing using tensorflow-macos,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

TF 2.8

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run the notebook Text classification with an RNN https://www.tensorflow.org/text/tutorials/text_classification_rnn from the TensorFlow website.
The code has LSTM and Bidirectional layers

Although I get the message ""Plugin optimizer for device_type GPU is enabled"", the GPU is not used.

When the GPU is “enabled”, although not used,  the time is 56 minutes/epoch.
When I am only using the CPU is 264 seconds/epoch.

I face the same issue when trying to run Transformer model for language understanding https://www.tensorflow.org/text/tutorials/transformer

I am using MacBook Pro 14 (10 CPU cores, 16 GPU cores) and TensorFlow-macos 2.8 with TensorFlow-metal 0.5.0.  I face the same problem for TensorFlow-macos 2.9.2 too.

My environment has:
tensorflow-macos          2.8.0  
tensorflow-metal          0.5.0 
tensorflow-text           2.8.1  
tensorflow-datasets       4.6.0                   
tensorflow-deps           2.8.0                         
tensorflow-hub            0.12.0                      
tensorflow-metadata       1.8.0                    
                   
When I am using CNNs the GPU is fully enabled and 3-4 times faster than when only using the CPU. 
```


### Standalone code to reproduce the issue

```shell
The codes that I have observed the issue are from the TensorFlow website:
https://www.tensorflow.org/text/tutorials/text_classification_rnn
and
https://www.tensorflow.org/text/tutorials/transformer
```


### Relevant log output

_No response_</details>",2022-07-15 02:03:38+00:00,"stat:awaiting response, stale, comp:gpu, type:performance, TF 2.8"
(CONCATENATION) failed to prepare,"### 1. System information

- OS Platform and Distribution: MacOS Monterey 12.4
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1
- Python: 3.7.10

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
converted checkpoint: https://drive.google.com/file/d/1iWPcJ3wC2xV-xz3lIiGQrgteGXZOLt5P/view?usp=sharing

```
interpreter = tf.lite.Interpreter(model_path=tflite_path)
interpreter.allocate_tensors()
input_data = np.ones([1, 3, 416, 416], dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs

When model is converted:

> 2022-07-18 10:49:42.054397: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1901] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
> Flex ops: FlexConv2D, FlexRange
> Details:
> 	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x12x24xf32>) -> (tensor<?x?x?x24xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
> 	tf.Range(tensor<i64>, tensor<i64>, tensor<i64>) -> (tensor<?xi64>) : {device = """"}
> See instructions: https://www.tensorflow.org/lite/guide/ops_select

Model fails on `invoke` with:

> INFO: Created TensorFlow Lite delegate for select TF ops.
> 2022-07-18 10:28:53.074011: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> INFO: TfLiteFlexDelegate delegate: 13 nodes delegated out of 797 nodes with 3 partitions.
> RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (9 != 13)Node number 250 (CONCATENATION) failed to prepare.

",2022-07-18 08:54:08+00:00,"stat:awaiting tensorflower, type:bug, comp:lite, TFLiteConverter, TF 2.9"
Different result in tflite conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0

### 2. Code


![image](https://user-images.githubusercontent.com/28351316/179805179-ed562a87-0bbd-4ca7-9e35-74583cc0b2f8.png)








I use the function above to convert TF fake quantize model to 8 bit tflite model. When I set time=True, which means input shape equals [1, 360, 640, 3], the tflite model in netron is below. It's the right model I want.

![right](https://user-images.githubusercontent.com/28351316/179800350-1673c16d-9491-4d84-ae72-fb09dc626876.png)


But when I set input shape=[1,None,None,3], the converted tflite shows below. It's strange that there are some unexpected op above bilinear upsample op. I found a model performance degradation when I convert the tflite model below, So I want to know how the difference comes. Thanks!

![wrong](https://user-images.githubusercontent.com/28351316/179800787-8103799c-1be3-4378-8b0c-915dfdb4688c.png)


",2022-07-19 16:34:50+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TFLiteConverter, TF 2.5"
[Feature Request] GELU activation with the Hexagon delegate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.9.1

I think I'd be able to implement this myself, but wanted to see if there was any interest in including this upstream.  Most of this I'm writing out to make sure my own understanding is correct.

### The problem

I'd like to add support for the GELU op to the Hexagon Delegate.  The motivation for this is mostly for use with [DistilBERT](https://huggingface.co/distilbert-base-multilingual-cased), which uses this activation function in its feedforward network layers.  (Also used by BERT, GPT-3, RoBERTa, etc.)

Adding this as a supported op for the Hexagon delegate would avoid creating a graph partition/transferring between DSP<-->CPU each time the GELU activation function is used.

### How I'd implement this

GELU in TF Lite is implemented as a lookup table when there are integer inputs ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc#L120-L140) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/gelu.h#L37-L53)).

This same approach could be used for the Hexagon delegate, as it has int8/uint8 data types and also supports lookup tables.

I'd plan to do this by adding a new op builder in the delegate, populating a lookup table for each node as is currently done for the CPU version of the op, and then using the [Gather_8](https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/hexagon/ops/src/op_gather.c)  nnlib library function to do the lookup.

### Possible workaround

A workaround I thought of:

I'm going to try removing the [pattern matching](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td#L1034-L1095) for approximate GELU in MLIR, and then using the approximate version of GELU (so that using tanh and not Erf).  This will probably be slower, but should let me keep execution on the DSP.

Since this will then be tanh, addition, multiplication ops instead of GELU they should all be runnable by the DSP.",2022-07-21 01:07:38+00:00,"stat:awaiting response, type:feature, stale, comp:lite, TFLiteHexagonDelegate"
TensorflowLite model run on Hexagon DSP different from CPU,"Hi, I convert a ""resnet-like"" .tflite model(quantized by int8) and find it has a certain precision loss on hexagon compared to cpu,  then i print the output probability,  they are really different.
I used to discover the ""inference_diff"" tool (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) that you guys provided and now test with it, the result is as follows:
---------------------------------------------------------------------------------------------------------------------------
```
./run_eval --model_file=resnet_quantized.tflite --delegate=hexagon                                                                                             
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
Hexagon delegate created.
INFO: TfLiteHexagonDelegate delegate: 34 nodes delegated out of 36 nodes with 1 partitions.
VERBOSE: Replacing 34 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 3 partitions.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:78 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:146 
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:92 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 50
Reference run latency: avg=240374(us), std_dev=142248(us)
Test run latency: avg=19779(us), std_dev=5008(us)
OutputDiff[0]: avg_error=0.000274915, std_dev=4.89316e-05
```
----------------------------------------------------------------------------------------------------------------------------
It looks like little difference, but in general, the output probability of the quantized model are mostly 0, so this probably doesn't mean much. Then I remove the softmax operation at the end of the model, and try again:
----------------------------------------------------------------------------------------------------------------------------
```
./run_eval --model_file=resnet_quantized_without_softmax.tflite --delegate=hexagon                                                                             
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
Hexagon delegate created.
INFO: TfLiteHexagonDelegate delegate: 33 nodes delegated out of 35 nodes with 1 partitions.
VERBOSE: Replacing 33 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 3 partitions.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:78 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:146 
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:92 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 50
Reference run latency: avg=234793(us), std_dev=146766(us)
Test run latency: avg=19949.9(us), std_dev=5333(us)
OutputDiff[0]: avg_error=2.66955, std_dev=0.73623
```
----------------------------------------------------------------------------------------------------------------------------
Now the difference is much bigger. I wonder whether these are normal, and what is the approximate level of accuracy loss of the model generally. I upload these two models which named resnet_quantized.tflite.tar.gz and resnet_quantized_without_softmax.tflite.
Thanks [!](url)
[resnet_quantized.tflite.tar.gz](https://github.com/tensorflow/tensorflow/files/9157839/resnet_quantized.tflite.tar.gz)
[resnet_quantized_without_softmax.tflite.tar.gz](https://github.com/tensorflow/tensorflow/files/9157842/resnet_quantized_without_softmax.tflite.tar.gz)

",2022-07-21 09:01:09+00:00,"stat:awaiting response, stale, comp:lite, type:performance, comp:lite-xnnpack, TF 2.9"
Bilinear upsampling layer in tflite cannot be 8-bit quantized correctly,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- TensorFlow installation (pip package or built from source):  pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0
- TensorFlow Model Optimization version (installed from source or binary): pip  0.7.2

### Abstract
Hello, I want to do full 8-bit quantization(input, weight all 8-bit) to the network with a bilinear upsampling layer. The fake QAT result in validation set is closed to FP32 result, but when I converted the QAT model to full 8-bit tflite model, the result in validation set decreased significantly, almost no accuracy. So I wonder whether I made a mistake or tflite doesn't support int8 bilinear upsampling correctly. The details are as follows.

### 2. Code

I aim to do 8-bit quantization to the network below, which has a bilinear upsample branch

```
def upsambiskip(scale=3, in_channels=3, num_fea=28, m=4, out_channels=3):
    inp = Input(shape=(None, None, 3)) 
    upsampled_inp=UpSampling2D(size=(3,3),data_format=None,interpolation='bilinear')(inp)

    x = Conv2D(num_fea, 3, padding='same', activation='relu', kernel_initializer=glorot_normal(), bias_initializer='zeros')(inp)

    for i in range(m):
        x = Conv2D(num_fea, 3, padding='same', activation='relu', kernel_initializer=glorot_normal(), bias_initializer='zeros')(x)

    x = Conv2D(out_channels*(scale**2), 3, padding='same', kernel_initializer=glorot_normal(), bias_initializer='zeros')(x)
    
    depth_to_space = Lambda(lambda x: tf.nn.depth_to_space(x, scale))
    out = depth_to_space(x)
    x = Add()([upsampled_inp, out])
    x = Conv2D(3, 3, padding='same', kernel_initializer=glorot_normal(), bias_initializer='zeros')(x)

    clip_func = Lambda(lambda x: K.clip(x, 0., 255.))
    out = clip_func(x)
    
    return Model(inputs=inp, outputs=out)
```
I had done FP32 training before, then I loaded FP32 checkpoint and did Quantization-aware training like below
```
class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
    def get_weights_and_quantizers(self, layer):
        return []
    def get_activations_and_quantizers(self, layer):
        return []
    def set_quantize_weights(self, layer, quantize_weights):
        pass
    def set_quantize_activations(self, layer, quantize_anctivations):
        pass
    def get_output_quantizers(self, layer):
        return []
    def get_config(self):
        return {}

def ps_quantization(self, layer):
    # lambda not quantization
    if 'lambda' in layer.name :
        return tfmot.quantization.keras.quantize_annotate_layer(layer, quantize_config=NoOpQuantizeConfig())
    return layer


# create fp32 model and load weight
p_model = create_model(args['networks'])
    
lg.info('Start copying weights and annotate Lambda layer...')
annotate_model = tf.keras.models.clone_model(
            p_model,
            clone_function=self.ps_quantization
            )
lg.info('Start annotating other parts of model...')
annotate_model = tfmot.quantization.keras.quantize_annotate_model(annotate_model)
lg.info('Creating quantize-aware model...')
depth_to_space = Lambda(lambda x: tf.nn.depth_to_space(x, 3))
with tfmot.quantization.keras.quantize_scope({'NoOpQuantizeConfig': NoOpQuantizeConfig, 'depth_to_space': depth_to_space, 'tf': tf}):
    self.model = tfmot.quantization.keras.quantize_apply(annotate_model)
    
# training...
```

the QAT model was correct, and the performance in validation set was closed to FP32's. Then I converted QAT model to tflite

```
def qat_quantize(model_path,output_path):

    fakeqmodel=tf.keras.models.load_model(model_path)
    print('fake quantize model validate..')

    # validate fake QAT model, and the performance is OK
    load_validate(fakeqmodel,save_path=None)

    # convert QAT model and store int8 tflite model
    converter = tf.lite.TFLiteConverter.from_keras_model(fakeqmodel)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    ### converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.experimental_new_converter=True
    converter.experimental_new_quantizer=True
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    quant_tf_model = converter.convert()
    with open(output_path, 'wb') as f:
        f.write(quant_tf_model)
```

I loaded tflite model and evaluated as below, but the performance decrease significantly. 
```
def evaluate(quantized_model_path, save_path):

    interpreter = tf.lite.Interpreter(model_path=quantized_model_path,num_threads=32)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    IS, IZ = input_details[0]['quantization']
    OS, OZ = output_details[0]['quantization']
    print('Input Scale: {}, Zero Point: {}'.format(IS, IZ))
    print('Output Scale: {}, Zero Point: {}'.format(OS, OZ))
    psnr = 0.0
    for i in range(801, 901):
        lr_path = 'data/DIV2K/DIV2K_train_LR_bicubic/X3_pt/0{}x3.pt'.format(i)
        with open(lr_path, 'rb') as f:
            lr = pickle.load(f)
        h, w, c = lr.shape
        lr = np.expand_dims(lr, 0).astype(np.float32)
        # ##lr = np.round(lr/IS+IZ).astype(np.uint8)
        lr = lr.astype(np.uint8)

        hr_path = 'data/DIV2K/DIV2K_train_HR_pt/0{}.pt'.format(i)
        with open(hr_path, 'rb') as f:
            hr = pickle.load(f)
        hr = np.expand_dims(hr, 0).astype(np.float32)
        interpreter.resize_tensor_input(input_details[0]['index'], lr.shape)
        interpreter.allocate_tensors()
        interpreter.set_tensor(input_details[0]['index'], lr)
        interpreter.invoke()

        sr = interpreter.get_tensor(output_details[0]['index'])
        #sr = np.clip(np.round((sr.astype(np.float32)-OZ)*OS), 0, 255)
        sr = np.clip(sr, 0, 255)
        b, h, w, c = sr.shape
        # save image
        if save_path is not None:
            save_name = osp.join(save_path, '{:04d}x3.png'.format(i))
            cv2.imwrite(save_name, cv2.cvtColor(sr.squeeze().astype(np.uint8), cv2.COLOR_RGB2BGR))

        mse = np.mean((sr[:, 1:h-1, 1:w-1, :].astype(np.float32) - hr[:, 1:h-1, 1:w-1, :].astype(np.float32)) ** 2)
        singlepsnr =  20. * math.log10(255. / math.sqrt(mse))
        print('[{}]/[100]: {}'.format(i, singlepsnr))
        psnr += singlepsnr
    print(psnr / 100)

```

### 3. Failure after conversion
the conversion is successful, but the int8 tflite model has a low accuracy performance.


### 5. (optional) Any other info / logs
the tflite model is below, though there are some unexpected op before bilinear upsampling because of the dynamic input Tensor, I thought it has nothing to do with performance degradation, beacause the psnr performance is still low when the input size is solid. 
![image](https://user-images.githubusercontent.com/28351316/180609889-a4c05f4c-717c-48a0-aefb-75683cc719c4.png)

",2022-07-23 14:49:20+00:00,"stat:awaiting response, type:support, stale, comp:lite, TFLiteConverter, TF 2.5"
using tf.function on a model call will not utilize gpu nor VRAM but log placement to gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.6

### GPU model and memory

3090 24GB

### Current Behaviour?

```shell
Utilize tf.function on a function that calls model(a,b) (aka model.call)
Although eager tensors have all been allocated to the gpu, actual training doesnt happen on the gpu, rather the cpu. Notice 20x slower training speeds than when utilizing nested tf.functions in model.call, which is not allowed in MirroredStrategy.
```


### Standalone code to reproduce the issue

```shell
Model.call:

def call(self,x,y):
    vals=self.disc_gradients(x,y)
    self.gen_gradients(x,y)
    return vals
```
model call wrapper:
```
@tf.function#this is the problematic tf.function, if this tf.function is moved to the below tf.functions it works fine.
def run_batch(self,batch_x,batch_y):
    val=self.model(batch_x,batch_y)
    return val
```
gradients functions:
```
#@tf.function
def get_disc_gradients(self,x,y):
    #calc val
    self.disc_optimizer.apply_gradients(disc_gradients,vars)
    return disc_gradients
```
Obviously this code currently does nothing, it's a minimal reproducible example.
```


### Relevant log output

```shell
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
```
The above log is relevant as it tries to make us wrap the entire model.call in a tf.function, which is where I discovered this entire issue. This issue is replicateable w/o using MirroredStrategy: all that matters is that tf.function is wrapping a model call.
No other relevant log output could be found.
Only way to notice the issues are the extremely long batch times, along with
the GPU only utilizing 3GB of VRAM compared to a full 20gb when the tf.function is moved.
</details>",2022-07-24 10:19:29+00:00,"type:bug, comp:gpu, comp:tf.function, TF 2.9"
Enable NNAPI Crash ,"**System information**
- Xamarin.Android Android 12.0 
- Xamarin Tensorflow Lite 2.6.0.1
- Visual Studio 2022 c#


**Standalone code to reproduce the issue**

Enableing NNAPI (options.SetUseNNAPI(true);)
interpreter = new Interpreter(mappedByteBuffer, options);
https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/1?lite-format=tflite
That works 


all my trained models Crash 
Work when i don't Enable NNAPI
(I Comment options.SetUseNNAPI(true) but its should be slower)

Even when i take 
http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz


tflite_directory = '/content/tfliteExport'
pipeline_file = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/pipeline.config'


last_model_path = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint'
!python /content/models/research/object_detection/export_tflite_graph_tf2.py \
    --pipeline_config_path={pipeline_file} \
    --trained_checkpoint_dir={last_model_path} \
    --output_directory={tflite_directory}
import cv2

# image_path = '/content/gdrive/MyDrive/Map/foto/Test'
image_path = '/content/img'

def representative_dataset_gen():
    for f_name in os.listdir(image_path):
      file_path = os.path.normpath(os.path.join(image_path, f_name))
      print(file_path)
      img = cv2.imread(file_path)
      img = cv2.resize(img, (640,640))
      img = img / 255.0
      img = np.reshape(img, (1, 640, 640, 3))
      image = img.astype(np.float32)
      yield [image]
converter = tf.lite.TFLiteConverter.from_saved_model('/content/tfliteExport/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.float32
# converter.inference_input_type = tf.uint8
# converter.inference_output_type = tf.uint8
tflite_model = converter.convert()
with open('{}/TFmodelV3.tflite'.format('/content/tfliteExport/saved_model'), 'wb') as w:
    w.write(tflite_model)
print(""tflite convert complete! - {}/TFmodelV3.tflite"".format('/content/newTFLite/saved_model'))
**Any other info / logs**
{Java.Lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1053 while adding operation.

Restored original execution plan after delegate application failure.
  at Java.Interop.JniEnvironment+InstanceMethods.CallNonvirtualVoidMethod (Java.Interop.JniObjectReference instance, Java.Interop.JniObjectReference type, Java.Interop.JniMethodInfo method, Java.Interop.JniArgumentValue* args) [0x00088] in /Users/runner/work/1/s/xamarin-android/external/Java.Interop/src/Java.Interop/Java.Interop/JniEnvironment.g.cs:12324 
  at Java.Interop.JniPeerMembers+JniInstanceMethods.FinishCreateInstance (System.String constructorSignature, Java.Interop.IJavaPeerable self, Java.Interop.JniArgumentValue* parameters) [0x0003e] in /Users/runner/work/1/s/xamarin-android/external/Java.Interop/src/Java.Interop/Java.Interop/JniPeerMembers.JniInstanceMethods.cs:142 
  at Xamarin.TensorFlow.Lite.Interpreter..ctor (Java.Nio.ByteBuffer byteBuffer, Xamarin.TensorFlow.Lite.Interpreter+Options options) [0x0009d] in <d3ecc302b2d54bbb87b0b5080c9dc98b>:0 
  at CameraTF.TensorflowLiteService.Initialize () [0x00017] in X:\Snelle TF Xam\CameraTF-master\src\CameraTF\AR\TensorflowLiteService.cs:62 
  --- End of managed Java.Lang.IllegalArgumentException stack trace ---
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1053 while adding operation.

Restored original execution plan after delegate application failure.
	at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
	at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:486)
	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88)
	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)
	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:247)
	at crc649b31f1000968ddb3.MainActivity.n_onCreate(Native Method)
	at crc649b31f1000968ddb3.MainActivity.onCreate(MainActivity.java:31)
	at android.app.Activity.performCreate(Activity.java:7994)
	at android.app.Activity.performCreate(Activity.java:7978)
	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)
	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3422)
	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3601)
	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)
	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2066)
	at android.os.Handler.dispatchMessage(Handler.java:106)
	at android.os.Looper.loop(Looper.java:223)
	at android.app.ActivityThread.main(ActivityThread.java:7656)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)
}
",2022-07-27 19:16:47+00:00,"stat:awaiting response, type:support, stale, comp:lite, TFLiteNNAPIDelegate, 2.6.0"
Efficiently get an equal number of per class data point continuously with `tf.data` API. ,"[Info]
```
TensorFlow: 2.6
Environment: Kaggle / Colab
Accelerator: TPU / GPU
```

# Current Behaviour

I am trying to get equal number of sample per class within a batch of data from `tf.data` API. With `batch_size = 14` and `sample_per_class = 3`, I'm expecting to get the following output for `num_classes = 5`:

```yaml
1st batch: [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]
2nd batch: [5, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5]
```

# Standalone code to reproduce the issue

Here is the standalone code and approach so far.

```python

def dataset_for_class(i):
    i = tf.cast(i, tf.int32)
    return dataset.filter(lambda label: label == i)

num_classes = 1000
num_contiguous_instances = 2

dataset = tf.random.uniform([num_classes], maxval=num_classes, dtype=tf.int32).numpy()
dataset = tf.data.Dataset.from_tensor_slices(dataset)
dataset = tf.data.Dataset.range(num_classes).interleave(
    dataset_for_class,
    cycle_length=num_classes,
    block_length=num_contiguous_instances,
)
dataset = dataset.prefetch(tf.data.AUTOTUNE)
list(dataset.as_numpy_iterator())
[1, 1, 2, 2, 4, 4, 6, 7, 8, 8, 10, 10, 11, 12, ...]
# 6 appears 1 time because of dummy random input.
```
```bash
%%timeit
a = next(iter(dataset))
16.6 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

The issue arises when `num_classes` gets bigger, for example, **image-net** (1000 classes). It gets too slow to train the model, GPU/TPU both. I'm looking for an efficient solution with `tf.data` API. ",2022-07-28 13:39:06+00:00,"stat:awaiting tensorflower, comp:data, type:performance, 2.6.0"
Pip install very slow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.6.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Ubuntu

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
pip install --upgrade tensorflow==2.6.1
Want normal speed
Country:Belarus.
No VPN
```


### Standalone code to reproduce the issue

```shell
Speed is 80kb/s
```


### Relevant log output

_No response_</details>",2022-08-07 18:38:58+00:00,"stat:awaiting response, type:build/install, type:performance, 2.6.0"
Keras Model `__call__` performance with large `StringLookup`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Debian 4.19.249-2

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running a custom Keras model with a large instance variable (for example a `StringLookup` layer with a large vocabulary), the `__call__` function is extremely slow, irrespective of what's in the custom user defined `call` function. It seems that this is happening because when running `__call__` in eager mode, `_clear_losses` is called, which eventually calls `_flatten_modules`, which recursively iterates over all trackable attributes of the model (which happens to include the large `StringLookup` table.

The table isn't touched at all in `_clear_losses`, since it's not actually a layer -- is there a way to stop these variables from being processed in that function, or am I looking at this the wrong way?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from datetime import datetime

class LookupModel(tf.keras.Model):
    def __init__(self, table_size, name=""lookup""):
        super().__init__(name=name)

        self.kvstore = tf.keras.layers.StringLookup(
            vocabulary=[str(i) for i in range(table_size)],
            trainable=False
        )

    def call(self, input):
        return input

small = LookupModel(10)
large = LookupModel(5_000_000)

small_start = datetime.now()
small([1])
print(""Small Lookup:"", datetime.now() - small_start)

large_start = datetime.now()
large([1])
print(""Large Lookup:"", datetime.now() - large_start)
```


### Relevant log output

```shell
Small Lookup: 0:00:00.132581
Large Lookup: 0:00:04.091659
```
</details>",2022-08-09 23:23:28+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.9"
Hexagon delegate (tflite) functionality broken after rebuilding project,"I had the hexagon delegate working fine for weeks, but yesterday I clicked ""Rebuild Project"" in Android Studio, and now I cannot get it working again.
I have scrubbed every place on my disk where I can find the files `libhexagon_interface.so` and `libtensorflowlite_hexagon_jni.so`, and made sure that the only copy of those files present on my computer is one that I downloaded and extracted manually from [oss.sonatype.org](https://oss.sonatype.org/#nexus-search;gav~org.tensorflow~tensorflow-lite-hexagon~~~). I place these files into their location in `.grade/caches/...`. I'm using the latest `0.0.0-nightly-SNAPSHOT` from there, which is dated `Fri Aug 12 2022`.
I delete all intermediate build directories, and wipe Android studio caches.
Still, I get this log dump when trying to load the hexagon delegate:

```
2022-08-12 09:34:50.182 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/rpcmem_android.c:159: rpcmem_init_internal: opened ION device fd 61, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)
2022-08-12 09:34:50.182 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:3087: fastrpc_apps_user_init done with default domain:0 and &fastrpc_trace:0x73af5f9fa0
2022-08-12 09:34:50.188 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/rpcmem_android.c:159: rpcmem_init_internal: opened ION device fd 62, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)
2022-08-12 09:34:50.189 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:3087: fastrpc_apps_user_init done with default domain:3 and &fastrpc_trace:0x73af548fa0
2022-08-12 09:34:50.195 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:2636: Successfully opened /vendor/dsp/cdsp/fastrpc_shell_33
2022-08-12 09:34:50.195 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_config.c:200: Reading configuration file: com.journeyapps.visiondemo.debugconfig
2022-08-12 09:34:50.185 6374-6374/com.journeyapps.visiondemo I/apps.visiondemo: type=1400 audit(0.0:190): avc: denied { read } for name=""adsprpc-smd-secure"" dev=""tmpfs"" ino=4237 scontext=u:r:untrusted_app_27:s0:c169,c256,c512,c768 tcontext=u:object_r:vendor_xdsp_device:s0 tclass=chr_file permissive=1 app=com.journeyapps.visiondemo
2022-08-12 09:34:50.197 6374-6374/com.journeyapps.visiondemo E/ion: ioctl c0044901 failed with code -1: Inappropriate ioctl for device
2022-08-12 09:34:50.189 6374-6374/com.journeyapps.visiondemo I/apps.visiondemo: type=1400 audit(0.0:192): avc: denied { ioctl } for path=""/dev/adsprpc-smd-secure"" dev=""tmpfs"" ino=4237 ioctlcmd=0x5208 scontext=u:r:untrusted_app_27:s0:c169,c256,c512,c768 tcontext=u:object_r:vendor_xdsp_device:s0 tclass=chr_file permissive=1 app=com.journeyapps.visiondemo
2022-08-12 09:34:50.241 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:2843: Created user PD on domain 3 (attrs 0x0, debug_trace 0x0)
2022-08-12 09:34:50.245 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/listener_android.c:111: listener thread starting
2022-08-12 09:34:50.245 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_perf.c:273: fastrpc_perf_init: enabled systrace 0x0 and RPC traces (kernel 0, dsp 0) with frequency 1000
2022-08-12 09:34:50.246 6374-6403/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:345: file_watcher_thread starting for domain 3
2022-08-12 09:34:50.247 6374-6404/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_latency.c:96: FastRPC latency thread started for QoS
2022-08-12 09:34:50.247 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1345: remote_handle_open: Successfully opened handle 0x29f0d640 for adsp_current_process on domain 3
2022-08-12 09:34:50.248 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /data/app/~~1vCWmfHaESKkWu1hV7LMOg==/com.journeyapps.visiondemo-v9RYULlL0KQNWz-o9hPkYA==/lib/arm64/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.249 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1798: remote_handle_control_domain: requested QOS 1, latency 115 for domain 3 handle 0xffffffffffffffff
2022-08-12 09:34:50.250 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /system/vendor/lib/rfsa/adsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.252 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /vendor/lib/rfsa/adsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.252 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /vendor/dsp/cdsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.252 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:687: open_mod_table_open_from_static: reverse module apps_std opened with handle 0xaf54b9e8 (idx 0)
2022-08-12 09:34:50.253 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /vendor/dsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.254 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:464: is_reverse_handle_opened: reverse module apps_std  already found with handle 0xaf54b9e8 (idx 0)
2022-08-12 09:34:50.254 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:687: open_mod_table_open_from_static: reverse module apps_std opened with handle 0xaf54b9e8 (idx 0)
2022-08-12 09:34:50.256 6374-6402/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/listener_android.c:64::error: 512: AEE_SUCCESS == (nErr = mod_table_close(handle, errStr, errStrLen, dlErr))
2022-08-12 09:34:50.257 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/apps_std_imp.c:868: Successfully opened file /data/app/~~1vCWmfHaESKkWu1hV7LMOg==/com.journeyapps.visiondemo-v9RYULlL0KQNWz-o9hPkYA==/lib/arm64/libhexagon_nn_skel_v66.so
2022-08-12 09:34:50.271 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:687: open_mod_table_open_from_static: reverse module apps_mem opened with handle 0xaf54bae8 (idx 1)
2022-08-12 09:34:50.281 6374-6374/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1325: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror _rtld_map_object_ex: cannot open oemconfig.so, errno 2 (no such file
2022-08-12 09:34:50.281 6374-6374/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1398: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp (errno Success)
2022-08-12 09:34:50.281 6374-6374/com.journeyapps.visiondemo W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
2022-08-12 09:34:50.282 6374-6374/com.journeyapps.visiondemo I/tflite: Hexagon Delegate is not supported.
```
Prior to hitting ""Rebuild Project"", this worked fine for weeks. Neither my Android hardware nor my software has changed.

This is a real-time project we're working on, and without hexagon support, inference time goes from 200ms to 1600ms, which makes this project dead in the water.

Please help!",2022-08-12 08:02:54+00:00,"type:support, comp:lite, TFLiteHexagonDelegate, TF 2.9"
how to use tfrecord data(dict format) (tf.data.dataset) in distribute training?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf2.2.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I tried to use both mirror and multimirror in distributed training, and i found the input data (from all doucements like tf official tutorial,github and so on )to the strategy.run() must be like ((None,256,256,64)(None)), while the batched data from the fnction below ""def make_dataset_for_distribute_train(self, all_paths):"" is a dict.  strategy.run() cannot accept dict data for distributed training , so i had to transform it(the output of ""make_dataset_for_distribute_train"" function) before the distributed training using the
function ""def parse_dict2_turple(self,data_tensor"" below.
It worked, but the cpu useage is above 300%(before is 200%）and gpu_utils_average is just 10%(before is 75% one gpu). As a result, the distributed training with 4 gpus even slower than normal training. 
Can someone tell me why and how to fix it? really appreciate for your help!!!
```


### Standalone code to reproduce the issue

```shell
def make_dataset_for_distribute_train(self, all_paths):
   dataset = tf.data.TFRecordDataset(all_paths)
   dataset = dataset.map(self.parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   dataset = dataset.map(self.make_feed_dict,num_parallel_calls=tf.data.experimental.AUTOTUNE)
   padding_dict = {""a"": [20], ""b"": [None], ""c"": [30],""d"": [13]}

   dataset_for_train = dataset.padded_batch(self.batch_size, padded_shapes=padding_dict,
                                                 drop_remainder=True).prefetch(
            buffer_size=tf.data.experimental.AUTOTUNE)
   return dataset_for_train
    



def parse_dict2_turple(self,data_tensor):

        d0 = data_tensor['a']
        d1 = data_tensor['b']
        d2 = data_tensor['c']
        d3 = data_tensor['d']
  
        return tf.data.Dataset.from_tensor_slices((d0, d1, d2, d3))

        def distributed_train_step(inputs):

            @tf.function
            def distributed_train_step_wrapped(ds):
                loss=0.0
                for k in ds:
                    # tf.print(k)
                    per_replica_average_loss = strategy.run(train_step, args=(k,))
                    loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_average_loss, axis=None)

                return loss
            return distributed_train_step_wrapped(inputs)
            for i in date:
                all_paths = self.parse_input_path(i, epoch)
                logger.info(all_paths)
                dataset = self.make_dataset_for_distribute_train(all_paths)

                total_loss = 0.0
                num_train_batches = 0
                for j in dataset:

                    dataset_parsed = self.parse_dict2_turple(j)
                    dataset_parsed = dataset_parsed.with_options(options)
                    train_dataset_distribute = strategy.experimental_distribute_dataset(dataset_parsed)
                    batch_loss = distributed_train_step(train_dataset_distribute)
                    num_train_batches += 1
                    total_loss += batch_loss
```


### Relevant log output

_No response_</details>",2022-08-16 10:05:50+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.2"
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

2080Ti 11Gb

### Current Behaviour?

```shell
I get lots of warnings during image augmentation. Because of them, the speed is super slow.
```


### Standalone code to reproduce the issue

```shell
I'm trying to run this sample Keras code: https://keras.io/examples/vision/semantic_image_clustering
The Warning I get happens during augmentation


from collections import defaultdict
import random
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from tqdm import tqdm

num_classes = 10
input_shape = (32, 32, 3)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_data = np.concatenate([x_train, x_test])
y_data = np.concatenate([y_train, y_test])

print(""x_data shape:"", x_data.shape, ""- y_data shape:"", y_data.shape)

classes = [
    ""airplane"",
    ""automobile"",
    ""bird"",
    ""cat"",
    ""deer"",
    ""dog"",
    ""frog"",
    ""horse"",
    ""ship"",
    ""truck"",
]


target_size = 32  # Resize the input images.
representation_dim = 512  # The dimensions of the features vector.
projection_units = 128  # The projection head of the representation learner.
num_clusters = 20  # Number of clusters.
k_neighbours = 5  # Number of neighbours to consider during cluster learning.
tune_encoder_during_clustering = False  # Freeze the encoder in the cluster learning.


data_preprocessing = keras.Sequential(
    [
        layers.Resizing(target_size, target_size),
        layers.Normalization(),
    ]
)
# Compute the mean and the variance from the data for normalization.
data_preprocessing.layers[-1].adapt(x_data)


data_augmentation = keras.Sequential(
    [
        layers.RandomTranslation(
            height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=""nearest""
        ),
        layers.RandomFlip(mode=""horizontal""),
        layers.RandomRotation(
            factor=0.15, fill_mode=""nearest""
        ),
        layers.RandomZoom(
            height_factor=(-0.3, 0.1), width_factor=(-0.3, 0.1), fill_mode=""nearest""
        )
    ]
)


image_idx = np.random.choice(range(x_data.shape[0]))
image = x_data[image_idx]
image_class = classes[y_data[image_idx][0]]
plt.figure(figsize=(3, 3))
plt.imshow(x_data[image_idx].astype(""uint8""))
plt.title(image_class)
_ = plt.axis(""off"")

```
```


### Relevant log output

```shell
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x000002581AA0BAF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x000002581BE0A288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
```
```
</details>",2022-08-17 08:50:06+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.9"
tensorflow lite for microcontroller is slower then tensorflow lite for python.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

ubuntu20

### Python version

3.9


### Current Behaviour?

```shell
The model is runing on armv7.using tensorflow lite for microcontroller is slower than using tensorflow lite for python.My test result showing that the fullconnected layer consumes most of the computing power.
The input signals type of my model are float32 while the weights type of my model are int8.
In order to make the micro version support the hybrid model, I made some code changes.
I want to know how can I make the fullyconnected layer of micro version have the same efficiency as the fullyconnected layer of the python version.
please give me some advice.
```


### Standalone code to reproduce the issue

```shell
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const float* input_data, 			const RuntimeShape& weights_shape,
    const int8_t* weights_data, 		const RuntimeShape& bias_shape,
    const float* bias_data, 			const RuntimeShape& output_shape,
    float* output_data,    				const TfLiteTensor* filter )
{
	float output_activation_min = params.float_activation_min;
	float output_activation_max = params.float_activation_max;
	output_activation_min =output_activation_min + 0;
	output_activation_max =output_activation_max + 0;
	// TODO(b/62193649): This really should be:
	//     const int batches = ArraySize(output_dims, 1);
	// but the current --variable_batch hack consists in overwriting the 3rd
	// dimension with the runtime batch size, as we don't keep track for each
	// array of which dimension is the batch dimension in it.
	const int output_dims_count = output_shape.DimensionsCount();
	const int weights_dims_count = weights_shape.DimensionsCount();
	const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
	const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
					   output_shape, output_dims_count - 1);
	const int accum_depth = weights_shape.Dims(weights_dims_count - 1);


	const float filter_scale = static_cast<float>(filter->params.scale);

	for (unsigned short b = 0; b < batches; ++b)						
	{
		unsigned short index1 = b * accum_depth;
		unsigned short index3 = output_depth * b;
		for (unsigned short out_c = 0; out_c < output_depth; ++out_c)		
		{
			float total = 0.f;
			int index2 = out_c * accum_depth;
			for (unsigned short d = 0; d < accum_depth; ++d)		
			{
				total += input_data[index1 + d] * weights_data[index2 + d];
			}
			total = total * filter_scale;
			float bias_value = 0.0f;
			if (bias_data)
			{
				bias_value = bias_data[out_c];
			}
			bias_value = bias_value + 0;
			output_data[out_c + index3] = ActivationFunctionWithMinMax(total + bias_value, output_activation_min, output_activation_max);
		}
	}
}
```


### Relevant log output

```shell
python version runtime: 2.6ms
micro c++ version runtime: 7ms
```
</details>",2022-08-19 09:28:19+00:00,"stat:awaiting response, type:performance, comp:micro, TF 2.9"
TF golang client memory leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 1 and tf 2

### Custom Code

Yes

### OS Platform and Distribution

ubuntu20.04

### Python version

3.8

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

11.4

### Current Behaviour?

```shell
Action1: new a tensor and use the only one tensor to do session run will not result in memory raise.
Action2: new a tensor every time before seesion run will lead to memory rasie.

Conclusion: NewTensor api lead to memory leak

Suggestion: Release tensor obvisly instead of just set finializer
```


### Standalone code to reproduce the issue

```shell
package main

import (
	""flag""
	""fmt""
	""io/ioutil""
	""log""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
)

func main() {
	// An example for using the TensorFlow Go API for image recognition
	// using a pre-trained inception model (http://arxiv.org/abs/1512.00567).
	//
	// Sample usage: <program> -dir=/tmp/modeldir -image=/path/to/some/jpeg
	//
	// The pre-trained model takes input in the form of a 4-dimensional
	// tensor with shape [ BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, 3 ],
	// where:
	// - BATCH_SIZE allows for inference of multiple images in one pass through the graph
	// - IMAGE_HEIGHT is the height of the images on which the model was trained
	// - IMAGE_WIDTH is the width of the images on which the model was trained
	// - 3 is the (R, G, B) values of the pixel colors represented as a float.
	//
	// And produces as output a vector with shape [ NUM_LABELS ].
	// output[i] is the probability that the input image was recognized as
	// having the i-th label.
	//
	// A separate file contains a list of string labels corresponding to the
	// integer indices of the output.
	//
	// This example:
	// - Loads the serialized representation of the pre-trained model into a Graph
	// - Creates a Session to execute operations on the Graph
	// - Converts an image file to a Tensor to provide as input to a Session run
	// - Executes the Session and prints out the label with the highest probability
	//
	// To convert an image file to a Tensor suitable for input to the Inception model,
	// this example:
	// - Constructs another TensorFlow graph to normalize the image into a
	//   form suitable for the model (for example, resizing the image)
	// - Creates and executes a Session to obtain a Tensor in this normalized form.
	modeldir := flag.String(""dir"", """", ""Directory containing the trained model files. The directory will be created and the model downloaded into it if necessary"")
	//imagefile := flag.String(""image"", """", ""Path of a JPEG-image to extract labels for"")
	flag.Parse()
	if *modeldir == """" {
		flag.Usage()
		return
	}
	// Load the serialized GraphDef from a file.
	// modelfile, labelsfile, err := modelFiles(*modeldir)
	//if err != nil {
	//	log.Fatal(err)
	//}
	model, err := ioutil.ReadFile(*modeldir)
	if err != nil {
		log.Fatal(err)
	}

	// Construct an in-memory graph from the serialized form.
	graph := tf.NewGraph()
	if err := graph.Import(model, """"); err != nil {
		log.Fatal(err)
	}

	// Create a session for inference over graph.
	session, err := tf.NewSession(graph, nil)
	if err != nil {
		log.Fatal(err)
	}
	defer session.Close()

	// Run inference on *imageFile.
	// For multiple images, session.Run() can be called in a loop (and
	// concurrently). Alternatively, images can be batched since the model
	// accepts batches of image data as input.
	//tensor, err := makeTensorFromImage(*imagefile)
	//if err != nil {
	//	log.Fatal(err)
	//}

	inputs := make([]string, 0)
	for i := 0; i< 8;i++ {
		inputs = append(inputs, ""测试文本输入"")
	}

	tensor, err := tf.NewTensor(inputs)
	if err != nil {
		log.Fatal(err)
	}
	for i := 0; i < 1000000; i++ {
		output, err := session.Run(
			map[tf.Output]*tf.Tensor{
				graph.Operation(""input_tensor_name"").Output(0): tensor,
			},
			[]tf.Output{
				graph.Operation(""output_tensor_name"").Output(0),
			},
			nil)
		if err != nil {
			log.Fatal(err)
		}
		probabilities := output[0].Value().([][]float32)[0]
		if i % 1000 == 0 {
			fmt.Println(""repeat "", probabilities)
		}
	}
   	// output[0].Value() is a vector containing probabilities of
	// labels for each image in the ""batch"". The batch size was 1.
	// Find the most probably label index.
	//printBestLabel(probabilities, labelsfile)
}
```",2022-08-21 07:22:44+00:00,"stat:awaiting response, type:bug, stale"
Converting a TF model to TFLite and then to EdgeTPU,"I'm trying to convert a simple add model from TF to TFLite to EdgeTPU
However, it seems this conversion is caught in a catch 22 scenario.
If I specify int8 data type as below, it ends up wanting to use the FlexAddV2 TF op, which fails to convert to EdgeTPU.
If I specify as int32 data type instead, it is able to use regular ADD op, but the data type is incompatible with EdgeTPU and says the data type is supported so it only runs on CPU. Is there any way to get this to convert for EdgeTPU as INT8 so it can run on TPU?


### 1. System information

- OS Platform and Distribution: Linux Ubuntu 20.04.4 LTS (Focal Fossa)
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
input = keras.Input(shape=(32,), name=""dummy_input"", dtype=tf.int8)
output = tf.add(input, 1)
model = keras.Model(inputs=input, outputs=output)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_quant_model = converter.convert()

$ edgetpu_compiler -s testmodel.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 0 (FlexAddV2) failed to prepare.

Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!



### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

INT8 conversion, resulting in unsupported op:

edgetpu_compiler -s testmodel.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 0 (FlexAddV2) failed to prepare.

Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!



INT32 conversion, resulting in invalid data type for TPU:

$ edgetpu_compiler -s testmodel.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 0 ms.

Input model: testmodel.tflite
Input size: 936.00B
Output model: testmodel_edgetpu.tflite
Output size: 476.00B
On-chip memory used for caching model parameters: 0.00B
On-chip memory remaining for caching model parameters: 0.00B
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 0
Total number of operations: 1
Operation log: testmodel_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 0
Number of operations that will run on CPU: 1

Operator                       Count      Status

ADD                            1          Operation is working on an unsupported data type
Compilation child process completed within timeout period.
Compilation succeeded!




",2022-08-28 21:23:07+00:00,"comp:lite, comp:micro, TFLiteConverter, TF 2.9"
TFLite with XNNPack delegate is performing slower than TFLite when XNNPack delegate is set to false,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Debian 10.2

### Mobile device

_No response_

### Python version

3.7.3 for python3, python 2.7.16

### Bazel version

5.1.0

### GCC/Compiler version

8.3.0

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

```shell
Expected performance benefit when XNNPack delegate is used as claimed.
But not happening. Tested with quant and float models.
```


### Standalone code to reproduce the issue

```shell
Used tensorflow/lite/examples/label_image/

Modified loop count to 100 and tested with num of threads to max and -1 and observed similar results.

% bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model mobilenet_v2_1.0_224.tflite --labels tensorflow/lite/examples/label_image/mobilenet_v1_1.0_224/labels.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp --use_xnnpack=true
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
INFO: Applied XNNPACK delegate.


INFO: invoked
INFO: average time: 16.0024 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet

% bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model mobilenet_v2_1.0_224.tflite --labels tensorflow/lite/examples/label_image/mobilenet_v1_1.0_224/labels.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp --use_xnnpack=false
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: invoked
INFO: average time: 3.64675 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet
```


### Relevant log output

```shell
With XNNPack
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
INFO: Applied XNNPACK delegate.
INFO: invoked
INFO: average time: 16.0024 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet


Without XNNPack
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: invoked
INFO: average time: 3.64675 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet
```
</details>",2022-08-30 09:06:50+00:00,"stat:awaiting response, stale, comp:lite, type:performance, comp:lite-xnnpack, TF 2.8"
"Keras model training is slow without ""tf.compat.v1.disable_eager_execution()""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04 in WSL2

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In documentation, keras.model.fit() runs in graph mode by default, even if eager mode is by default in TF2.x. So I expect that training a simple keras model (13 parameters) should be fast. But it is very slow on my computer (~30s). However, it will be 10 times faster (~3s) if I add this line in the code: tf.compat.v1.disable_eager_execution()
It seems that keras fit function do not run in graph mode(?). Adding ""run_eagerly=False"" option while compiling the model do not solve the problem. Is this a bug ?
As a workaround, is there another way to disable eager mode globally in TF2 ? The command ""tf.compat.v1.disable_eager_execution()"" works but changes something in TF behavior behind the scences. In my case, when using this command, I cannot use predict function of my trained model which was cached in the Streamlit framework (using @st.cache(allow_output_mutation=True) or @st.experimental_singleton). The following error arises:
""InvalidArgumentError: Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph"" (traceback in log output)
This does not happen when disable_eager_execution() is not used.
Thank you!
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.optimizers import Adam

from datetime import datetime

# Do not use GPU
tf.config.set_visible_devices([], ""GPU"")
# tf.compat.v1.disable_eager_execution()  # adding this line make keras training much faster

# Generate data
def f(x):
    return (x + 1) * np.sin(5 * x)


x_plot = np.arange(-1, 1 + 0.001, 0.001)
y_plot = f(x_plot)

x_train = np.arange(-1 + 0.05, 1, 0.2)
y_train = f(x_train)

x_val = np.arange(-1 + 0.15, 1, 0.2)
y_val = f(x_val)

# Plot the problem
plt.figure()
plt.plot(x_plot, y_plot, ""-"", label=""Orgininal function"")
plt.plot(x_train, y_train, ""o"", label=""Training points"")
plt.plot(x_val, y_val, ""s"", label=""Validation points"")
plt.xlim(-1, 1)
plt.ylim(-2, 2)
plt.xlabel(""x"")
plt.ylabel(""f"")
plt.grid()
plt.legend()
plt.show(block=False)

# Reshape
X_train = x_train.reshape(x_train.shape[0], 1)
Y_train = y_train.reshape(x_train.shape[0], 1)

X_val = x_val.reshape(x_val.shape[0], 1)
Y_val = y_val.reshape(x_val.shape[0], 1)

# Simple model
tf.keras.utils.set_random_seed(1)
model = Sequential()
model.add(Input(shape=(1,)))  # Input layer
model.add(
    Dense(
        4,
        activation=""sigmoid"",
        kernel_initializer=RandomNormal(mean=0.0, stddev=1.0, seed=1),
    )
)
model.add(Dense(1))

model.compile(loss=""mean_squared_error"", optimizer=Adam(learning_rate=3e-1))

start_time = datetime.now()
history = model.fit(
    X_train,
    Y_train,
    validation_split=0.0,
    validation_data=(X_val, Y_val),
    validation_freq=1,
    batch_size=X_train.shape[0],
    epochs=2000,
    verbose=0,
)

run_time = datetime.now() - start_time
print(""Training time : {:.4f} s"".format(run_time.total_seconds()))
```


### Relevant log output

```shell
InvalidArgumentError: Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph
Traceback:
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/streamlit/scriptrunner/script_runner.py"", line 557, in _run_script
    exec(code, module.__dict__)
File ""dashboard.py"", line 227, in <module>
    Y_train_pred = model.predict(X_train)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 969, in predict
    return func.predict(
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 700, in predict
    return predict_loop(
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 377, in model_iteration
    batch_outs = f(ins_batch)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/backend.py"", line 4282, in __call__
    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/backend.py"", line 4218, in _make_callable
    callable_fn = session._make_callable_from_options(callable_opts)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1513, in _make_callable_from_options
    return BaseSession._Callable(self, callable_options)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1471, in __init__
    self._handle = tf_session.TF_SessionMakeCallable(
```
</details>",2022-09-08 16:46:32+00:00,"stat:awaiting response, stale, comp:eager, comp:keras, type:performance, TF 2.9"
How to disable eager mode?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04 in WSL2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Is there a way to disable the eager mode globally in TensorFLow 2? I do not want to use the following command:
```
""tf.compat.v1.disable_eager_execution()""
```
as my trained keras model is kind of corrupted after reload from cache in Streamlit (see #57645 report)


### Standalone code to reproduce the issue

```
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.optimizers import Adam

from datetime import datetime

import streamlit as st

# Do not use GPU
tf.config.set_visible_devices([], ""GPU"")
tf.compat.v1.disable_eager_execution()  # adding this line make keras training much slower

# Generate data
def f(x):
    return (x + 1) * np.sin(5 * x)


x_plot = np.arange(-1, 1 + 0.001, 0.001)
y_plot = f(x_plot)

x_train = np.arange(-1 + 0.05, 1, 0.2)
y_train = f(x_train)

x_val = np.arange(-1 + 0.15, 1, 0.2)
y_val = f(x_val)

# Plot the problem
plt.figure()
plt.plot(x_plot, y_plot, ""-"", label=""Orgininal function"")
plt.plot(x_train, y_train, ""o"", label=""Training points"")
plt.plot(x_val, y_val, ""s"", label=""Validation points"")
plt.xlim(-1, 1)
plt.ylim(-2, 2)
plt.xlabel(""x"")
plt.ylabel(""f"")
plt.grid()
plt.legend()
plt.show(block=False)

# Reshape
X_train = x_train.reshape(x_train.shape[0], 1)
Y_train = y_train.reshape(x_train.shape[0], 1)

X_val = x_val.reshape(x_val.shape[0], 1)
Y_val = y_val.reshape(x_val.shape[0], 1)

# Simple model
start_time = datetime.now()


@st.experimental_singleton
def train():
    tf.keras.utils.set_random_seed(1)
    model = Sequential()
    model.add(Input(shape=(1,)))  # Input layer
    model.add(
        Dense(
            4,
            activation=""sigmoid"",
            kernel_initializer=RandomNormal(mean=0.0, stddev=1.0, seed=1),
        )
    )
    model.add(Dense(1))

    model.compile(
        loss=""mean_squared_error"", optimizer=Adam(learning_rate=3e-1), run_eagerly=False
    )
    history = model.fit(
        X_train,
        Y_train,
        validation_split=0.0,
        validation_data=(X_val, Y_val),
        validation_freq=1,
        batch_size=X_train.shape[0],
        epochs=2000,
        verbose=0,
    )
    return model


run_time = datetime.now() - start_time

model2 = train()

print(""Training time : {:.4f} s"".format(run_time.total_seconds()))

st.write(""Please rerun the app to see the error"")

model2.predict(X_val)
```

### Relevant log output

_No response_</details>",2022-09-09 12:11:46+00:00,type:support
Memory issue using Pose Estimation model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

iPhone X

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
memory not deallocated once I back to root VC.

App crashes after some time.
```


### Standalone code to reproduce the issue

```shell
I used Tensorflow sample example, and added one empty page before that, on click button will redirect to next camera screen.

This process I done many time so after some time memory issue will be occurred & app crash.
```


### Relevant log output

_No response_</details>",2022-09-16 05:32:54+00:00,"stat:awaiting response, stale, type:performance, comp:lite-examples, TF 2.9"
Memory leak when using Metal delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

TFLite 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

iOS 15

### Mobile device

iPhone X

### Python version

N.A.

### Bazel version

5.0.0

### GCC/Compiler version

Apple clang 13.1.6

### CUDA/cuDNN version

N.A.

### GPU model and memory

iPhone X

### Current Behaviour?

```shell
There seems to be a memory leak while using Metal delegate. It doesn't go away no matter how I try to clean up (be it reusing the delegate or not). I have tried to use without the Metal delegate and the leak doesn't happen.
```


### Standalone code to reproduce the issue

```shell
TfLiteDelegate *CreateGPUDelegate(MetalDelegateWaitType waitType = MetalDelegateWaitType::PASSIVE, bool allowPrecisionLoss = false, bool enableQuantization = true)
{
    TFLGpuDelegateOptions options;
    switch(waitType)
    {
    case PASSIVE:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive;
      break;
    case ACTIVE:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeActive;
      break;
    case DO_NOT_WAIT:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeDoNotWait;
      break;
    case AGGRESIVE:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeAggressive;
      break;
    }
    options.enable_quantization = enableQuantization;
    options.allow_precision_loss = allowPrecisionLoss;
    return TFLGpuDelegateCreate(&options);
}
void DeleteGPUDelegate(TfLiteDelegate *delegate)
{
  TFLGpuDelegateDelete(delegate);
}

// Somewhere in main
  std::thread([this]() {
    std::unique_ptr<tflite::FlatBufferModel> model;
    model = tflite::FlatBufferModel::BuildFromFile(path.c_str());
    while (true)
    {
      std::unique_ptr<tflite::Interpreter> interpreter;
      tflite::ops::builtin::BuiltinOpResolver resolver;
      const TfLiteStatus isInterpreterOk = tflite::InterpreterBuilder(*model, resolver)(&interpreter);
      TfLiteDelegate *delegate = CreateGPUDelegate();
      interpreter->ModifyGraphWithDelegate(delegate);
      interpreter->Invoke();
      interpreter = nullptr;
      DeleteGPUDelegate(delegate);
      std::this_thread::sleep_for(std::chrono::seconds(1));
    }
  }).detach();
```


### Relevant log output

_No response_</details>",2022-09-16 06:24:47+00:00,"stat:awaiting tensorflower, comp:lite, type:performance, TFLiteGpuDelegate, TF 2.9"
"2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to run deepxde with tensorflow (TF2) backend.  
I think this is related to
https://github.com/tensorflow/tensorflow/issues/33375
This question partly relates to the answer provided by there

In that issue, the following answer is given
> Hi @kleyersoma. The workaround for this particular problem on unix-based machines is to link your cuda bin to your working directory. Go to the directory, where you launch your python code and create the link:
> `ln -s /full/path/to/your/cuda/installation/bin .`
> This sovles the problem. The point is that TF first tries to load the ptxas from ./bin directory, then from /usr/local/cuda/bin. Unfortunately, it completely ignores the environment variables (which I consider to be a bug).

It is not clear to me what ""the directory, where you launch your python code"" refers to

I am running Ubuntu 22.04, and 
which python3
gives
/usr/bin/python3
Is this the directory you're referring to?
If so, in my case I would do
ln -s /usr/local/cuda/bin /usr/bin/python3
Is that correct? Thanks.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1rYD_GMLAWJ6uTx76RfYvLWXB2nf9NP7Q?usp=sharing
```


### Relevant log output

```shell
U instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-17 23:48:43.139452: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-09-17 23:48:43.139501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10126 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1
mexclusions 
 []
Compiling model...
'compile' took 0.000393 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

2022-09-17 23:48:46.762887: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x560ea9633610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-09-17 23:48:46.762922: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1
2022-09-17 23:48:46.842763: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-09-17 23:48:51.944301: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'input_fusion_reduce_4', 33468 bytes spill stores, 38624 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_accurate_pow', 132 bytes spill stores, 132 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 56 bytes spill stores, 48 bytes spill loads

2022-09-17 23:48:51.953246: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-09-17 23:48:57.307073: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'input_fusion_reduce_4', 33468 bytes spill stores, 38624 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_accurate_pow', 132 bytes spill stores, 132 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 56 bytes spill stores, 48 bytes spill loads

Step      Train loss                                                                                    Test loss                                                                                     Test metric
0         [1.98e+03, 3.67e+02, 3.39e-02, 2.04e-01, 3.69e-02, 2.30e-01, 9.99e-02, 2.29e-01, 7.95e-02]    [1.98e+03, 3.67e+02, 3.39e-02, 2.04e-01, 3.69e-02, 2.30e-01, 9.99e-02, 2.29e-01, 7.95e-02]    []  
2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads
```
</details>",2022-09-18 04:54:34+00:00,"stat:awaiting response, type:support, stale, comp:core, TF 2.9"
save preprocessor is too slow for large vocabulary size: 2500e4 lookup table size ~ 0.50 hour,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


It takes near **an hour(too long)** to save the preprocessor for a lookup size ~ 2500e4.

Maybe related to `saved_model/save.py: list_children()`. 

It seems like it iterate the lookup table for each of 2500e4 elements. (Not sure)

Any suggestions?


### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""demo of preprcoessing large layers
""""""

# pylint:disable=no-member
import time
from typing import Dict

import tensorflow as tf


@tf.keras.utils.register_keras_serializable()
def split_vertical(x):
    """"""
    用于TextVectorization的split入参, split by `|`
    """"""
    return tf.strings.split(x, sep=""|"")


# 25242127 -> 3323 seconds
class TestPreprocessor(tf.keras.Model):
    def __init__(self, name=""test_preprocessor"", vocab_size: int = 100, **kwargs):
        super().__init__(name=name, trainable=False, **kwargs)
        self._lookup = {}
        self.vocab_size = vocab_size
        print(f""\nvocab_size={vocab_size}"")

    def adapt(self):
        vocab = [str(i) for i in range(self.vocab_size)]

        tic = time.time()
        text_vectorizer = tf.keras.layers.TextVectorization(
            vocabulary=vocab,
            output_mode=""int"",
            split=split_vertical,
            standardize=None,
            output_sequence_length=7,
            name=""text_vectorization_"" + ""faked_id"",
        )
        toc = time.time()
        print(f""Construct TextVectorization: {toc - tic:.2f} seconds"")
        self._lookup[""faked_id""] = text_vectorizer

    def save(self, filepath: str = ""/tmp/debug_large_vocab""):
        ds = tf.data.Dataset.from_tensor_slices({""faked_id"": [str(e) for e in range(100)]}).batch(3).map(self)
        _ = next(iter(ds))
        tic = time.time()
        super().save(filepath)
        toc = time.time()
        print(f""Save TextVectorzation:       {toc-tic:.2f} seconds"")

    def call(self, batch: Dict):
        ans = {}
        for f, lookup in self._lookup.items():
            ans[f] = lookup(batch[f])
        return ans


def main():
    vsize = [1000, 10000, 100000, 1000000, 25000000]
    for V in vsize:
        preprocessor = TestPreprocessor(vocab_size=V)
        preprocessor.adapt()
        preprocessor.save()


if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
vocab_size=1000
Construct TextVectorization: 0.01 seconds
Save TextVectorzation:       0.29 seconds

vocab_size=10000
Construct TextVectorization: 0.04 seconds
Save TextVectorzation:       0.68 seconds

vocab_size=100000
Construct TextVectorization: 0.35 seconds
Save TextVectorzation:       5.58 seconds

vocab_size=1000000
Construct TextVectorization: 3.58 seconds
Save TextVectorzation:       57.34 seconds

vocab_size=25000000
Construct TextVectorization: 90.79 seconds
Save TextVectorzation:       1612.00 seconds
```
</details>",2022-09-18 16:15:25+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.10"
Some conv2d operations remain float32 after post training full integer quantization,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monteley 12.5.1, Android 12
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.3

### 2. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

Some of the conversion codes are as follows.  

```
converter.experimental_new_quantizer = True
converter._experimental_disable_per_channel = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
```

### 3. Failure after conversion

- The conversion is successful, but some conv2d operations remain float32 after post training full integer quantization. Some conv2d operations are sandwiched between dequantize and quantize operation, and these conv2d operations has float32 weights and bias. All conv2d should have int8 weights and int32 biases.  
- I tried running this model on Pixel 6 NNAPI delegate with TFLite model benchmark tool. Some operations fallbacks to the XNNPACK delegate. I suspect that the fallback to XNNPACK delegate is due to the float32 conv2d layers and it causes a slight degradation in inference speed.
- How can I quantize all operations to int8 weights and int32 biases? Also, will quantizing all operations to int8 improve inference speed?

Quantized tflite model is here.  

[model_full_integer_quant.tflite.zip](https://github.com/tensorflow/tensorflow/files/9603395/model_full_integer_quant.tflite.zip)

### 4. (optional) RNN conversion support  

None.  

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  

The command to run model benchmark tool on Pixel 6 is as follows. I found specifying `nnapi_accelerator_name=""google-edgetpu""` accelerates the inference speed the most.  

`android_aarch64_benchmark_model --graph=./model_full_integer_quant.tflite --use_nnapi=true --nnapi_accelerator_name=""google-edgetpu"" --enable_op_profiling=true`  

The log is here.  

<details>
<summary>Click here to expand the log.</summary>

```
STARTING!
Log parameter values verbosely: [0]
Graph: [./exp_2/model_full_integer_quant.tflite]
Enable op profiling: [1]
Use NNAPI: [1]
NNAPI accelerator name: [google-edgetpu]
NNAPI accelerators available: [google-edgetpu,google-armnn,nnapi-reference]
Loaded model ./exp_2/model_full_integer_quant.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
NNAPI delegate created.
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
VERBOSE: Replacing 198 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 7 partitions.
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
VERBOSE: Replacing 18 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 7 partitions.
The input model file size (MB): 3.56052
Initialized session in 1620.78ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=7 first=114164 curr=68436 min=67413 max=114164 avg=75216.3 std=15972

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=50 first=68293 curr=66811 min=66170 max=70650 avg=68201.1 std=1021

Inference timings in us: Init: 1620775, First inference: 114164, Warmup (avg): 75216.3, Inference (avg): 68201.1
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=9.42969 overall=51.1094
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	 1614.218	  808.024	 50.129%	 50.129%	  1252.000	        2	ModifyGraphWithDelegate/0
	         AllocateTensors	 1607.685	  803.867	 49.871%	100.000%	     0.000	        2	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	 1614.218	  808.024	 50.129%	 50.129%	  1252.000	        2	ModifyGraphWithDelegate/0
	         AllocateTensors	 1607.685	  803.867	 49.871%	100.000%	     0.000	        2	AllocateTensors/0

Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	 ModifyGraphWithDelegate	        1	  1616.047	    50.129%	    50.129%	  1252.000	        2
	         AllocateTensors	        1	  1607.735	    49.871%	   100.000%	     0.000	        2

Timings (microseconds): count=1 curr=3223782
Memory (bytes): count=0
2 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	     TfLiteNnapiDelegate	    6.458	    6.441	  9.446%	  9.446%	     0.000	        1	[Identity]:219
	   TfLiteXNNPackDelegate	    5.023	    5.078	  7.448%	 16.894%	     0.000	        1	[tfl.quantize4]:222
	     TfLiteNnapiDelegate	   10.697	   10.688	 15.675%	 32.569%	     0.000	        1	[model/tf.math.add_33/Add;model/conv2d_12/Conv2D;model/conv2d_32/Conv2D;model/tf.math.add_33/Add/y1, model/tf.nn.relu_24/Relu;model/tf.math.add_35/Add;model/conv2d_12/Conv2D;model/conv2d_34/Conv2D;model/tf.math.add_35/Add/y, model/tf.math.add_71/Add;model/conv2d_9/Conv2D;model/conv2d_66/Conv2D;model/tf.math.add_71/Add/y1, model/tf.math.add_5/Add;model/conv2d_12/Conv2D;model/conv2d_5/Conv2D;model/tf.math.add_5/Add/y11, model/tf.math.add_56/Add;model/conv2d_12/Conv2D;model/conv2d_53/Conv2D;model/tf.math.add_56/Add/y1]:218
	   TfLiteXNNPackDelegate	   10.060	    9.159	 13.432%	 46.001%	     0.000	        1	[tfl.quantize1, tfl.quantize3]:221
	     TfLiteNnapiDelegate	   15.087	   15.096	 22.140%	 68.141%	     0.000	        1	[model/tf.math.add_14/Add;model/conv2d_12/Conv2D;model/conv2d_14/Conv2D;model/tf.math.add_14/Add/y11, model/tf.math.add_29/Add;model/conv2d_12/Conv2D;model/conv2d_28/Conv2D;model/tf.math.add_29/Add/y1, model/tf.math.add_99/Add;model/conv2d_transpose/stack;model/conv2d_transpose_7/conv2d_transpose;model/tf.math.add_99/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y11, model/tf.math.add_75/Add;model/conv2d_12/Conv2D;model/conv2d_70/Conv2D;model/tf.math.add_75/Add/y1]:217
	     TfLiteNnapiDelegate	    8.404	    8.342	 12.235%	 80.376%	     0.000	        1	[tfl.dequantize, model/tf.strided_slice/StridedSlice31, model/tf.math.add_11/Add;model/conv2d_9/Conv2D;model/conv2d_11/Conv2D;model/tf.math.add_11/Add/y11, model/tf.math.add_47/Add;model/conv2d_9/Conv2D;model/conv2d_44/Conv2D;model/tf.math.add_47/Add/y1]:216
	   TfLiteXNNPackDelegate	   12.543	   13.381	 19.624%	100.000%	     0.000	        1	[tfl.quantize, model/tf.math.add_94/Add;model/conv2d_9/Conv2D;model/conv2d_79/Conv2D;model/tf.math.add_94/Add/y11, tfl.quantize2]:220

============================== Top by Computation Time ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	     TfLiteNnapiDelegate	   15.087	   15.096	 22.140%	 22.140%	     0.000	        1	[model/tf.math.add_14/Add;model/conv2d_12/Conv2D;model/conv2d_14/Conv2D;model/tf.math.add_14/Add/y11, model/tf.math.add_29/Add;model/conv2d_12/Conv2D;model/conv2d_28/Conv2D;model/tf.math.add_29/Add/y1, model/tf.math.add_99/Add;model/conv2d_transpose/stack;model/conv2d_transpose_7/conv2d_transpose;model/tf.math.add_99/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y11, model/tf.math.add_75/Add;model/conv2d_12/Conv2D;model/conv2d_70/Conv2D;model/tf.math.add_75/Add/y1]:217
	   TfLiteXNNPackDelegate	   12.543	   13.381	 19.624%	 41.764%	     0.000	        1	[tfl.quantize, model/tf.math.add_94/Add;model/conv2d_9/Conv2D;model/conv2d_79/Conv2D;model/tf.math.add_94/Add/y11, tfl.quantize2]:220
	     TfLiteNnapiDelegate	   10.697	   10.688	 15.675%	 57.439%	     0.000	        1	[model/tf.math.add_33/Add;model/conv2d_12/Conv2D;model/conv2d_32/Conv2D;model/tf.math.add_33/Add/y1, model/tf.nn.relu_24/Relu;model/tf.math.add_35/Add;model/conv2d_12/Conv2D;model/conv2d_34/Conv2D;model/tf.math.add_35/Add/y, model/tf.math.add_71/Add;model/conv2d_9/Conv2D;model/conv2d_66/Conv2D;model/tf.math.add_71/Add/y1, model/tf.math.add_5/Add;model/conv2d_12/Conv2D;model/conv2d_5/Conv2D;model/tf.math.add_5/Add/y11, model/tf.math.add_56/Add;model/conv2d_12/Conv2D;model/conv2d_53/Conv2D;model/tf.math.add_56/Add/y1]:218
	   TfLiteXNNPackDelegate	   10.060	    9.159	 13.432%	 70.871%	     0.000	        1	[tfl.quantize1, tfl.quantize3]:221
	     TfLiteNnapiDelegate	    8.404	    8.342	 12.235%	 83.106%	     0.000	        1	[tfl.dequantize, model/tf.strided_slice/StridedSlice31, model/tf.math.add_11/Add;model/conv2d_9/Conv2D;model/conv2d_11/Conv2D;model/tf.math.add_11/Add/y11, model/tf.math.add_47/Add;model/conv2d_9/Conv2D;model/conv2d_44/Conv2D;model/tf.math.add_47/Add/y1]:216
	     TfLiteNnapiDelegate	    6.458	    6.441	  9.446%	 92.552%	     0.000	        1	[Identity]:219
	   TfLiteXNNPackDelegate	    5.023	    5.078	  7.448%	100.000%	     0.000	        1	[tfl.quantize4]:222

Number of nodes executed: 7
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     TfLiteNnapiDelegate	        4	    40.564	    59.495%	    59.495%	     0.000	        4
	   TfLiteXNNPackDelegate	        3	    27.616	    40.505%	   100.000%	     0.000	        3

Timings (microseconds): count=50 first=68272 curr=66794 min=66156 max=70634 avg=68184.7 std=1022
Memory (bytes): count=0
7 nodes observed

Delegate internal: 
============================== Run Order ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	        DelegateOpInvoke	    0.265	    0.283	  1.025%	  1.025%	     0.000	        1	Delegate/Convert (NC, F32, QS8):5
	        DelegateOpInvoke	    1.737	    2.685	 29.222%	 30.247%	     0.000	        3	Delegate/Convolution (NHWC, F32) IGEMM:0
	        DelegateOpInvoke	    0.035	    0.035	  0.382%	 30.629%	     0.000	        3	Delegate/Convert (NC, F32, QS8):1
	        DelegateOpInvoke	    0.978	    4.353	 31.581%	 62.210%	     0.000	        2	Delegate/Convolution (NHWC, F32) IGEMM:2
	        DelegateOpInvoke	    0.133	    0.149	  1.085%	 63.295%	     0.000	        2	Delegate/Convert (NC, F32, QS8):3
	        DelegateOpInvoke	    9.371	   10.118	 36.705%	100.000%	     0.000	        1	Delegate/Convolution (NHWC, F32) IGEMM:4

============================== Top by Computation Time ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	        DelegateOpInvoke	    9.371	   10.118	 36.705%	 36.705%	     0.000	        1	Delegate/Convolution (NHWC, F32) IGEMM:4
	        DelegateOpInvoke	    0.978	    4.353	 31.581%	 68.286%	     0.000	        2	Delegate/Convolution (NHWC, F32) IGEMM:2
	        DelegateOpInvoke	    1.737	    2.685	 29.222%	 97.508%	     0.000	        3	Delegate/Convolution (NHWC, F32) IGEMM:0
	        DelegateOpInvoke	    0.265	    0.283	  1.025%	 98.533%	     0.000	        1	Delegate/Convert (NC, F32, QS8):5
	        DelegateOpInvoke	    0.133	    0.149	  1.085%	 99.618%	     0.000	        2	Delegate/Convert (NC, F32, QS8):3
	        DelegateOpInvoke	    0.035	    0.035	  0.382%	100.000%	     0.000	        3	Delegate/Convert (NC, F32, QS8):1

Number of nodes executed: 6
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	        DelegateOpInvoke	        6	    27.562	   100.000%	   100.000%	     0.000	       12

Timings (microseconds): count=50 first=27574 curr=26304 min=26187 max=29665 avg=27565.3 std=973
Memory (bytes): count=0
6 nodes observed
```
</details>

",2022-09-20 02:05:57+00:00,"stat:awaiting response, type:support, comp:lite, TFLiteConverter, TF 2.5, ModelOptimizationToolkit"
can i use graph_transforms where the input model is saved model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2022-09-22 02:53:02+00:00,"stat:awaiting response, type:support, stale, comp:model, ModelOptimizationToolkit"
`tf.compat.v1.nn.embedding_lookup` computes wrong forward gradient when indices are out of bound,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.compat.v1.nn.embedding_lookup` computes wrong forward gradient when indices are out of bound. In the example code below, `ids=[4]` is out of bound, so the output is all zero. The jacobian therefore is all zeros. However, the forward gradient is wrongly computed as`1.0` w.r.t. to `params[1,0]`.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

params = tf.constant([[0.4], [0.6]], dtype=tf.float32)
ids = tf.constant([4], dtype=tf.int32)

def f(params):
    partition_strategy = ""mod""
    validate_indices = False
    max_norm = None
    return tf.compat.v1.nn.embedding_lookup(params, ids, 
    partition_strategy=partition_strategy, 
    validate_indices=validate_indices, 
    max_norm=max_norm, )

with tf.GradientTape(persistent=True) as tape:
    tape.watch(params)
    out = f(params)
print(""input params: \n"", params)
print(""outputs: "", out)


jac = tape.jacobian(out, [params])
print(""jacobian: "", jac)

tangents = tf.constant([[0.], [1.]])
with tf.autodiff.ForwardAccumulator(params,tangents) as acc:
  out = f(params)
print(""forward outputs: "", out)
jvp = acc.jvp(params)
print(""forward gradient: "", jvp)

```


### Relevant log output

```shell
WARNING:tensorflow:Converting IndexedSlices(indices=Tensor(""gradient_tape/Reshape_2:0"", shape=(1,), dtype=int32), values=Tensor(""gradient_tape/Reshape_1:0"", shape=(1, 1), dtype=float32), dense_shape=Tensor(""gradient_tape/Cast:0"", shape=(2,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)) to a dense representation may make it slow. Alternatively, output the indices and values of the IndexedSlices separately, and handle the vectorized outputs directly.
input params: 
 tf.Tensor(
[[0.42328522]
 [0.47058594]], shape=(2, 1), dtype=float32)
outputs:  tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
jacobian:  [<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
array([[[[0.],
         [0.]]]], dtype=float32)>]
forward outputs:  tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
forward gradient:  tf.Tensor(
[[0.]
 [1.]], shape=(2, 1), dtype=float32)
```
</details>",2022-09-29 00:57:26+00:00,"stat:awaiting response, type:bug, stale, comp:ops, TF 2.9"
Missing ops on using movenet/singlepose/lightning model ,"**System information**
- Linux (armv7l) with Mali G31 GPU
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2.6.0

Following is the output on trying to run the movenet singlpose lightning model (FP16 - taken from TFHub) on the device:
```
ERROR: Following operations are not supported by GPU delegate: 
ARG_MAX: Operation is not supported. 
CAST: Not supported cast case
CONCATENATION: OP is supported, but tensor type/shape doesn't supported. 
DEQUANTIZE: 
FLOOR_DIV: OP is supported, but tensor type/shape doesn't supported. 
GATHER_ND: Operation is not supported. 
MUL: OP is supported, but tensor type/shape doesn't supported. 
PACK: OP is supported, but tensor type/shape doesn't supported. 
RESHAPE: OP is supported, but tensor type/shape doesn't supported. 
SUB: OP is supported, but tensor type/shape doesn't supported. 
UNPACK: Operation is not supported. 

97 operations will run on the GPU, and the remaining 200 operations will run on the CPU.
```

On trying to run FP32 model available from TF Hub, following is observed:
```
ERROR: Following operations are not supported by GPU delegate: 

ARG_MAX: Operation is not supported. 
CAST: Not supported cast case 
FLOOR_DIV: OP is supported, but tensor type/shape doesn't supported. 
GATHER_ND: Operation is not supported. 
MUL: OP is supported, but tensor type/shape doesn't supported. 
PACK: OP is supported, but tensor type/shape doesn't supported. 
RESHAPE: OP is supported, but tensor type/shape doesn't supported. 
SUB: OP is supported, but tensor type/shape doesn't supported. 
UNPACK: Operation is not supported. 

97 operations will run on the GPU, and the remaining 45 operations will run on the CPU.
```

**Standalone code to reproduce the issue** 
    Used `TfLiteGpuDelegateCreate `to create TF lite OpenGL delegate with default options and the model is built with `FlatBufferModel::BuildFromFile`.

 Able to run the model though there are missing ops. The model runs slower with single digit FPS. 
1. Could you please confirm if the aforementioned ops are not supported by TF lite?
2. Is it due to the use of OpenGL and/or Mali GPU, or it is unavailable for all backend/platforms?
3. In case if custom ops are to be implemented, are there any references (code segment/doc) you can suggest which implements ops using GPU with OpenGL backend?",2022-09-29 14:17:39+00:00,"stat:awaiting response, stale, comp:lite, type:others, TFLiteGpuDelegate, 2.6.0"
"Ram memory leak when using tf.function at tf 2.8,2.9 and 2.10, doesn't happen with tf 2.5","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu, also on Windows

### Mobile device

_No response_

### Python version

3.10 and 3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Every time I use a tf.function decorator there is a memory leak in the ram, that keeps growing every iteration, I observed this in ubuntu and windows, it didn't happen with tf 2.5 but I had the server updated to python 3.10 and all the version compatible with python 3.10 (tf 2.8,2.9,2.10) have this issue. 
I tried with python 3.9 in my personal device and the issue still occurs. The leak is small but in an RL setting this is a critical issue in RL since we have to call a tf.function to take actions in the environment millions of times during training and without the decorator things go like 10 times slower.

Note that it is not an issue about not passing tensorflow objects to the wrapper, neither about having the shape of the tensor changing.

Please I need this to be fixed as soon as possible.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function
def operate(img):
    return img

img = tf.zeros((6,6,7))
for i in range (100000000):
    s= operate(img)
```


### Relevant log output

```shell
just monitor the ram, you will see it growing constantly if you are using tf>=2.8 with tf 2.5.0 ram remains constant for the code above
```
</details>",2022-10-05 11:55:16+00:00,"stat:awaiting response, type:bug, comp:tf.function, TF 2.10"
Tensorflow Build Fail with `config=asan/msan/ubsan`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TensorFlow cannot be successfully built from source with address/memory/undefined
behavior sanitizers.
```


### Standalone code to reproduce the issue

```shell
bazel build --config=asan //tensorflow/tools/pip_package:build_pip_package
bazel build --config=msan //tensorflow/tools/pip_package:build_pip_package
bazel build --config=ubsan //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
For ubsan, the building gets stuck at linking:
[12,750 / 12,808] .../python:_pywrap_tensorflow_internal.so; 15247s local  // cannot terminate

For asan/msan, the building fails with the following message:
ERROR: /home/yuyao/.cache/bazel/_bazel_yuyao/ccc1279be1ec9be064e98d7fead8e6f8/external/libjpeg_turbo/BUILD.bazel:227:8: Executing genrule @libjpeg_turbo//:simd_x86_64_assemblage23 failed: (Exit 1): bash failed: error executing command 
  (cd /home/yuyao/.cache/bazel/_bazel_yuyao/ccc1279be1ec9be064e98d7fead8e6f8/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/yuyao/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-x86_64/bin:/home/yuyao/.local/bin:/usr/local/cuda/compute-sanitizer:/usr/local/cuda/bin:/home/yuyao/.vscode-server/bin/74b1f979648cc44d385a2286793c226e611f59e7/bin/remote-cli:/home/yuyao/.conda/envs/ten-sanitizer/bin:/opt/miniconda3/condabin:/home/yuyao/.local/bin:/usr/local/cuda/compute-sanitizer:/usr/local/cuda/bin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PYTHON_BIN_PATH=/opt/miniconda3/bin/python3 \
    PYTHON_LIB_PATH=/opt/miniconda3/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jchuff-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcphuff-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctflt-sse.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctfst-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctflt-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctfst-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctred-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jquantf-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jsimdcpu.o; do
  bazel-out/k8-opt/bin/external/nasm/nasm -f elf64    -DELF -DPIC -D__x86_64__    -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfig.h)/    -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfigint.h)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jsimdcfg.inc.h)/    -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/    -o $out    $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)
done')
# Configuration: 014280d9fadad28e5975c79178a18398bf834eb4a872c13f9a858491aca2ea7f
# Execution platform: @local_execution_config_platform//:platform

=================================================================
==3375800==ERROR: LeakSanitizer: detected memory leaks

Direct leak of 14584 byte(s) in 360 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd4a17 in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236ccd4a17 in nasm_strcat external/nasm/nasmlib/malloc.c:123
    #3 0x56236cc7505a in find_label external/nasm/asm/labels.c:227
    #4 0x56236cc7535c in is_extern external/nasm/asm/labels.c:285
    #5 0x56236cc6ba70 in expr6 external/nasm/asm/eval.c:937
    #6 0x56236cc6be81 in expr5 external/nasm/asm/eval.c:567
    #7 0x56236cc6cf94 in expr4 external/nasm/asm/eval.c:542
    #8 0x56236cc6d107 in expr3 external/nasm/asm/eval.c:508
    #9 0x56236cc6d577 in expr2 external/nasm/asm/eval.c:482
    #10 0x56236cc6da47 in expr1 external/nasm/asm/eval.c:456
    #11 0x56236cc6df1b in expr0 external/nasm/asm/eval.c:430
    #12 0x56236cc6fa3c in evaluate external/nasm/asm/eval.c:988
    #13 0x56236cc7acf8 in parse_line external/nasm/asm/parser.c:879
    #14 0x56236cc54c8a in assemble_file external/nasm/asm/nasm.c:1502
    #15 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #16 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Direct leak of 14584 byte(s) in 360 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd4a17 in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236ccd4a17 in nasm_strcat external/nasm/nasmlib/malloc.c:123
    #3 0x56236cc7505a in find_label external/nasm/asm/labels.c:227
    #4 0x56236cc75286 in lookup_label external/nasm/asm/labels.c:268
    #5 0x56236cc6ba23 in expr6 external/nasm/asm/eval.c:918
    #6 0x56236cc6be81 in expr5 external/nasm/asm/eval.c:567
    #7 0x56236cc6cf94 in expr4 external/nasm/asm/eval.c:542
    #8 0x56236cc6d107 in expr3 external/nasm/asm/eval.c:508
    #9 0x56236cc6d577 in expr2 external/nasm/asm/eval.c:482
    #10 0x56236cc6da47 in expr1 external/nasm/asm/eval.c:456
    #11 0x56236cc6df1b in expr0 external/nasm/asm/eval.c:430
    #12 0x56236cc6fa3c in evaluate external/nasm/asm/eval.c:988
    #13 0x56236cc7acf8 in parse_line external/nasm/asm/parser.c:879
    #14 0x56236cc54c8a in assemble_file external/nasm/asm/nasm.c:1502
    #15 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #16 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Direct leak of 8409 byte(s) in 207 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd4a17 in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236ccd4a17 in nasm_strcat external/nasm/nasmlib/malloc.c:123
    #3 0x56236cc7505a in find_label external/nasm/asm/labels.c:227
    #4 0x56236cc75b5e in define_label external/nasm/asm/labels.c:452
    #5 0x56236cc7a75d in parse_line external/nasm/asm/parser.c:489
    #6 0x56236cc54c8a in assemble_file external/nasm/asm/nasm.c:1502
    #7 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #8 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Direct leak of 5632 byte(s) in 32 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd485c in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236cc8fd04 in do_directive external/nasm/asm/preproc.c:3062
    #3 0x56236cc9bd6f in pp_getline external/nasm/asm/preproc.c:5216
    #4 0x56236cc54c38 in assemble_file external/nasm/asm/nasm.c:1488
    #5 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #6 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Direct leak of 768 byte(s) in 192 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd485c in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236cc84113 in new_Token external/nasm/asm/preproc.c:1222
    #3 0x56236cc9d1ed in expand_mmacro external/nasm/asm/preproc.c:4901
    #4 0x56236cc9d1ed in pp_getline external/nasm/asm/preproc.c:5255
    #5 0x56236cc54c38 in assemble_file external/nasm/asm/nasm.c:1488
    #6 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #7 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Direct leak of 312 byte(s) in 3 object(s) allocated from:
    #0 0x7fc521520a06 in __interceptor_calloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:153
    #1 0x56236ccd4894 in nasm_zalloc external/nasm/nasmlib/malloc.c:85
    #2 0x56236ccae79f in elf_make_section external/nasm/output/outelf.c:396
    #3 0x56236ccae79f in elf_section_names external/nasm/output/outelf.c:463
    #4 0x56236cc678f9 in process_directives external/nasm/asm/directiv.c:249
    #5 0x56236cc54c6e in assemble_file external/nasm/asm/nasm.c:1498
    #6 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #7 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Direct leak of 128 byte(s) in 4 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd485c in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236cc87f4b in pp_reset external/nasm/asm/preproc.c:5031
    #3 0x56236cc54a08 in assemble_file external/nasm/asm/nasm.c:1481
    #4 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #5 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Indirect leak of 1536 byte(s) in 64 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd485c in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236cc9c9fb in pp_getline external/nasm/asm/preproc.c:5227
    #3 0x56236cc54c38 in assemble_file external/nasm/asm/nasm.c:1488
    #4 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #5 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

Indirect leak of 24 byte(s) in 2 object(s) allocated from:
    #0 0x7fc521520808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
    #1 0x56236ccd492f in nasm_malloc external/nasm/nasmlib/malloc.c:75
    #2 0x56236ccd492f in nasm_strdup external/nasm/nasmlib/malloc.c:104
    #3 0x56236ccae846 in elf_make_section external/nasm/output/outelf.c:407
    #4 0x56236ccae846 in elf_section_names external/nasm/output/outelf.c:463
    #5 0x56236cc678f9 in process_directives external/nasm/asm/directiv.c:249
    #6 0x56236cc54c6e in assemble_file external/nasm/asm/nasm.c:1498
    #7 0x56236cc527c8 in main external/nasm/asm/nasm.c:617
    #8 0x7fc520f14082 in __libc_start_main ../csu/libc-start.c:308

SUMMARY: AddressSanitizer: 45977 byte(s) leaked in 1224 allocation(s).
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/yuyao/dev/tensorflow/tensorflow/python/framework/BUILD:598:27 Linking tensorflow/python/framework/_op_def_util.so failed: (Exit 1): bash failed: error executing command 
  (cd /home/yuyao/.cache/bazel/_bazel_yuyao/ccc1279be1ec9be064e98d7fead8e6f8/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/yuyao/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-x86_64/bin:/home/yuyao/.local/bin:/usr/local/cuda/compute-sanitizer:/usr/local/cuda/bin:/home/yuyao/.vscode-server/bin/74b1f979648cc44d385a2286793c226e611f59e7/bin/remote-cli:/home/yuyao/.conda/envs/ten-sanitizer/bin:/opt/miniconda3/condabin:/home/yuyao/.local/bin:/usr/local/cuda/compute-sanitizer:/usr/local/cuda/bin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PYTHON_BIN_PATH=/opt/miniconda3/bin/python3 \
    PYTHON_LIB_PATH=/opt/miniconda3/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jchuff-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcphuff-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctflt-sse.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctfst-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctflt-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctfst-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctred-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jquantf-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-avx2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-sse2.o bazel-out/k8-opt/bin/external/libjpeg_turbo/simd/x86_64/jsimdcpu.o; do
  bazel-out/k8-opt/bin/external/nasm/nasm -f elf64    -DELF -DPIC -D__x86_64__    -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfig.h)/    -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfigint.h)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jsimdcfg.inc.h)/    -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/    -o $out    $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)
done')
# Configuration: 014280d9fadad28e5975c79178a18398bf834eb4a872c13f9a858491aca2ea7f
# Execution platform: @local_execution_config_platform//:platform
```
</details>",2022-10-07 02:31:05+00:00,"type:build/install, subtype:bazel, TF 2.10"
Custom layer with tf.extract_image_patches extremely slow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.7.14

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm new to tensorflow. I'm trying to implement a custom pooling layer, using owa operators (https://github.com/jiforcen/ordered-weighted-pooling). For that I'm using tf.extract_image_patches, but this operation is extremely slow when the input data dimensions are large, as raised in issue #13017. 

I believe that the pooling layer that I implemented has a behavior very similar to what is performed by the tf.keras.layers.MaxPooling2D layer. Inspecting the code of MaxPooling2D I saw that it calls the gen_nn_ops.max_pool method. 

I tried to take a look at what's inside gen_nn_ops.max_pool, but I can't find it in the repository. From what I've googled I can't find the source code because it's automatically generated by bazel. If I build from source, I'll see this file inside bazel-genfiles, right? This file contains automatically generated Python wrappers to underlying C++ implementations.

Is it possible to create a custom pooling operation in C++ and use it in my Python code? I'm adding the custom layer code that I implemented.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

from keras import backend as K
from tensorflow.keras.constraints import Constraint
from tensorflow.python.util.tf_export import keras_export
from tensorflow.python.keras.utils import conv_utils
# import skimage.measure

@keras_export('keras.constraints.UnitSumNonNeg', 'keras.constraints.unit_sum_non_neg')
class UnitSumNonNeg(Constraint):
    """"""Limits weights to be non-negative and with sum equal to one

    Also available via the shortcut function `keras.constraints.unit_sum_non_neg`.
    """"""
    def __call__(self, w):
        aux =  w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)

        return aux/(K.epsilon() + tf.reduce_sum(aux, axis=[0], keepdims=True))

class OWAPoolingNew(tf.keras.layers.Layer):
    def __init__(self,
               pool_size=(2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               name=None,
               sort=True,
               train=True, 
               seed=None,
               all_channels=False,
               **kwargs):
        super(OWAPoolingNew, self).__init__(name=name, **kwargs)

        self.pool_size = pool_size
        self.strides = pool_size if strides == None else strides
        self.padding = padding
        self.data_format = conv_utils.normalize_data_format('channels_last')
        self.sort = sort
        self.train = train
        self.seed = seed if seed != None else 10
        self.all_channels = all_channels
        
    def build(self, input_shape):
      
      if self.all_channels:
        weights_shape = (self.pool_size[0] * self.pool_size[1], input.shape[-1])
      else:
        weights_shape = (self.pool_size[0] * self.pool_size[1], 1)
      
      tf.random.set_seed(self.seed)
      kernel = tf.random.uniform(shape=weights_shape)
      kernel /= tf.reduce_sum(kernel, axis=[0], keepdims=True)
      
      self.kernel = tf.Variable(initial_value = kernel, trainable=self.train, dtype='float32', constraint=UnitSumNonNeg())

    def call(self, inputs):

        _, height, width, channels = inputs.get_shape().as_list()

        # Extract pooling regions
        stride = [1, self.strides[0], self.strides[1], 1]
        ksize = [1, self.pool_size[0], self.pool_size[1], 1]

        inputs = tf.image.extract_patches(inputs, sizes = ksize, strides = stride,
                            rates = [1, 1, 1, 1], padding='SAME')

        _, pool_height, pool_width, elems = inputs.get_shape().as_list()

        # Extract pooling regions for each channel
        elems =  int(elems / channels)
        inputs = tf.reshape(inputs, [-1, pool_height, pool_width, elems, channels]) # Reshape tensor

        # Sort values for pooling
        if self.sort:
            inputs = tf.sort(inputs, axis=-2, direction='DESCENDING', name=None)

        outputs = tf.reduce_sum(tf.math.multiply(self.kernel, inputs), axis=-2)

        return outputs
```


### Relevant log output

_No response_</details>",2022-10-13 12:48:01+00:00,"stat:awaiting tensorflower, comp:ops, type:performance, TF 2.9"
Adding Augumentation Layer extremely slow on Tensorflow 2.9 and 2.10,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9, tf2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This code for adding Agumentation Layers:
        
data_augmentation = tf.keras.Sequential(
        [
            # preprocessing.RandomCrop(34, 20),
            # preprocessing.Resizing(40, 24),  
            preprocessing.RandomFlip(""horizontal""),
            preprocessing.RandomFlip(""vertical""),
            # preprocessing.RandomZoom(0.2,0.2),
            preprocessing.RandomTranslation(0.2,0.2),
            # preprocessing.RandomRotation(0.1),
            # preprocessing.RandomContrast(0.2)
```


### Standalone code to reproduce the issue

```shell
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
```


### Relevant log output

_No response_</details>",2022-10-16 08:19:51+00:00,"stat:awaiting response, type:bug, stale, TF 2.10"
"Increased memory usage while loading model graph  and observed a drop in performance throughput , When upgrade from TF version 2.8.0 to  either 2.8.1 or 2.9.1","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.9.1 , 2.8.1

### Custom Code

Yes

### OS Platform and Distribution

Red Hat Enterprise Linux release 8.6 (Ootpa)

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
we have a docker container with memory 1.5GB  where  we load our Custom RetinaNet Model and use it for inference.
We were using the Tensorflow version 2.8.0 and everything was working fine. 

Due to the vulnerabilities ,  we  tried upgrading to Minor version 2.9.1. But we are getting SIGKILL - out of memory error during the load of the model graph. When checked the Docker stats, we could see that the memory usage is crossing the existing 1.5GB.  There is no other code changes other than the version upgrade.

After increasing the pod memory to 4GB we are able to load the model graph and get the inference. The same behaviour is seen when we try to upgrade version 2.8.1 as well.

We also see the  time taken for inference has increased about 40% when we upgrade the version from 2.8 to 2.9.1

```


### Standalone code to reproduce the issue

```shell
1) Load a Retinanet Model  with Tensorflow version 2.8.0 in a docker container with memory 1.5GB
2) Load the same Retinanet model with Tensorflow version 2.8.1/2.9.1  in a docker container with the  memory 1.5GB
3) Load the same Retinanet model with Tensorflow version 2.8.1/2.9.1  in a docker container with the  memory 4GB

In this case you can see that the step1 will succeed and in step2 it will crash due to out of memory. Step3 will succeed.
```


### Relevant log output

```shell
2022-10-12 00:33:08.206766: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-10-12 00:33:08.894968: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28753920 exceeds 10% of free system memory.
2022-10-12 00:33:09.311896: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28753920 exceeds 10% of free system memory.
2022-10-12 00:33:12.049115: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28753920 exceeds 10% of free system memory.
2022-10-12 00:33:12.279661: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28753920 exceeds 10% of free system memory.
2022-10-12 00:33:13.079146: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28753920 exceeds 10% of free system memory.
2022-10-12 00:33:16,694 INFO reaped unknown pid 8697 (exit status 0)
Process 'ForkPoolWorker-1' pid:133 exited with 'signal 9 (SIGKILL)'
```
</details>",2022-10-18 05:57:25+00:00,"stat:awaiting response, stale, comp:model, type:performance, comp:core, TF 2.9"
XLA CPU performance issues,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.10 and 2.11 branch

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

5.3.0

### GCC/Compiler version

12.1

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

```shell
CPU only.

I try to enable xla on my models, but found it became slower.

I found: without xla, tf can use all cores(8 for my case), there are enough ops to distributed across multiple cores.
but when enable xla, critical path became `_XlaRun`, and it seems to run in single thread.

I tried `--tf_xla_max_cluster_size=10`, and still slower. 

I want to know if this result is as expected, and if there is anyway to make xla utilize all cores.
```
",2022-10-18 08:38:34+00:00,"stat:awaiting response, stale, comp:xla, type:performance, TF 2.10"
"when running tflite model , the performance of GPU  is obviously slower than  cpu on PC","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.7

### Custom Code

No

### OS Platform and Distribution

win64

### Mobile device

no

### Python version

3.7

### Bazel version

no 

### GCC/Compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

111

### Current Behaviour?

```shell
device : AMD Ryzen 5 5600U with Radeon Graphics （notebook）
the notebook only has a Core graphics card.

I run a .tflite model on notebook PC , I find the time of inference time on GPU is 28ms, but on cpu by using xnnpack its time is 18ms. Why the performance of GPU  is obviously slower than  cpu? thanks！
```


### Standalone code to reproduce the issue

```shell
no
```


### Relevant log output

_No response_</details>",2022-10-31 09:11:22+00:00,"stat:awaiting response, stale, comp:lite, type:performance, TFLiteGpuDelegate, TF 2.7"
Tensorflow inference slow down: Repeatedly load and delete Keras model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

Mac OS M1 Pro

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am using Tensorflow-Macos together with Tensorflow-Metal on M1 Pro.
After loading the model and I run inference with . Everything goes well.
When I delete the model, re-build the model, re-load the weight, and re-run ```predict_on_batch```, the performance is 20% slower. What's the reason? How can I fix it?
```


### Standalone code to reproduce the issue

```shell
See descriptions above.
```


### Relevant log output

_No response_</details>",2022-11-04 23:37:16+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.10"
Bug in MLP notebook ,"I have found a problem with a memory explosion on the MLP notebook, if it's configured to use non deterministic Nvidia cuda.

What I found is that the memory usage went up massively, when I last tested it I think using gelu activation functions, instead of relu.

When I tested this it resulted in memory usage increases, until it used up all my system memory (32GB), on Ubuntu 22.10.

This was using python 3.10 and tensorflow 2.10, Nvidia cuda 11.7

Is there any possibility of fixing this?",2022-11-09 22:25:42+00:00,"stat:awaiting response, stale, TF 2.10"
Tensorflow Lite C API works much slower than C++ API,"### Issue Type

Performance

### Source

source

### Tensorflow Version

tflite v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

CMake was used

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I built TFLite v2.4.0 static library and used in our project using C++ API 

(we are using it cross-platform Ubuntu/Android/iOS). It shows inference time 5-10 ms on Ubuntu 18. 

We decided to switch to C API (iOS xcframework is available on C). But I see that on 

C API inference time becomes 45-50 ms. I tried to test using TFLite v2.10.0 C API 

and it shows 180-200 ms. The model tested in Ubuntu is a small OCR model 

with size smaller than 200 KB.
```


### Standalone code to reproduce the issue

```shell
I can't upload the whole files, so here is snippets:

class TextRecognizer {
    ...
    virtual TfLiteModel* getModel() = 0;
    TfLiteInterpreterOptions* options;
    TfLiteInterpreter* interpreter;
};
```

Cpp file:
```
vdoc::TextLinePrediction vdoc::TextRecognizer::predict(const cv::Mat &srcImage) {
    ...
    options = TfLiteInterpreterOptionsCreate();
    interpreter = TfLiteInterpreterCreate(getModel(), options);
    TfLiteInterpreterAllocateTensors(interpreter);
    ...
    TfLiteTensor *input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
    TfLiteTensorCopyFromBuffer(input_tensor, convertedImage.data,
                                   convertedImage.total() * convertedImage.elemSize());
    TfLiteInterpreterInvoke(interpreter);
    const TfLiteTensor *output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 0);
    float *output = output_tensor->data.f;
    ...
}
```


### Relevant log output

```shell
Output is measured time of each TFLite method call:
Time taken to init interpreter: 0.00191s
Time taken to allocate tensors: 0.00060s
Time taken to copy buffer data: 0.00031s
Time taken to Invoke inference: 0.05476s

As I mentioned before on C++ API inference time is about 10 times faster 0.005s.
```
",2022-11-18 15:42:56+00:00,"comp:lite, type:performance, TF 2.10"
MoviNet has memory leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 (recreated in Ubuntu 20.4)

### Mobile device

NA

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2 / 8.1

### GPU model and memory

_No response_

### Current Behaviour?

Repeated inference calls to MoviNet (loaded from SavedModel) continuously increase memory usage until a crash occurs.



### Standalone code to reproduce the issue


The process to recreate is somewhat long, so full code cannot be provided. The steps are:

1. Create movinet network:
```shell
    from official.projects.movinet.modeling import movinet, movinet_layers, movinet_model
    backbone = movinet.Movinet(model_id=model_id)   # output_states=False
    model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600, output_states=False)
    model.build([1, 1, 1, 1, 3])

    checkpoint_dir = download_and_extract_file(model_id=model_id)
    checkpoint_path = os.path.join(checkpoint_dir, 'ckpt-1')
    checkpoint = tf.train.Checkpoint(model=model)
    status = checkpoint.restore(checkpoint_path)
    status.assert_existing_objects_matched()

    model = build_classifier(backbone=backbone, input_shape= input_shape, num_classes=n_output_classes,
                             movinet_layers_to_freeze = movinet_layers_to_freeze)
```

2. Fine-tune the model

3. Convert to savedmodel:
```
tf.keras.models.save_model(self.net, path, signatures=None)
```

4. Load savedmodel:
```
model = tf.keras.models.load_model(savedmodel_path, custom_objects={'tf': tf, 'K': tf.keras.backend})
```

5. Perform inference in a for loop:
```
for i in range(n_samples):
    out[i] = model(input[i], training=training)
```


### Relevant log output

I have ran this from my Windows PC, as well as EC2 instances with and without a GPU. I used ClearML to log machine stats over iterations and saw the following:

CPU-Only:
![image](https://user-images.githubusercontent.com/19536781/203179971-490e64f6-b198-43c6-a2dc-500c8190ccae.png)

Note that in the beginning, there is alot of writing to disk (orange) while RAM stays constant (blue). After a certain amount of writing to disk, RAM starts being used until it is depleted and program crashes. 

GPU:
The same behavior appears on GPU machines, with the addition that when it starts writing to RAM, GPU starts filling up as well (top plot, blue/green lines):
![image](https://user-images.githubusercontent.com/19536781/203180382-19affc22-bd43-4dc0-8a48-e97e7e80422d.png)

I have tried this on my windows machine as well, and observed the same behavior (RAM usage increasing until crash occurs) in Task Manager. 

```
</details>",2022-11-21 23:41:06+00:00,"stat:awaiting response, type:bug, comp:keras, TF 2.10"
error occurs when convert a tflite model with uint8 inference_input_type during quantization-aware-training,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installation (pip package or built from source): tf 2.8
- TensorFlow library (version, if pip package or github SHA, if built from source): tf2.8

### 2. Code
I want to convert a model by quantization aware training. the fp32 model is simple and it can only be convert to a tflite model with fp32 input/output. how can i convert a fp32 model with uint8 intput and output, since it can be done by post-training quantization by assign the converter.inference_input_type = tf.uint8 and converter.inference_output_type  = tf.uint8.  
if i can not assign inference_input_type or inference_output_type   during quantization-aware-training, how can i convert a tflite model with specified input output dtype?
 Thank you !
**minimum code:**
```
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Conv2DTranspose


num_filters=512
inputs=tf.keras.Input((32,32,1024))

outputs= Conv2DTranspose(filters=num_filters,
                    kernel_size=(2, 2), 
                    strides=2, 
                    padding=""same"",name='convTrans')(inputs)

base_model=tf.keras.Model(inputs=inputs,outputs=outputs,name=""test_transConv2D_model"")
base_model.summary()
print(base_model.layers[1].weights[0].shape)
print(base_model.layers[1].weights[1].shape)
# (2, 2, 512, 1024)
# (512,)

import tensorflow_model_optimization as tfmot
quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)

x_train = np.random.randn(4,32, 32, 1024).astype(np.float32)
y_train = np.random.randn(4, 64, 64, 512).astype(np.float32)
quant_aware_model.compile(
    loss=tf.keras.losses.categorical_crossentropy,
    optimizer='adam',
    metrics=['accuracy']
)
quant_aware_model.fit(x_train, y_train,epochs=1)
quant_aware_model.summary()

quant_aware_model.input.set_shape((1,) + quant_aware_model.input.shape[1:])

converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

quantized_tflite_model = converter.convert()
tflite_model_filename='test_transConv2D_model_uint8_inout.tflite'
with open(tflite_model_filename, 'wb') as f:
    f.write(quantized_tflite_model)
    print(""wirte tflite file done!"")
```
**ERRORS:**
```
C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(""Statistics for quantized inputs were expected, but not ""
Traceback (most recent call last):

  File ""D:\OneDrive\AI_Working_Directory\bio_cv\zz_test_transposcvon2d.py"", line 48, in <module>
    quantized_tflite_model = converter.convert()

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\lite.py"", line 803, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\lite.py"", line 789, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\lite.py"", line 1221, in convert
    self).convert(graph_def, input_tensors, output_tensors)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\lite.py"", line 1010, in convert
    result, self._quant_mode, quant_io=self.experimental_new_quantizer)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 216, in wrapper
    raise error from None  # Re-throws the exception.

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 206, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\lite.py"", line 759, in _optimize_tflite_model
    model = _modify_model_io_type(model, m_in_type, m_out_type)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\util.py"", line 979, in modify_model_io_type
    return _convert_model_from_object_to_bytearray(model_object)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\util.py"", line 561, in _convert_model_from_object_to_bytearray
    model_offset = model_object.Pack(builder)

  File ""C:\Users\indeed\.conda\envs\TF28_py37_1\lib\site-packages\tensorflow\lite\python\schema_py_generated.py"", line 5890, in Pack
    operatorCodes = builder.EndVector(len(self.operatorCodes))

TypeError: EndVector() takes 1 positional argument but 2 were given
```

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2022-11-28 06:40:43+00:00,"stat:awaiting response, type:bug, comp:lite, TFLiteConverter, TF 2.8"
How to write to input tensors in custom op?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How can I write to input tensors in a custom op? I can read and write to output tensors but only read on inputs. (compiling outputs ""assignment of read-only location"" error when trying to write to input tensor). I want to write an inplace op, and making it inplace is crucial for my project, as writting the results to an output tensor and then assigning the output to the variable is too slow). Here is a minimal example that just sets the input tensor to zero. What code can I add to this example code so that I can be able to write to the input tensor?
```


### Standalone code to reproduce the issue

```shell
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/util/work_sharder.h""

#include <iostream>

using namespace tensorflow;

REGISTER_OP(""Example"")
    .Input(""variable: float"")
    ;

class ExampleOp : public OpKernel {
public:
    
    explicit ExampleOp(OpKernelConstruction* context) : OpKernel(context) {}

    void Compute(OpKernelContext* context) override {

        const Tensor& variable_tensor = context->input(0);
        auto variable = variable_tensor.flat<float>();
        
        for(int i = 0; i < variable.size(); ++i)
            variable(i) = 0; 

    };

};


REGISTER_KERNEL_BUILDER(Name(""Example"").Device(DEVICE_CPU), ExampleOp);
```


### Relevant log output

```shell
minimal.cc: In member function ‘virtual void ExampleOp::Compute(tensorflow::OpKernelContext*)’:
minimal.cc:25:25: error: assignment of read-only location ‘variable.Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>::operator()(((Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>::Index)i))’
   25 |             variable(i) = 0;
      |             ~~~~~~~~~~~~^~~
```
</details>",2022-11-28 22:37:01+00:00,"stat:awaiting response, type:support, stale, type:others, comp:ops, TF 2.8"
tf.numpy_function in tf.data.Dataset.map causes CUDA_ERROR_OUT_OF_MEMORY after hundreds of epochs,"### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.2

### GCC/Compiler version

gcc 9.3.0

### CUDA/cuDNN version

11.8/8.6.0

### GPU model and memory

Nvidia Quadro RTX 6000, 25.8G memory

### Current Behaviour?

```shell
I use `tf.data.Dataset` for my input pipeline as follows:
  1. `from_generator`
  2. `cache`
  3. `shuffle`
  4. `repeat`
  5. `map`
  6. `batch`
  7. `prefetch`

The step 5 call a data-augmentation function where there is a call to `tf.numpy_function` which wraps `scipy.ndimage.rotate`.

After a variable number of epochs (in general more than 100), a `CUDA_OUT_OF_MEMORY` appears and the kernel crashed. If I remove the call to `tf.numpy_function` from the data-augmentation function, I don't have this problem. Note that I don't use `tf.py_function` as it looks much slower.
```


### Standalone code to reproduce the issue

```shell
I haven't managed to create a simple reproducer for this bug.
```


### Relevant log output

```shell
2022-12-01 02:26:32.244730: E tensorflow/stream_executor/cuda/cuda_driver.cc:796] failed to alloc 17179869184 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-12-01 02:26:32.299769: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 17179869184
```
",2022-12-01 09:40:39+00:00,"stat:awaiting response, type:support, comp:data, TF 2.10"
Error in inference when the model is exported with ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tensorflow-inference-2.10.0

### Custom Code

No

### OS Platform and Distribution

ubuntu20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2.1/8.1.0.77-1

### GPU model and memory

Tesla T4

### Current Behaviour?

```shell
The model is exported like this:

MAX_SEQ_LENGTH=128
class ExportModel(tf.Module):
  def __init__(self, classifier):
    self.classifier = classifier

  @tf.function(input_signature=[{
      'input_word_ids': tf.TensorSpec(shape=[None, MAX_SEQ_LENGTH], dtype=tf.int32, name='input_word_ids'),
      'input_mask': tf.TensorSpec(shape=[None, MAX_SEQ_LENGTH], dtype=tf.int32, name='input_mask'),
      'input_type_ids': tf.TensorSpec(shape=[None, MAX_SEQ_LENGTH], dtype=tf.int32, name='input_type_ids')}]
               ,jit_compile=True
              )
  def __call__(self, inputs):
    logits =  self.classifier(inputs, training=False)
    probs = tf.nn.softmax(logits)[:,1]
    return {
        'predictions': probs
    }

On my notebook I get around 30% decrease in response time, but on a Sagemaker Endpoint with tensorflow-inference: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.10.0-cpu-py39-ubuntu20.04-sagemaker, I get this error:
external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at xla_ops.cc:296 : UNIMPLEMENTED: Could not find compiler for platform CUDA: NOT_FOUND: could not find registered compiler for platform CUDA -- was support for that platform linked in?
```


### Standalone code to reproduce the issue

```shell
1- Export any model with jit_compile=True
2- Use this image 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.10.0-cpu-py39-ubuntu20.04-sagemaker for load and inference
```


### Relevant log output

```shell
external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at xla_ops.cc:296 : UNIMPLEMENTED: Could not find compiler for platform CUDA: NOT_FOUND: could not find registered compiler for platform CUDA -- was support for that platform linked in?
```
</details>",2022-12-01 16:50:04+00:00,"stat:awaiting response, type:bug, stale, comp:core, TF 2.10"
How to convert model with multiple input?,"### 1. System information

- OS Platform and Distribution (e.g., window 10):
- TensorFlow installation (pip package):
- TensorFlow library (tensorflow 2.9.1):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

I'm use code as below, what should I change to make convertion success? 
```
import numpy as np
import tensorflow as tf
from tensorflow import keras

# how to write ""representative_dataset_gen"" function?
def representative_dataset_gen():
    for _ in range(20):
        data1 = np.random.rand(1, 3, 16, 16).astype(np.float32)
        data2 = np.random.rand(1, 3, 16, 16).astype(np.float32)
        yield [data1, data2]

# build model
kinput1 = keras.Input(shape=(16, 16, 3), batch_size=1, name=""input_1"")
kinput2 = keras.Input(shape=(16, 16, 3), batch_size=1, name=""input_2"")
conv1 = keras.layers.Conv2D(16, 3, 2)(kinput1)
conv2 = keras.layers.Conv2D(16, 3, 2)(kinput2)
out = conv1 + conv2

keras_model = keras.Model(inputs=[kinput1, kinput2], outputs=out)
keras_model.trainable = False
keras_model.summary()

# convert
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8  # or tf.int8
converter.inference_output_type = tf.uint8  # or tf.int8
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
with open(""./model.tflite"", ""wb"") as fp:
    fp.write(tflite_model)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

error info:
```
ValueError: The inference_input_type and inference_output_type must be tf.float32.
```
",2022-12-02 05:09:17+00:00,"stat:awaiting response, type:support, stale, comp:lite, TFLiteConverter, TF 2.9"
TF2.9 perf is slower,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100 

### Current Behaviour?

```shell
We migrate from tf2.4 to tf2.9, and observed that the training speed of some models has ~20% decrease. 

On tf2.4, it takes ~30mins after starting the job, before processing the 1st batch. Training speed increase and then become stable.
On tf2.9, it takes ~5mins after starting the job, before processing the 1st batch. Training speed does not increase.

Q1: Can we use tf2.4 to train the model and use tf2.9 for inferencing? Any potential issues?
Q2: How can we find the root cause of tf2.9 training slowness?
```


### Standalone code to reproduce the issue

```shell
Can't share the source code.
```


### Relevant log output

_No response_</details>",2022-12-05 23:42:19+00:00,"stat:awaiting response, stale, comp:model, type:performance, TF 2.9"
How to tell choosen tf.data.AUTOTUNE values? Troubleshooting low parallelism issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to tell Autotune actual chosen values?  
I'm training a NN on a machine with 1 GPU and 8 vCPUs. I see that the GPU utilization is about 20% and CPU utilization pegs at 200%/800%. Was expecting either of these to be closer to be fully utilized.
I'm using tf.data and set parallelism of different operations [like map(), prefetch()] to -1 (Autotune).  
Based on the 200%/800% cpu usage it seems the degree of parallization isn't high enough, how can I figure out what values were chosen for num_parallel_calls and other parallel parameters that I asked to be auto tune. e.g., Any logging I can turn on?
Before diving into serious profiling work, I would like to check these basic values.
```


### Standalone code to reproduce the issue

```shell
Not necessary in order to address the issue.
```


### Relevant log output

_No response_</details>",2022-12-08 11:49:15+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.8"
Significant slowdown in Keras model.fit() on simple problem when using validation data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2 / 8.1.0

### GPU model and memory

NVIDIA Quadro GV100 (32 GB)

### Current Behaviour?

I observe a significant slowdown when using Keras model.fit() to train a simple
3-layer fully-connected network when the validation_data parameter is set.  A
typical run of the provided code on my machine results in the following output:
```
Time with validation:     3.89 s
Time without validation:  0.39 s
```
The validation data set is 1/8 the size of the training dataset.  I don't
expect validation to be free, but it should not result in a 10x slowdown.

Some notes / things I've tried:

 - Setting validation_batch_size makes no difference.

 - Using a single NumPy array to hold both the training and validation datasets
   and using validation_split instead of validation_data also makes no
   difference.

 - I found the possibly related issue #57645, which suggests that it might be a
   problem related to eager execution.  The poster of that issue is able to fix
   things by calling tf.compat.v1.disable_eager_execution(), but if I try this
   in my own code with TF 2.11.0, I run into errors (not really surprising).

 - One of the respondents on that same issue suggested swapping out the NumPy
   arrays for tf.data.Dataset objects.  I found this made no difference, even
   using `.prefetch(tf.data.AUTOTUNE)` as recommended.

 - Deactivating the GPU (by setting CUDA_VISIBLE_DEVICES=-1) makes no
   difference to the time with validation but increases the time without
   validation by about 50%.  That suggests the validation step might not be
   leveraging the GPU, which would certainly be an issue, but that's still
   not enough to account for a 6x slowdown.

Is this a bug?  Or am I missing something obvious?

### Standalone code to reproduce the issue

```python
import tensorflow as tf
import numpy as np
import time
    
xTrain = np.random.randn(2048, 128)
yTrain = np.random.randn(2048, 128)
xValid = np.random.randn(256,  128)
yValid = np.random.randn(256,  128)
                
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape = (128,)),
    tf.keras.layers.Dense(128, activation = ""tanh""),
    tf.keras.layers.Dense(128)
])      
        
model.compile(
    optimizer = tf.keras.optimizers.Adam(1.0e-3),
    loss = tf.keras.losses.MeanSquaredError(),
)   
    
# Preliminary run to build graphs.
model.fit(x = xTrain, y = yTrain, epochs = 5, batch_size = 2048,
    validation_data = (xValid, yValid)
)

t1 = time.time()

model.fit(x = xTrain, y = yTrain, epochs = 100, batch_size = 2048,
    validation_data = (xValid, yValid)
)

t2 = time.time()

model.fit(x = xTrain, y = yTrain, epochs = 100, batch_size = 2048)

t3 = time.time()

print(""Time with validation:     %.2f s"" % (t2 - t1))
print(""Time without validation:  %.2f s"" % (t3 - t2))
```


### Relevant log output

_No response_</details>",2022-12-08 22:13:59+00:00,"comp:keras, type:performance, TF 2.11"
Memory leak problem due to multi-line comments,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

None

### Python version

3.8.10

### Bazel version

5.3.2

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

Used CPU build

### GPU model and memory

Used CPU build

### Current Behaviour?

```shell
Multi-line comments are not ignored by the compiler(?), and loaded to memory.
Both source build installation on local machine and pip binary installation were tested.
I suggest changing all multi-line comments to single line comments.
```


### Standalone code to reproduce the issue

```shell
Following github repo. can reproduce the issue.
https://github.com/Armature/dump_memory
```


### Relevant log output

```shell
15751362 bytes of 35521408 bytes ( 44.3433% ) are alphabets.
```
</details>",2022-12-12 09:33:35+00:00,"stat:awaiting response, stale, type:performance, comp:core, TF 2.11"
"while tflite model has Dequantize, gpu deledate performance is slower on android for  2.10 version","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7 and 2.10

### Custom Code

Yes

### OS Platform and Distribution

android

### Mobile device

android

### Python version

3.7

### Bazel version

no

### GCC/Compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

111

### Current Behaviour?

```shell
A bug happened!
Here compare performance between 2.7 version  and 2.10 version
the 2.7 version date is 2021/11/10.
tflite model has dequantize operator （for weight and bias ,input is  f16 ,output is fp32）.
Gpu delegate (opencl)performace on 2.10 is slower than 2.7 ,the diffrence is a little big
my model is 128 ms in 2.7 ,but   78ms in 2.10.
I want to know the reason,thanks
```


### Standalone code to reproduce the issue

```shell
If model has dequantize operator,Gpu delegate performace is slower in new version for example 2.10.
```


### Relevant log output

_No response_</details>",2022-12-16 05:30:00+00:00,"stat:awaiting response, type:performance, TFLiteGpuDelegate, TF 2.10"
Crash when running tensorflow.python.ops.ragged.ragged_string_ops.ngrams,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When tensorflow.python.ops.ragged.ragged_string_ops.ngrams is given input parameter ngram_width with very large elements, the program crashes due to high memory usage.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops.ragged import ragged_string_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = ""b'a'""
      arg_0_0_1 = ""b'z'""
      arg_0_0 = [arg_0_0_0,arg_0_0_1,]
      arg_0_1_0 = ""b'b'""
      arg_0_1_1 = ""b''""
      arg_0_1 = [arg_0_1_0,arg_0_1_1,]
      arg_0_2_0 = ""b'e'""
      arg_0_2_1 = ""b'f'""
      arg_0_2 = [arg_0_2_0,arg_0_2_1,]
      arg_0 = [arg_0_0,arg_0_1,arg_0_2,]
      ngram_width = 12509
      separator = ""b'|'""
      pad_values_0 = ""b'LP'""
      pad_values_1 = ""b'RP'""
      pad_values = [pad_values_0,pad_values_1,]
      ragged_string_ops.ngrams(arg_0,ngram_width=ngram_width,separator=separator,pad_values=pad_values,)
  except Exception as e:
    ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,arg_0_0_1,]
      arg_0_1 = [arg_0_1_0,arg_0_1_1,]
      arg_0_2 = [arg_0_2_0,arg_0_2_1,]
      arg_0 = [arg_0_0,arg_0_1,arg_0_2,]
      pad_values = [pad_values_0,pad_values_1,]
      ragged_string_ops.ngrams(arg_0,ngram_width=ngram_width,separator=separator,pad_values=pad_values,)
  except Exception as e:
    ""Error:""+str(e)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:18:16.437999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.392284: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:18:17.392761: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:18:17.429344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.429509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:18:17.429533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.431839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:18:17.431883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:18:17.432835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:18:17.433038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:18:17.435378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:18:17.435859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:18:17.435958: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:18:17.436039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.436195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.436294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:18:17.437215: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:18:17.437300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.437411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:18:17.437427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.437442: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:18:17.437453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:18:17.437465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:18:17.437476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:18:17.437487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:18:17.437497: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:18:17.437507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:18:17.437553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.437682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.437772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:18:17.437796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.757806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:18:17.757832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:18:17.757837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:18:17.757972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.758091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.758181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.758259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4661 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
```
</details>",2022-12-25 19:23:39+00:00,"stat:awaiting response, type:bug, comp:ops, TF 2.4"
Crash when running tf.test.create_local_cluster,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tf.test.create_local_cluster is given input parameter num_workers with negative integers, it results in crash via high memory usage.
```


### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import os
try:
  num_workers = -2
  num_ps = 2
  tf.test.create_local_cluster(num_workers=num_workers,num_ps=num_ps,)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:47:09.860092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:10.804554: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:47:10.805055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:47:10.834019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:10.834127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:47:10.834141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:10.835582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:47:10.835611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:47:10.836274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:47:10.836408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:47:10.837944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:47:10.838270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:47:10.838382: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:47:10.838472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:10.838617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:10.838725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:47:10.838743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:11.149744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:47:11.149777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:47:11.149783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:47:11.149944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.150080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.150173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.150254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:ps/replica:0/task:0/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:47:11.152650: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job ps -> {0 -> localhost:15553, 1 -> localhost:16499}
2022-12-25 14:47:11.152660: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {}
2022-12-25 14:47:11.153114: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:15553
2022-12-25 14:47:11.153309: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:47:11.153410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:47:11.153515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:11.153546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:47:11.153556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:47:11.153566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:47:11.153576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:47:11.153585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:47:11.153594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:47:11.153604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:47:11.153649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:47:11.153820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:47:11.153825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:47:11.153828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:47:11.153874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.154027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:ps/replica:0/task:1/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:47:11.157618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job ps -> {0 -> localhost:15553, 1 -> localhost:16499}
2022-12-25 14:47:11.157632: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {}
2022-12-25 14:47:11.157808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:16499
```
</details>",2022-12-25 19:47:35+00:00,"stat:awaiting response, type:bug, comp:ops, TF 2.4"
Training horizontally stacked layers does not happen in parallel," ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### Current Behaviour?

When using Keras to build a multiple layer model, by stacking layers horizontally instead of vertically (deep network), training happens sequentially, and adding more layers increases each epoch duration linearly, while GPU utilization and memory usage remains the same as a single layer regardless of the number of total layers.



### Standalone code to reproduce the issue

In the example below I would expect that each sub-network called though the `cnn_example()` function, would run in parallel before the `GlobalAveragePooling1D` operation is called. However this does not happen, regardless the number of times the layer is repeated. Is this expected behavior or am I missing something?
```
import tensorflow as tf
from tensorflow.keras import layers

input_img= layers.Input(shape=(112, 112, 3))

x = [cnn_example()(input_img) for _ in range(n_repeats)]
x = layers.Lambda(lambda x: tf.stack(x, axis=1))(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dense(1)(x)

model = tf.keras.Model(input_img, x)
```
",2022-12-29 11:45:23+00:00,"stat:awaiting response, type:bug, stale, comp:keras, TF 2.7"
SIMJO77 ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-01-03 19:24:12+00:00,"comp:lite, type:others, TFLiteConverter"
Pre loads pages and crashes during transition in colab,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-01-05 07:34:10+00:00,TFLiteConverter
custom model with TF2 (2.11.0) for M.2 Edge TPU,"Hello.

I am new to this matter.
Through a lot of reading and various guides, I managed to get TensorFlow 2.11.0 running on Ubuntu 20.04 WSL.
I was also able to create a model and use the tensoboard to see the progress.

But the TF2 models don't seem to work with the Coral TPU.

Or I am doing something fundamentally wrong.

Maybe someone has an idea what could be wrong.

My setup....

from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md , I got SSD MobileNet V2 FPNLite 320x320.

from https://github.com/tensorflow/models a git-clone

with generate_tfrecord.py I created train.record and test.record

with
model_main_tf2.py --model_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline.config --num_train_steps=2000

created the model.

with
model_main_tf2.py --model_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline. config --checkpoint_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite

started an eval.

with
exporter_main_v2.py --input_type=image_tensor --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline. config --trained_checkpoint_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --output_directory=/home/dom/tensorflow/workspace/training_demo/exported-models/

and

export_tflite_graph_tf2.py --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline. config --trained_checkpoint_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --output_directory=/home/dom/tensorflow/workspace/training_demo/exported-models/tfliteexport/

one freeze and one model export each as TFLite

finally per
tflite_convert --saved_model_dir=/home/dom/tensorflow/workspace/training_demo/exported-models/tfliteexport/saved_model --output_file=/home/dom/tensorflow/workspace/training_demo/exported-models/tfliteexport/saved_model/detect. tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess: 2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_nudging_weights_to_use_fast_gemm_kernel=true

as last then per
edgetpu_compiler -s detect.tflite to create the coral file.

but with this probelem.....

input model: detect.tflite
input size: 10.97MiB
output model: detect_edgetpu.tflite
Output size: 10.97MiB
On-chip memory used for caching model parameters: 0.00B
On-chip memory remaining for caching model parameters: 0.00B
Off-chip memory used for streaming non-cached model parameters: 0.00B
Number of edge TPU subgraphs: 0
Total number of operations: 157
Operation log: detect_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 0
Number of operations that will run on CPU: 157
See the operation log file for individual operation details.

pipeline.config

model {
ssd {
num_classes: 5
image_resizer {
fixed_shape_resizer {
height: 320
width: 320
}
}
feature_extractor {
type: ""ssd_mobilenet_v2_fpn_keras""
depth_multiplier: 1.0
min_depth: 16
conv_hyperparams {
regularizer {
l2_regularizer {
weight: 3.9999998989515007e-05
}
}
initializer {
random_normal_initializer {
mean: 0.0
stddev: 0.009999999776482582
}
}
activation: RELU_6
batch_norm {
decay: 0.996999979019165
scale: true
epsilon: 0.0010000000474974513
}
}
use_depthwise: true
override_base_feature_extractor_hyperparams: true
fpn {
min_level: 3
max_level: 7
additional_layer_depth: 128
}
}
box_coder {
faster_rcnn_box_coder {
y_scale: 10.0
x_scale: 10.0
height_scale: 5.0
width_scale: 5.0
}
}
matcher {
argmax_matcher {
matched_threshold: 0.5
unmatched_threshold: 0.5
ignore_thresholds: false
negatives_lower_than_unmatched: true
force_match_for_each_row: true
use_matmul_gather: true
}
}
similarity_calculator {
iou_similarity {
}
}
box_predictor {
weight_shared_convolutional_box_predictor {
conv_hyperparams {
regularizer {
l2_regularizer {
weight: 3.9999998989515007e-05
}
}
initializer {
random_normal_initializer {
mean: 0.0
stddev: 0.009999999776482582
}
}
activation: RELU_6
batch_norm {
decay: 0.996999979019165
scale: true
epsilon: 0.0010000000474974513
}
}
depth: 128
num_layers_before_predictor: 4
kernel_size: 3
class_prediction_bias_init: -4.599999904632568
share_prediction_tower: true
use_depthwise: true
}
}
anchor_generator {
multiscale_anchor_generator {
min_level: 3
max_level: 7
anchor_scale: 4.0
aspect_ratios: 1.0
aspect_ratios: 2.0
aspect_ratios: 0.5
scales_per_octave: 2
}
}
post_processing {
batch_non_max_suppression {
score_threshold: 9.99999993922529e-09
iou_threshold: 0.6000000238418579
max_detections_per_class: 100
max_total_detections: 100
use_static_shapes: false
}
score_converter: SIGMOID
}
normalize_loss_by_num_matches: true
loss {
localization_loss {
weighted_smooth_l1 {
}
}
classification_loss {
weighted_sigmoid_focal {
gamma: 2.0
alpha: 0.25
}
}
classification_weight: 1.0
localization_weight: 1.0
}
encode_background_as_zeros: true
normalize_loc_loss_by_codesize: true
inplace_batchnorm_update: true
freeze_batchnorm: false
}
}
train_config {
batch_size: 6
data_augmentation_options {
random_horizontal_flip {
}
}
data_augmentation_options {
random_crop_image {
min_object_covered: 0.0
min_aspect_ratio: 0.75
max_aspect_ratio: 3.0
min_area: 0.75
max_area: 1.0
overlap_thresh: 0.0
}
}
sync_replicas: true
optimizer {
momentum_optimizer {
learning_rate {
cosine_decay_learning_rate {
learning_rate_base: 0.07999999821186066
total_steps: 50000
warmup_learning_rate: 0.026666000485420227
warmup_steps: 1000
}
}
momentum_optimizer_value: 0.8999999761581421
}
use_moving_average: false
}
fine_tune_checkpoint: ""/home/dom/tensorflow/workspace/training_demo/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0""
num_steps: 50000
startup_delay_steps: 0.0
replicas_to_aggregate: 8
max_number_of_boxes: 100
unpad_groundtruth_tensors: false
fine_tune_checkpoint_type: ""detection""
fine_tune_checkpoint_version: V2
}
train_input_reader {
label_map_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/label_map.pbtxt""
tf_record_input_reader {
input_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/train.record""
}
}
eval_config {
metrics_set: ""coco_detection_metrics""
use_moving_averages: false
}
eval_input_reader {
label_map_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/label_map.pbtxt""
shuffle: false
num_epochs: 1
tf_record_input_reader {
input_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/test.record""
}
}

graph_rewriter {
quantization {
delay: 48000
weight_bits: 8
activation_bits: 8
}
}

pip list

Package Version

absl-py 1.4.0
anyio 3.6.2
apache-beam 2.43.0
argon2-cffi 21.3.0
argon2-cffi-bindings 21.2.0
arrow 1.2.3
astor 0.8.1
asttokens 2.2.1
astunparse 1.6.3
attrs 22.2.0
avro-python3 1.10.2
backcall 0.2.0
beautifulsoup4 4.11.1
bleach 5.0.1
cachetools 5.2.1
certifi 2022.12.7
cffi 1.15.1
chardet 5.1.0
charset-normalizer 2.1.1
click 8.1.3
cloudpickle 2.2.0
cmake 3.25.0
colorama 0.3.3
comm 0.1.2
contextlib2 21.6.0
contourpy 1.0.6
crcmod 1.7
cycler 0.11.0
cython 0.29.33
debugpy 1.6.5
decorator 5.1.1
defusedxml 0.7.1
dill 0.3.1.1
dm-tree 0.1.8
docopt 0.6.2
entrypoints 0.4
etils 1.0.0
executing 1.2.0
fastavro 1.7.0
fasteners 0.18
fastjsonschema 2.16.2
flatbuffers 23.1.4
fonttools 4.38.0
fqdn 1.5.1
guest 0.4.0
gin-config 0.5.0
git-clone 1.0.6
google-api-core 2.11.0
google-api-python client 2.72.0
google-auth 2.16.0
google-auth-httplib2 0.1.0
google-auth-oauthlib 0.4.6
google-pasta 0.2.0
googleapis-common-protos 1.58.0
grpcio 1.51.1
h5py 3.7.0
hdfs 2.7.0
html5lib 1.1
httplib2 0.20.4
idna 3.4
immutabledict 2.2.3
importlib-metadata 6.0.0
importlib-Ressourcen 5.10.2
ipykernel 6.20.1
ipython 8.8.0
ipython-genutils 0.2.0
ipywidgets 8.0.4
isoduration 20.11.0
jedi 0.18.2
Jinja2 3.1.2
joblib 1.2.0
jsonpointer 2.3
jsonschema 4.17.3
jupyter 1.0.0
jupyter-client 7.4.8
jupyter-Konsole 6.4.4
jupyter-core 5.1.3
jupyter-events 0.6.2
jupyter-server 2.0.6
jupyter-server-terminals 0.4.4
jupyterlab-pygments 0.2.2
jupyterlab-widgets 3.0.5
kaggle 1.5.12
keras 2.11.0
Keras-Applikationen 1.0.8
Keras-Vorverarbeitung 1.1.2
kiwisolver 1.4.4
libclang 15.0.6.1
lvis 0.5.3
lxml 4.9.2
Markdown 3.4.1
MarkupSafe 2.1.1
matplotlib 3.6.3
matplotlib-inline 0.1.6
mistune 2.0.4
nbclassic 0.4.8
nbclient 0.7.2
nbconvert 7.2.7
nbformat 5.7.3
nest-asyncio 1.5.6
notebook 6.5.2
notebook-shim 0.2.2
numpy 1.22.4
oauth2client 4.1.3
oauthlib 3.2.2
objekt-erkennung 0.1
objsize 0.5.2
opencv-python 4.7.0.68
opencv-python-headless 4.7.0.68
opt-einsum 3.3.0
orjson 3.8.5
paketierung 23.0
pandas 1.5.2
pandoc-Filter 1.5.0
parso 0.8.3
pexpect 4.8.0
pickleshare 0.7.5
pillow 9.4.0
pip 20.2.4
platformdirs 2.6.2
portalocker 2.6.0
prometheus-Klient 0.15.0
promise 2.3
prompt-toolkit 3.0.36
proto-plus 1.22.2
protobuf 3.19.6
psutil 5.9.4
ptyprocess 0.7.0
pure-eval 0.2.2
py-cpuinfo 9.0.0
pyarrow 9.0.0
pyasn1 0.4.8
pyasn1-Baustein 0.2.8
pycocotools 2.0.6
pycparser 2.21
pydot 1.4.2
Pygments 2.14.0
pymongo 3.13.0
pyparsing 2.4.7
pyrsistent 0.19.3
python-dateutil 2.8.2
python-json-logger 2.0.4
python-slugify 7.0.0
pytz 2022.7
PyYAML 5.4.1
pyzmq 25.0.0
qtconsole 5.4.0
QtPy 2.3.0
regex 2022.10.31
requests 2.28.1
requests-oauthlib 1.3.1
rfc3339-validator 0.1.4
rfc3986-validator 0.1.1
rsa 4.9
sacrebleu 2.2.0
scikit-learn 1.2.0
scipy 1.10.0
Send2Trash 1.8.0
sentencepiece 0.1.97
seqeval 1.2.2
setuptools 65.6.3
six 1.16.0
sniffio 1.3.0
soupsieve 2.3.2.post1
stack-data 0.6.2
tabulieren 0.9.0
tensorboard 2.11.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit 1.8.1
tensorflow 2.11.0
tensorflow-addons 0.19.0
tensorflow-datensätze 4.8.1
tensorflow-schätzer 2.11.0
tensorflow-hub 0.12.0
tensorflow-io 0.29.0
tensorflow-io-gcs-filesystem 0.29.0
tensorflow-metadaten 1.12.0
tensorflow-model-optimization 0.7.3
tensorflow-text 2.11.0
termcolor 1.1.0
terminado 0.17.1
text-unidecode 1.3
tf-models-official 2.11.2
tf-slim 1.1.0
threadpoolctl 3.1.0
tinycss2 1.2.1
toml 0.10.2
tornado 6.2
tqdm 4.31.1
traitlets 5.8.1
typguard 2.13.3
typing-erweiterungen 4.4.0
unzip 1.0.0
uri-vorlage 1.2.0
uritemplate 4.1.1
urllib3 1.26.14
wcwidth 0.2.5
webcolors 1.12
webencodings 0.5.1
websocket-client 1.4.2
Werkzeug 2.2.2
rad 0.37.1
widgetsnbextension 4.0.5
wrapt 1.14.1
zipp 3.11.0
zstandard 0.19.0",2023-01-13 17:55:59+00:00,
How to totally restore a session to avoid sess.run speed declination?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Here's part of my code:

```python
 def fun(self):
        self.lr=1e-2
        self.gen_optimizer()
        self.sess.run(tf.variables_initializer(self._optimizer.variables()))
        self.sess_backup=copy.copy(self.sess)
        #tf.train.Saver().save(self.sess,'tmp.ckpt')
        for u in range(0,self.num_test_users): 
            print('Evaluating user',u,':')
            self.sess=copy.copy(self.sess_backup)
            #tf.train.Saver().restore(self.sess,'tmp.ckpt') 
            #self.sess.run(tf.variables_initializer(self._optimizer.variables()))
            tmp1,tmp2,tmp3=self.fun2(12,u)
            for i in range(0,self.adapt_itrs): #local adaption
                self.adjust(i,tmp1,tmp2,tmp3,u)
```
I want to totally restore the session each time it enters the for loop. Just as what is commented out, I tried to use tf.train.Saver, but it can't totally restore: in the for loop, the model is trained several times using sess.run, and as time goes by, the running speed is slower and slower, which means that there must be redundant graphs stored in self.sess. So I tried to use copy.copy to copy the whole session as a backup to be restored later, but it reports that `Run with options is not supported for this session.` since the following steps use the feed_dict.



### Standalone code to reproduce the issue

Here's a meaning less program but it can clarify my problem:
```python
import copy
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
sess=tf.Session()
_input=tf.placeholder(dtype=tf.float32, shape=[100, 20], name=""input"")
_output=tf.placeholder(dtype=tf.float32, shape=[100, 1], name=""output"")
W = tf.get_variable(name=""W"", initializer=tf.truncated_normal(shape=[20, 1],
                                         mean=0, stddev=0.03),dtype=tf.float32)
output=tf.matmul(_input,W)
loss=tf.reduce_sum(tf.square(_output-output))
_optimizer = tf.train.AdamOptimizer(0.01)
optimizer = _optimizer.minimize(loss, global_step=tf.Variable(0, trainable=False))
input_mat=np.random.rand(100,20)
output_mat=input_mat@np.random.rand(20,1)
init=tf.global_variables_initializer()
sess.run(init)
_,l=sess.run([optimizer,loss],feed_dict={_input:input_mat,_output:output_mat})
for i in range(5):
    '''
    Each time entering the loop, reset the session to the original one.
    Don't use tf.train.Saver, because it cannot totally restore the whole session since the running speed
    is still lower and lower.
    '''
    sess2=copy.copy(sess)
    _,l=sess2.run([optimizer,loss],feed_dict={_input:input_mat,_output:output_mat}) #Reports ""Run with options is not supported for this session.""
    sess2.close()
sess.close()
```


### Relevant log output

```shell
= RESTART: D:/programming/python/research_backup/AutoRec/Autorec_filmtrust_exp2/try.py
WARNING:tensorflow:From D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Traceback (most recent call last):
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1377, in _do_call
    return fn(*args)
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1360, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1453, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.UnimplementedError: Run with options is not supported for this session.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/programming/python/research_backup/AutoRec/Autorec_filmtrust_exp2/try.py"", line 26, in <module>
    _,l=sess2.run([optimizer,loss],feed_dict={_input:input_mat,_output:output_mat}) #Reports ""Run with options is not supported for this session.""
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1370, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1396, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:

Run with options is not supported for this session.
```
```
</details>",2023-01-16 03:01:08+00:00,"stat:awaiting response, stale, type:others, comp:apis, TF 2.8"
tf.lite from tensorflow runs faster than the tflite_runtime,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu

### Mobile device

Arm64

### Python version

3.9

### Bazel version

a

### GCC/Compiler version

a

### CUDA/cuDNN version

a

### GPU model and memory

a

### Current Behaviour?

```shell
If you run the tf.lite method from tf 2.11.0 its approx 30-50% faster than the tflite_runtime 2.11.0

I have been trying different compile options as presume something in the build if giving the full TF Lite methods better performnce, but not found anything.

All works fine but just thought I would mention tflite_runtime does seem slower and curious to why?
```


### Standalone code to reproduce the issue

```shell
a
```


### Relevant log output

```shell
a
```
</details>",2023-01-16 16:38:59+00:00,"stat:awaiting response, stale, comp:lite, comp:runtime, type:performance, TF 2.11"
TFLite performance breakdown on GAN models seems inaccurate (using TFLite benchmarking tool),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TF2.7

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

3.7.2

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
My goal in running these models in TFLite was to understand the performance breakdown for each type of layer within the GAN models. 

From the ESRGAN model the benchmarking tool reports the following: 
node type	count	avg_ms	avg %
CONV_2D	169	12217.5	89.00%
CONCATENATION	132	596.505	4.35%
TRANSPOSE_CONV	2	475.784	3.47%
LEAKY_RELU	136	319.739	2.33%
ADD	47	103.243	0.75%
MUL	11	14.042	0.10%

 There is a very limited number of TRANSPOSE_CONV layers which barely affect the model's performance. From my understanding of most GAN models, transposed convolutional layers are usually the bottleneck. Research papers have shown performance (latency) improvements across the network by simply accelerating the transposed convolutional layers, but how does it become so effective if these layers are only a small amount of the performance breakdown?

How reliable is the performance benchmarking tool for TFLite in profiling GAN models? Are these results accurate, or is there potential for error where inference time breakdown is represented accurately?
```


### Standalone code to reproduce the issue

```shell
I used the ERSGAN model and Style Transfer ""magenta"" model available at:  https://tfhub.dev/captain-pool/lite-model/esrgan-tf2/1  and https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1 respectively.
I used the Benchmark Model binary available at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark compiled from T2.7 source code.

I simply ran the benchmarking tool with the following parameters:
./benchmark_model  --use_gpu=false //
--num_threads=1 //
--enable_op_profiling=true //
--graph=./models/esrgan.tflite //
--num_runs=2 //
--warmup_runs=0 //
--warmup_min_secs=0 //
--use_vm_sim_delegate=false //
--print_postinvoke_state=true //
--profiling_output_csv_file=test.csv
```


### Relevant log output

_No response_</details>",2023-01-25 16:19:53+00:00,"stat:awaiting response, stale, comp:lite, type:performance, TF 2.7"
## cc,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-01-27 04:10:24+00:00,"stat:awaiting response, invalid, TFLiteConverter"
Slow compilation of certain operation files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

Not applicableapplicable

### Python version

Not applicable

### Bazel version

Not applicable

### GCC/Compiler version

clang14

### CUDA/cuDNN version

Not applicable

### GPU model and memory

Not applicable

### Current Behaviour?

```shell
Compiling certain ops takes too many time.
Here is a table with measurement of 10 slowest files:
The longest 10 tasks being:

* [673513 ms]: tensorflow/core/kernels/gather_op.cc
* [624861 ms]: tensorflow/core/kernels/argmax_op.cc
* [576809 ms]: tensorflow/core/kernels/resource_variable_ops.cc
* [526177 ms]: tensorflow/core/kernels/conv_grad_input_ops.cc
* [404275 ms]: tensorflow/core/kernels/bias_op.cc 
* [400613 ms]: tensorflow/core/kernels/batch_matmul_op_real.cc
* [395665 ms]: tensorflow/core/kernels/cwise_op_select.cc
* [381854 ms]: tensorflow/core/kernels/matmul_op.cc
* [379893 ms]: tensorflow/core/kernels/conv_ops.cc
* [340674 ms]: tensorflow/core/kernels/cwise_op_equal_to_1.cc
```

These measurements were done on `Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz`.
Out internal build system, being a heterogenious cluster, might schedule these compilations on older CPUs, where single compilation can take up to 15 minutes according to out measurements.

It looks like simply splitting macro instantations into multiple files will allow running compilations on parallel.

I will provide `gather_op.cc` split as an example.


### Standalone code to reproduce the issue

```shell
I am omitting -I and -D flags for the sake of brewity.
I believe that internal build system is able to provide the same statistics as well


clang++-c -m64 -O3 -g -ggnu-pubnames -fexceptions -fno-common -fuse-init-array -fcolor-diagnostics -faligned-allocation -fdebug-default-version=4 -ffunction-sections -fdata-sections -fopenmp=libomp -fopenmp-version=52 -msse2 -msse3 -mssse3 -msse4.1 -msse4.2 -mpopcnt -mcx16 -std=c++20 -w -Wno-everything $(file)
```
```


### Relevant log output

_No response_</details>",2023-01-31 15:32:39+00:00,"type:build/install, comp:ops, TF 2.4, awaiting PR merge"
RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 34 (RSQRT) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

```
!pip install git+https://github.com/openai/whisper.git 
!pip install onnx
!pip install onnx_tf
!git clone https://github.com/usefulsensors/openai-whisper.git
!git clone https://github.com/openai/whisper.git 
%%capture
!pip install optimum[onnxruntime] transformers git+https://github.com/openai/whisper.git
# -*- coding: utf-8 -*-
import warnings
warnings.filterwarnings(""ignore"")

from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
from transformers import (
    set_seed,
    AutoProcessor
)
from pathlib import Path
import os

SEED = 42

# Export vanilla & optimized onnx model
def export_vanilla_optimized_onnx(model_checkpoint):
    set_seed(SEED)
    processor = AutoProcessor.from_pretrained(model_checkpoint)

    # Vanilla
    model = ORTModelForSpeechSeq2Seq.from_pretrained(model_checkpoint, from_transformers=True, use_cache=True)
    onnx_path = Path(os.path.join(""exported_onnx_models/"", model_checkpoint))
    model.save_pretrained(onnx_path)
    processor.save_pretrained(onnx_path)


export_vanilla_optimized_onnx('openai/whisper-tiny')
import whisper
import torch
import tensorflow as tf
import onnx
import numpy as np
import argparse
import os
import warnings
import tqdm
from onnx_tf.backend import prepare
from whisper.audio import load_audio, log_mel_spectrogram,pad_or_trim,N_FRAMES, SAMPLE_RATE
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

#load openai->whisper(pytorch)->tiny model
tiny_model = whisper.load_model(""tiny"")

#Export to onnx format
torch.onnx.export(tiny_model.encoder, torch.randn(1,80,3000).to(device), ""./whisper-encoder.onnx"")
onnx_model_path = './whisper-encoder.onnx'
tf_model_path = 'model_tf-encoder'

onnx_model = onnx.load(onnx_model_path)
tf_rep = prepare(onnx_model)
tf_rep.export_graph(tf_model_path)
from datasets import load_dataset
ds = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")
from transformers import WhisperProcessor, TFWhisperForConditionalGeneration

processor = WhisperProcessor.from_pretrained(""openai/whisper-tiny"")
saved_model_dir = 'model_tf-encoder'
tflite_model_path = 'whisper-encoder-hybrid.tflite'

def representative_dataset_data():
    for x in range(5):
      inputs = processor(ds[x][""audio""][""array""], return_tensors=""tf"")
      input_features = inputs.input_features
      yield [input_features]

def representative_dataset_random():
    for _ in range(100):
      data = np.random.rand(1, 80, 3000)
      yield [data.astype(np.float32)]

def representative_dataset():
    for _ in range(1):#Change this to 100 and provide 100 different audio files from known dataset 
      mel_from_file = log_mel_spectrogram('/content/whisper/tests/jfk.flac')
      segment = pad_or_trim(mel_from_file, N_FRAMES)
      segment = tf.expand_dims(segment, 0)
      print(segment.shape)
      yield [segment]

# Convert to tflite(int8) model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.representative_dataset = representative_dataset_data
#converter.inference_input_type = tf.int8  # or tf.uint8
#converter.inference_output_type = tf.int8  # or tf.uint8
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()



# Save the model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

import tensorflow as tf
import numpy as np
tflite_model_path = '/content/whisper-encoder-hybrid.tflite'

# Load the TFLite model and allocate tensors
interpreter_enc = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter_enc.allocate_tensors()

print(""== Input details =="")
print(""name:"", interpreter_enc.get_input_details()[0]['name'])
print(""shape:"", interpreter_enc.get_input_details()[0]['shape'])
print(""type:"", interpreter_enc.get_input_details()[0]['dtype'])

print(""\nDUMP INPUT"")
print(interpreter_enc.get_input_details()[0])

print(""\n== Output details =="")
print(""name:"", interpreter_enc.get_output_details()[0]['name'])
print(""shape:"", interpreter_enc.get_output_details()[0]['shape'])
print(""type:"", interpreter_enc.get_output_details()[0]['dtype'])

print(""\nDUMP OUTPUT"")
print(interpreter_enc.get_output_details()[0])

# Get input and output tensors
input_details = interpreter_enc.get_input_details()
output_details = interpreter_enc.get_output_details()
output_tensor = interpreter_enc.get_output_details()[0]['index']

# Test the model with random data
input_shape = input_details[0]['shape']
mel_from_file = log_mel_spectrogram('/content/whisper/tests/jfk.flac')
input_tensor = pad_or_trim(mel_from_file, N_FRAMES)
input_tensor = tf.expand_dims(input_tensor, 0)

audio = whisper.load_audio('/content/whisper/tests/jfk.flac')
audio = whisper.pad_or_trim(audio)
mel = whisper.log_mel_spectrogram(audio)
mel = np.expand_dims(mel,0)
#input_tensor = np.array(input_tensor-128, dtype=np.int8)
interpreter_enc.set_tensor(input_details[0]['index'], mel)

interpreter_enc.invoke()
print(""Whisper Encoder Inference executed successfully\n"")
encoder_output_data = interpreter_enc.get_tensor(output_tensor)
print(encoder_output_data.shape)
print(encoder_output_data)
np.savetxt(""encoder_output.txt"", encoder_output_data.reshape((3,-1)), fmt=""%s"", header=str(encoder_output_data.shape))

```

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
Conversion is successful ,however while running the model getting below error
RuntimeError                              Traceback (most recent call last) <ipython-input-32-17a970b6c12f> in <module>       5 # Load the TFLite model and allocate tensors       6 interpreter_enc = tf.lite.Interpreter(model_path=tflite_model_path) ----> 7 interpreter_enc.allocate_tensors()       8        9 print(""== Input details =="")  /usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)     511   def allocate_tensors(self):     512     self._ensure_safe() --> 513     return self._interpreter.AllocateTensors()     514      515   def _safe_to_run(self):  RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 34 (RSQRT) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-02-01 01:25:56+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TFLiteConverter, TF 2.11"
TF dataset generator memory leak?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA RTX 2080ti (1 GPU used)

### Current Behaviour?

```shell
Hi everyone,

Hope you are doing well! I have an issue where potentially a memory leak occurs when I create a TensorFlow sampling dataset via tf.data.Dataset.from_generator. I am currently working on some model-based RL ideas, focusing on using exploration inside a Dreamer VAE model for best results in visual continuous control. The model is written in JAX/Haiku, and I decided to try to use a TensorFlow dataset pipeline to load in the data and use it to train the model.

The data is organized in trajectory format in a directory (400 trajectories, stored as .npz files, are in a directory for use). I've written code for the sampling procedure (see line 86 for the visual sequence buffer impl, line 349 for dataset/loader creation at https://github.com/dhruvsreenivas/byol-offline/blob/main/memory/replay_buffer.py for details), which I've done some preliminary tests on in line 170 of this file: https://github.com/dhruvsreenivas/byol-offline/blob/main/testing.py).

The behavior I'm seeing is that the model can effectively train for around 50 epochs with 50 GB of RAM requested, but jobs ultimately die due to memory leakage after that. I want to train the model for 1000 epochs, which makes it seem like an obscene amount of memory is required, which I don't have available to me.

I have a suspicion that a memory leak is occurring somewhere, which is odd given that I believe that memory should be dynamically allocated based off of the size of the batch requested (I am sampling a batch of 50 subtrajectories, each of length 10 for the initial experiments). I'm wondering how to fix this problem (the sampling is required, so the from_generator part should stay the same I think).
```


### Standalone code to reproduce the issue

```shell
The code to reproduce the experiments is here:
- Replay buffer code: https://github.com/dhruvsreenivas/byol-offline/blob/main/memory/replay_buffer.py -- line 86, 349
- Testing file: https://github.com/dhruvsreenivas/byol-offline/blob/main/testing.py -- line 170

For data I'm using to run the experiments, please see https://github.com/conglu1997/v-d4rl for how to collect and organize the data. I'm using cheetah_run 64x64px, medium expert data directory, seen here: https://drive.google.com/drive/folders/1zbAqR0gYBNG2W-F5_ItweCGg6_YHpZ7o.
```


### Relevant log output

```shell
I only really have the job's code, which basically says that the out-of-memory handler was used because the job ran out of memory. Used 2 CPUs and 1 GPU, with 50 GB RAM.
```
</details>",2023-02-06 23:17:50+00:00,"stat:awaiting response, comp:data, type:performance, TF 2.9"
Cannot import tensorflow after latest tensorboard release: TypeError: Descriptors cannot not be created directly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.3.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Since tensorboard-2.12.0 release, we cannot install and then import tensorflow-2.3.0.
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow==2.3.0
python -c ""import tensorflow""
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "".../venv-py38-tf/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 560, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
</details>",2023-02-13 10:27:09+00:00,"stat:awaiting response, type:bug, type:build/install, stale, TF 2.3"
Super Slow Performance (tf.function fails) of GradientTape with LSTM and Manual Training Loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

TF 2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0 (V12.0.140)

### GPU model and memory

2* Nvidia RTX A6000 48 GB

### Current Behaviour?

```shell
Hello all, 
For a specific problem, I need a manual training loop that includes a intermediate function. This function gets the model jacobian (output w.r.t to model's weights) along with other variables to generate the gradients and the loss function. I tried to calculate the model's Jacobian with GradientTape inside a function wrapped by tf.function. However, I get the error that tf.function fails. I saw similar issues dating back to 2020, but they were solved back then. I would be grateful to know your thoughts as the performance of the current code is super slow (~13 seconds for each step).
P.S. I cannot wrap the whole train_step function below with tf.function as the intermediate functions use Python dicts and Numpy arrays (the algorithm is a recursive algorithm if it matters).
Many thanks in advance!
```


### Standalone code to reproduce the issue

```shell
#fn to calculate the model jacobian
@tf.function
def jacobian_calc(model, inputs):
    with tf.GradientTape() as tape:
        tape.watch(model.trainable_weights)
        predictions = model(inputs, training=True)
    model_jacobian = tape.jacobian(predictions,model.trainable_weights)
    return model_jacobian, predictions

def train_step(model, inputs, targets, acc):
    st = time.time()
    model_jacobian, predictions = jacobian_calc(model,inputs)
    et = time.time()
    print(f""Elapsed time is {et-st}"")
    dLoss = []
    for jacobian in model_jacobian:
        p_loss, p_dLoss = loss_grads(targets, predictions, acc, jacobian)
        p_dLoss = tf.convert_to_tensor(p_dLoss, dtype=tf.float32)
        print(type(p_dLoss))
        print(type(model.trainable_weights[0]))
        dLoss.append(p_dLoss/K.eval(inputs).size)
    Loss = p_loss/K.eval(inputs).size
    opt.apply_gradients(zip(dLoss, model.trainable_weights))
    logs = {}
    print(f"" Loosssss is = {Loss}"")
    logs[""loss""] = Loss
    return logs

# load the samples
with open(seq_path+'/real_1000_300.pkl','rb') as dumper:
    samples_real = pickle.load(dumper)
with open(seq_path+'/model_1000_300.pkl','rb') as dumper:
    samples_model = pickle.load(dumper)
with open(seq_path+'/acc_1000_300.pkl','rb') as dumper:
    samples_acc = pickle.load(dumper)

# create training and testing sets
train_x = samples_model[:val_index_opt, :, 0]    #Umodel
train_y = samples_real[:val_index_opt, :, 0]     #Ureal
train_acc = samples_acc[:val_index_opt, :, 0]    #acc
val_x = samples_model[val_index_opt:, :, 0]     #Umodel
val_y = samples_real[val_index_opt:, :, 0]      #Ureal
val_acc = samples_acc[val_index_opt:, :, 0]     #acc

# no data scaling as I need to figure this out later. A dirty fix for now! Sorry:)
train_scaled_x = train_x.reshape(train_x.shape[0],train_x.shape[1],-1)
train_scaled_y = train_y.reshape(train_y.shape[0],train_y.shape[1],-1)
train_scaled_acc = train_acc.reshape(train_acc.shape[0],train_acc.shape[1],-1)
val_scaled_x = val_x.reshape(val_x.shape[0],val_x.shape[1],-1)
val_scaled_y = val_y.reshape(val_y.shape[0],val_y.shape[1],-1)
val_scaled_acc = val_acc.reshape(val_acc.shape[0],val_acc.shape[1],-1)

## Creating the RNN
model = Sequential()
                        
model.add(LSTM(32, input_shape=(None, train_scaled_x.shape[2]), mreturn_sequences=True))
                        model.add(Dense(32, input_shape=(None, train_scaled_x.shape[2])))
model.add(Dense(dense_1, activation='relu'))
model.add(Dense(1, activation='linear'))

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=0.001)

#Train model (only presented the training and not the validation here
training_set = tf.data.Dataset.from_tensor_slices((train_scaled_x,train_scaled_acc,train_scaled_y))
training_set = training_set.batch(batch)
history = {""loss"": []}
for ee in range(1,epoch+1):
    for inputs_batch, acc_batch, targets_batch in training_set:
        logs = train_step(model, inputs_batch,targets_batch,acc_batch)
    print(f""Results at the end of epoch {epoch}"")
    for key, value in logs.items(): #have some other metrics that I omitted here
        print(f""...{key}: {value:.4f}"")
    history[""loss""].append(logs[""loss""])
```


### Relevant log output

```shell
2023-02-13 21:51:20.518488: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-13 21:51:21.105332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/spack/v0.17.0/opt/spack/linux-ubuntu20.04-zen2/gcc-9.3.0/slurm-21-08-1-1-ctwolps4xy7mz7h3hooji4t72xs5vsyz/lib:/home/kshamsaei/miniconda3/envs/tf/lib/
2023-02-13 21:51:21.105406: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/spack/v0.17.0/opt/spack/linux-ubuntu20.04-zen2/gcc-9.3.0/slurm-21-08-1-1-ctwolps4xy7mz7h3hooji4t72xs5vsyz/lib:/home/kshamsaei/miniconda3/envs/tf/lib/
2023-02-13 21:51:21.105417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
HELLO
/home/kshamsaei/sdof/sdof-300-1000
2023-02-13 21:51:21.662319: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-13 21:51:22.347877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46671 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:21:00.0, compute capability: 8.6
2023-02-13 21:51:22.348397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46685 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6
WARNING:tensorflow:From /home/kshamsaei/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
2023-02-13 21:51:25.417706: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/while_grad/while_grad/pfor/PartitionedCall/while/enter/_746 was passed int32 from sequential/lstm/PartitionedCall:20 incompatible with expected variant.
2023-02-13 21:51:25.926875: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/while_grad/while_grad/pfor/PartitionedCall/while/enter/_746 was passed int32 from sequential/lstm/PartitionedCall:20 incompatible with expected variant.
2023-02-13 21:51:26.001961: W tensorflow/core/common_runtime/process_function_library_runtime.cc:915] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/while_grad/while_grad/pfor/PartitionedCall/while/enter/_728 was passed float from sequential/lstm/PartitionedCall:11 incompatible with expected int32.
2023-02-13 21:51:31.457799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
```
</details>",2023-02-14 05:53:38+00:00,"stat:awaiting response, type:bug, stale, comp:tf.function, TF 2.11"
LSTM prediction() - too slow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### Specs
Memory: 32GB
Ubuntu 22.04 LTS
Processor: 12th Gen Intel-Core i9-12900KSx24
OS-type; 64-bit



### Current Behaviour?

```
I have trained an LSTM Autoencoder (see code for the structure). 
With the trained model I attempt to make predictions. For that, I have re-formated the input data to fit the input shape needed for LSTM. 
The issue is, that the speed of prediction is too slow. I am using the model to predict anomalies in sensor data in real time. My system runs in 125HZ -> 0.008 sec, so predictions of 0.03 slow down my system, which results in inability to execute the tasks properly.

I have previously seen a similar issue discussed: https://github.com/tensorflow/tensorflow/issues/40261, I was unable to draw conclusions that can help in my case.

Does anyone have any clue how can I speed up the prediction time? - any help would be much appreciated

Using `model.predict(test_seq)`: avg: 0.03 sec
Using `model(test_seq)`: abg: 0.03 sec
```


### Standalone code to reproduce the issue

```shell
def to_sequence(data, timesteps=1):
    n_features=data.shape[2]
    seq = []
    for i in range(len(data)-timesteps):
        # takes a window of data of specified timesteps
        temp = data[i:(i+timesteps)]
        temp = temp.reshape(timesteps, n_features)
        seq.append(temp)
        
    return np.array(seq)


def LSTM_autoencoder(data):
    
    
    n_timesteps = data.shape[1]
    n_features = data.shape[2]
    
    keras.backend.clear_session()
    
    
    model = keras.Sequential()
    model.add(keras.layers.Input(shape=(n_timesteps, n_features)))
    model.add(keras.layers.Conv1D(filters=32, kernel_size=15, padding='same', 
                            data_format='channels_last',dilation_rate=1, activation=""linear""))
    model.add(keras.layers.LSTM(units=50, activation='relu', name='LSTM_1', return_sequences=False))
    model.add(keras.layers.Dropout(0.2))
    # to connect encoder with decoder RepeatVector repeats the provided 2D input multiple times to create 3D output
    model.add(keras.layers.RepeatVector(n=n_timesteps))
    # decoder expects the 3D input
    model.add(keras.layers.LSTM(units=50, activation='relu', name='LSTM_2', return_sequences=True))
    model.add(keras.layers.Conv1D(filters=32, kernel_size=15, padding='same', 
                            data_format='channels_last',dilation_rate=1, activation=""linear""))
    model.add(keras.layers.Dropout(0.2))
    # allows the same output layer to be reused for each element in sequence
    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=n_features)))

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=""mse"")
    
    return model

model = keras.models.load_model('Lstm3')

test=np.ones(255).reshape(51,5) # (51,5)
test_expanded = np.expand_dims(test,axis=1) # (51,1,5)
test_seq = to_sequence(test_expanded , 50) # (1,50,5)

t=0
while t<100:
  t_start = time.time()
  model.predict(test_seq)
  print(time.time()-t_start)
  t+=1
```


### Relevant log output

_No response_</details>",2023-02-15 17:03:40+00:00,"stat:awaiting response, stale, subtype: ubuntu/linux, type:performance, TF 2.11"
XLA compilation slow on new calls although no recompiling is performed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.7.0-rc1-69-gc256c071bb2 2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
(ON CPU) After compilation I expect speed up in speed using @tf.function(jit_compile=True). This is the case, however when used in a loop some of the iterations happen almost instantly whilst others take quite a long time. I feels like the code is recompiling, however using print statements I see that this is not the case. At first I thought there could be easy and difficult input samples to the function, however for further debugging I added an additional loop that just repeats every function evaluation multiple times. Here I saw that it is just one time a very slow one (feels like recompiling) and afterwards on the same input and exact same function it's really quick. No additional print statement is executed, hinting to the fact that the function is not compiled again. 
I was wondering if this is a known issue and if so how to fix it.

The last function below is called in a main script and makes use of a tf.keras.layers.Layer object and its call function. The call function is also given below and makes use of the jit_compiled function RK_multistep.
```


### Standalone code to reproduce the issue

```shell
@tf.function(jit_compile=True)
def RK_multistep(hybMod, x_k, u_k, dt):
    print(""compiling RK"")
    M = 5
    DT = dt/M
    ...

def call(self, model, x_k, u_k):
        print(""----- START RK repeats -----"")
        t1 = time.time()
        for k in range(3):
            t3 = time.time()
            x_kk = self.method(model, x_k, u_k, self.dt)
            t4 = time.time()
            print(f""RK iteration {k} took {t4 - t3}"")
        t2 = time.time()
        print(f""All RK iterations Took {t2 - t1}"")
        print(""----- END RK repeats -----"")
        x_kk = self.RK_multistep(model, x_k, u_k, self.dt)
    return x_kk

print(""----- START model in batches -----"")
t1 = time.time()
for k in range(20):
    t3 = time.time()
    Xbatch = np.expand_dims(X_train[k, :], 0)       # add batch dimension: shape (1, state_dims)
    Ubatch = np.expand_dims(Utrain[1][k, :, :], 0)  # add batch dimension: shape (1, prediction_length, control_dims)
    model_input = [Xbatch, Ubatch]
    Modelout = model(model_input)                   # here the model is called that used the call as defined above
    t4 = time.time()
    print(f""Outer layer iteration {k} took {t4 - t3}"")
t2 = time.time()
print(f""Took {t2-t1}"")
```


### Relevant log output

```shell
----- START model in batches -----
compiling g
compiling g_inner
compiling PI_big
compiling flow
compiling flow
----- START RK repeats -----
compiling RK
compiling f
compiling forcebalance
compiling pressure_change
RK iteration 0 took 16.773661613464355
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 16.773661613464355
----- END RK repeats -----
Outer layer iteration 0 took 17.035797357559204
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 1 took 0.015622138977050781
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 2 took 0.0
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 3 took 0.0
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 4 took 0.015624761581420898
----- START RK repeats -----
RK iteration 0 took 15.472918510437012
RK iteration 1 took 0.01561427116394043
RK iteration 2 took 0.0
All RK iterations Took 15.488532781600952
----- END RK repeats -----
Outer layer iteration 5 took 15.488532781600952
----- START RK repeats -----
RK iteration 0 took 14.715417385101318
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 14.715417385101318
----- END RK repeats -----
Outer layer iteration 6 took 14.715417385101318
----- START RK repeats -----
RK iteration 0 took 13.904020547866821
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 13.904020547866821
----- END RK repeats -----
Outer layer iteration 7 took 13.921808958053589
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 8 took 0.0
...
```
</details>",2023-02-16 18:00:25+00:00,"comp:xla, type:performance, TF 2.7"
What is the final training result of asynchronous synchronous parallel distributed training？,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When distributed training adopts asynchronous synchronous parallel, the parameters updated by each worker are inconsistent. Is the final result of distributed training the slowest parameter updated by workers?
```


### Standalone code to reproduce the issue

```shell
When distributed training adopts asynchronous synchronous parallel, the parameters updated by each worker are inconsistent. Is the final result of distributed training the slowest parameter updated by workers?
```


### Relevant log output

_No response_</details>",2023-02-20 14:05:03+00:00,"stat:awaiting response, type:support, comp:dist-strat, TF 2.8"
tf.data.Dataset is much slower than Python generator producing the same data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.6.3

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Iterating tf.data.Dataset with 10k records (consisting of 10 short strings each) is 78 times slower than iterating a Python generator producing the same data
```


### Standalone code to reproduce the issue

```shell
from time import perf_counter
import tensorflow as tf

def gen():
    for _ in range(10000):
        yield {str(k): 'string value' for k in range(10)}

start = perf_counter()
for _ in gen():
    pass
print('Python generator', perf_counter() - start)

ds = tf.data.Dataset.from_generator(
    gen,
    output_signature={str(k): tf.TensorSpec(shape=(), dtype=tf.string) for k in range(10)}
)

ds.save('/tmp/ds')
ds = tf.data.Dataset.load('/tmp/ds')

start = perf_counter()
for _ in ds:
    pass
print('tf.data.Dataset', perf_counter() - start)
```


### Relevant log output

```shell
Python generator 0.018086554016917944
tf.data.Dataset 1.4203055879333988
```
</details>",2023-02-20 16:58:15+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.11"
how to convert tensorflow lite using yolov6 .pt file ?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-03-05 19:37:38+00:00,"comp:lite, TFLiteConverter, comp:lite-support"
Tensorflow optimizer.apply_gradients is very slow.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm using a custom training loop for training a model made using EfficientNetV2 with biFPN. I found that `optimizer.apply_gradients` was running very slow. It took around 1.7 seconds to `apply_gradients` while model was taking only 0.1 seconds. I tried to replicate this using smaller example, so followed tutorial [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch). I found a similar issue here also.  `apply_gradients` here also is taking twice the time compared to model+loss. Is it fine to have such behavior? I tried a different version of tensorflow, but found the same behavior.
```


### Standalone code to reproduce the issue

```shell
The code used can be found here:
https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/writing_a_training_loop_from_scratch.ipynb
```


### Relevant log output

```shell
Model+Loss  0.027350187301635742
Grad  0.027795791625976562
Apply Grad  0.056526899337768555
```
</details>",2023-03-12 05:18:17+00:00,"stat:awaiting response, comp:keras, type:performance, TF 2.12"
Can't convert openimages_v4/ssd/mobilenet_v2,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

I follow the code to convert a saved model to tflite from tensorflow website here : https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_

The model I want to convert is this one : 
https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1

I would like to use it in the android example of tensorflow lite: 

https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

The conversion is unsuccessful.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```

2023-03-12 19:16:36.621905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-12 19:16:36.719912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-03-12 19:16:36.719933: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-03-12 19:16:37.381249: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-03-12 19:16:37.381323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-03-12 19:16:37.381337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
File ""/home/chauxb/Downloads/tensorflow/convert.py"", line 4, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(""/home/chauxb/Downloads/tensorflow/openimages/"") # path to the SavedModel directory
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"", line 1792, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py"", line 828, in load
    result = load_partial(export_dir, None, tags, options)[""root""]
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py"", line 977, in load_partial
    root = load_v1_in_v2.load(export_dir, tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py"", line 284, in load
    result = loader.load(tags=tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py"", line 209, in load
    meta_graph_def = self.get_meta_graph_def_from_tags(tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py"", line 88, in get_meta_graph_def_from_tags
    return super(_EagerSavedModelLoader, self).get_meta_graph_def_from_tags(
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 391, in get_meta_graph_def_from_tags
    raise RuntimeError(
RuntimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel, with available tags '[set()]'. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`.

```

Am I missing something in my conversion?
I am new to this so maybe I am doing something wrong (first time doing a model conversion)?

Thank you for your help in advance ",2023-03-12 18:27:47+00:00,"stat:awaiting response, type:bug, stale, TFLiteConverter, TF 2.11"
TFlite model has bad performance,"Hi, I reduced .h5 model size using TFLiteConverter, but the tflite model has a bad accuracy and is not usable. [Here](https://colab.research.google.com/drive/14zMCm0XaszVrUoWd9bMvBjSXSdPvSWuw?userstoinvite=tirumaleshk%40google.com&actionButton=1#scrollTo=JWvOm1R5WyPs) is the code I used to reduce the model size. I wanted to know - is there something wrong with reducing the model size code or with the model architecture itself - that it is not possible to reduce the size for this specific model?
I also tested both - .h5 model and .tflite model on 400 images (good and bad quality), but the .tflite model evaluates good quality images the same way as bad quality images.
Here are MAD and RMSE scores:
.tflite vs .h5 (images that are in a good quality)
MAD: 1.8
RMSE: 1.9
tflite vs .h5 (same images but in a bad quality)
MAD: 0.3
RMSE: 0.4
[Here ](https://drive.google.com/drive/folders/1YcCSJgG6R4wRJ5qeU3gUoUd2296GghoL)are some examples. First MOS score in image name is .h5 result and second one is .tflite result.
Thank you in advance!",2023-03-14 10:27:24+00:00,"stat:awaiting response, stale, type:performance, TFLiteConverter, TF 2.11"
Tensoflow Installation Problem,"I'm using python ```version 3.7.8```
while installing specific version of tensorflow i.e, ```1.14.0``` using pip command ```pip install tensorflow==1.14.0```
it is showing 
```
using cached tensorflow-1.14.0-cp37-cp37m-win_amd64.whl
....
```
and unable to ```import tensorflow``` properly.

It shows error: 
```
File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\node_def_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 42, in <module>
    serialized_options=None, file=DESCRIPTOR),
  File ""C:\Python37\lib\site-packages\google\protobuf\descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```",2023-03-18 12:44:40+00:00,"type:build/install, TF 1.14"
TFLITE mobilenetv2 conversion from tf leads to poor accuracy on imagenet evaluation tool,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.11
-   **Python version**: 3.9

### Describe the problem
Mobilenetv2 converted from TF to TFLITE using default MLIR converter leads to poor accuracy on tflite imagenet evaluation tool. On imagenet evaluation tool I get the below output:

> INFO: Num evaluation runs: 50000
> INFO: Preprocessing latency: avg=3837.7(us), std_dev=0(us)
> INFO: Inference latency: avg=16046.3(us), std_dev=486(us)
> INFO: Top-1 Accuracy: 0.43192
> INFO: Top-2 Accuracy: 0.5436
> INFO: Top-3 Accuracy: 0.60002
> INFO: Top-4 Accuracy: 0.6364
> INFO: Top-5 Accuracy: 0.66124
> INFO: Top-6 Accuracy: 0.68194
> INFO: Top-7 Accuracy: 0.69918
> INFO: Top-8 Accuracy: 0.71268
> INFO: Top-9 Accuracy: 0.72398
> INFO: Top-10 Accuracy: 0.73382

I used imagenet ILSVRC2012_img_val dataset with mobilenet labels downloaded from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt. I ran the evaluation on all 50000 images. Top1 accuracy came to 43% whereas official mobilenetv2 top1 accuracy is ~72%. I did not quantize the model and used the default float32. 
what can be the cause of this delta in accuracy? If i remove `converter.optimizations = [tf.lite.Optimize.DEFAULT]` from script, 
 i still get 43% Top1 accuracy on imagenet. Shouldn't tflite models have similar accuracy to tf models?

When i quantize the model to use uint8/int8, i get even worse accuracy to the tune of 0.4%.

below is the output log for a uint8 model converted from mobilenetv2:

> INFO: Num evaluation runs: 50000
> INFO: Preprocessing latency: avg=3801.64(us), std_dev=0(us)
> INFO: Inference latency: avg=5958.53(us), std_dev=166(us)
> INFO: Top-1 Accuracy: 0.0044
> INFO: Top-2 Accuracy: 0.00758
> INFO: Top-3 Accuracy: 0.01062
> INFO: Top-4 Accuracy: 0.01338
> INFO: Top-5 Accuracy: 0.01634
> INFO: Top-6 Accuracy: 0.0191
> INFO: Top-7 Accuracy: 0.02216
> INFO: Top-8 Accuracy: 0.02482
> INFO: Top-9 Accuracy: 0.02758
> INFO: Top-10 Accuracy: 0.03016


### Source code / logs
Below code is used for conversion:
1. fp32: 

> import tensorflow as tf
> import tensorflow_hub as hub
> 
> m = tf.keras.Sequential([hub.KerasLayer(""https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5"")])
> m.build([None, 224, 224, 3])# Batch input shape.
> 
> m.save('model')
> 
> converter = tf.lite.TFLiteConverter.from_saved_model(""/content/model"")
> converter.target_spec.supported_ops = [
>     tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
>     tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.
> ]
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> 
> tflite_file = ""mbnv2_model_test1.tflite""
> with open(tflite_file, 'wb') as f:
>   f.write(converter.convert())

generated model is attached in zip file. 
[mbnv2_model_test1.zip](https://github.com/tensorflow/tensorflow/files/11044725/mbnv2_model_test1.zip)


2. uint8:

> import tensorflow as tf
> import tensorflow_hub as hub
> 
> m = tf.keras.Sequential([hub.KerasLayer(""https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5"")])
> m.build([None, 224, 224, 3])# Batch input shape.
> 
> m.save('model')
> 
> import numpy as np
> def representative_dataset():
>     for _ in range(100):
>         data = tf.random.normal([1,224,224,3])
>         yield [(tf.cast(data, tf.float32) / 127.5) - 127.5]
> 
> converter = tf.lite.TFLiteConverter.from_saved_model(""/content/model"")
> converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # enable TensorFlow int8ops.
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> converter.representative_dataset = representative_dataset
> converter.inference_input_type = tf.uint8  # or tf.int8
> converter.inference_output_type = tf.uint8  # or tf.int8
> 
> tflite_file = ""mbnv2_model_uint8_test1.tflite""
> with open(tflite_file, 'wb') as f:
>   f.write(converter.convert())

generated model is attached [mbnv2_model_uint8_test1.zip](https://github.com/tensorflow/tensorflow/files/11044895/mbnv2_model_uint8_test1.zip)",2023-03-22 21:04:00+00:00,"stat:awaiting response, stale, comp:lite, type:performance, TF 2.11"
"Memory leak in forward pass (e.g., of ResNet50 model) with TensorFlow 2.12.0 and Python 3.11","The following minimal example reproduces the memory leak I ran into. (No GPU, just CPU.)

`memleak.py`:

```python3
import numpy as np
import psutil
import tensorflow as tf

model = tf.keras.applications.ResNet50()  # VGG19 seems to not leak.

# tf.config.threading.set_inter_op_parallelism_threads(0) and tf.config.threading.set_intra_op_parallelism_threads(0) do not help.

inp = (np.random.rand(1, 224, 224, 3) * 255).astype('uint8')

for run in range(1, 9999999):
    model(inp)
    memory_usage_in_MiB = psutil.Process().memory_info().rss / (1024 * 1024)
    print(f'Memory usage after {run} run(s) (in MiB): {memory_usage_in_MiB:.3f}', flush=True)
```

`Dockerfile`:
```Dockerfile
FROM python:3.11.2

RUN pip install --no-cache-dir tensorflow==2.12.0 psutil==5.9.4

# Disable the Docker cache from this stage on, see https://stackoverflow.com/a/58801213/1866775
ADD ""https://www.random.org/cgi-bin/randbyte?nbytes=10&format=h"" skipcache

ADD ./memleak.py /
RUN python /memleak.py
```

[Output](https://gist.github.com/Dobiasd/ba800b40e117aa13d97deb44761888f6) (`docker build --rm .`):
```
Memory usage after 1 run(s) (in MiB): 604.324
Memory usage after 2 run(s) (in MiB): 606.906
Memory usage after 3 run(s) (in MiB): 606.906
Memory usage after 4 run(s) (in MiB): 606.906
Memory usage after 5 run(s) (in MiB): 606.906
Memory usage after 6 run(s) (in MiB): 607.164
Memory usage after 7 run(s) (in MiB): 607.164
Memory usage after 8 run(s) (in MiB): 607.164
Memory usage after 9 run(s) (in MiB): 607.164
Memory usage after 10 run(s) (in MiB): 607.164
Memory usage after 11 run(s) (in MiB): 607.422
Memory usage after 12 run(s) (in MiB): 607.422
[...]
Memory usage after 498 run(s) (in MiB): 626.242
Memory usage after 499 run(s) (in MiB): 626.242
Memory usage after 500 run(s) (in MiB): 626.242
Memory usage after 501 run(s) (in MiB): 626.500
Memory usage after 502 run(s) (in MiB): 626.500
[...]
[...]
Memory usage after 1996 run(s) (in MiB): 683.477
Memory usage after 1997 run(s) (in MiB): 683.734
Memory usage after 1998 run(s) (in MiB): 683.734
Memory usage after 1999 run(s) (in MiB): 683.734
Memory usage after 2000 run(s) (in MiB): 683.734
Memory usage after 2001 run(s) (in MiB): 683.734
[...]
Memory usage after 9996 run(s) (in MiB): 960.258
Memory usage after 9997 run(s) (in MiB): 960.508
Memory usage after 9998 run(s) (in MiB): 960.508
Memory usage after 9999 run(s) (in MiB): 960.508
Memory usage after 10000 run(s) (in MiB): 960.508
Memory usage after 10001 run(s) (in MiB): 960.508
[...]
Memory usage after 24997 run(s) (in MiB): 1547.840
Memory usage after 24998 run(s) (in MiB): 1547.840
Memory usage after 24999 run(s) (in MiB): 1534.230
Memory usage after 25000 run(s) (in MiB): 1532.348
Memory usage after 25001 run(s) (in MiB): 1533.441
Memory usage after 25002 run(s) (in MiB): 1544.711
[...]
```

When using the same TensorFlow version (`2.12.0`) but with Python `3.10.10` (instead of `3.11.2`), the memory usage does not grow.",2023-03-27 15:36:35+00:00,"stat:awaiting response, type:bug, comp:keras, TF 2.12"
TensorFlow Lite Converter Issue,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-04-04 12:27:28+00:00,"stat:awaiting response, stale, comp:lite, TFLiteConverter"
TFLite NNAPI Delegate converts INT8 UnidirectionalSequenceLSTM to incorrect NN operation type,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

No

### OS Platform and Distribution

Android 12, aarch64

### Mobile device

Pixel 4 xl

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run a basic LSTM TFLite model with NNAPI delegate to explore the acceleration from Snapdragon 855's DSP Hexagon 690. I converted the simple LSTM model with full integer post-training quantization with intention to maximize hardware acceleration support, and ran this model on Pixel 4xl (snapdragon 855) with the latest pre-downloaded TFLite benchmark binary tool.

I am able to run other non-8bit model - env setup is correct. But 8bit model encountered error ` Unsupported input operand type for UNIDIRECTIONAL_SEQUENCE_LSTM op: TENSOR_QUANT8_ASYMM_SIGNED`.

I looked at NNAPI's operation support doc. It seems that operation `ANEURALNETWORKS_QUANTIZED_LSTM` is supported with the int8 inputs/outputs weights. But logcat suggests that TFLite NNAPI Model building process has converted the 8bit TFlite `UnidirectionalSequenceLSTM` to the non-quantized version `ANEURALNETWORKS_UNIDIRECTIONAL_SEQUENCE_LSTM`. Could this incorrect TFLite->NN operation conversion led to [this error](https://android.googlesource.com/platform/frameworks/ml/+/master/nn/common/operations/UnidirectionalSequenceLSTM.cpp#156)?
```


### Standalone code to reproduce the issue

```shell
### To make the dummy TFLite model

def representative_dataset():
""""""Just to make dummy input data for full integer quantization""""""
    for _ in range(100):
        data = np.random.rand(8, 16)
        yield [data.astype(np.float32)]

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense

# make keras model
units = 512
batch_size = 8

model_in = Input(shape=(16,), batch_size=batch_size)
model = Model(model_in, Dense(units, activation=""relu"")(LSTM(736)(Embedding(4001, units,)(model_in))))

# convert to TFLite format
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter._experimental_default_to_single_batch_in_tensor_list_ops = True
tflite_model = converter.convert()
``` 

### To run on device with TFLite benchmark tool
Whether to turn `disable_nnapi_cpu`  to true or false makes no difference.
```
./android_aarch64_benchmark_model --graph=/data/local/model/lstm_w_emb_and_dense_3.8b.tflite --use_nnapi=true --verbose=true
```
```


### Relevant log output

```shell
--------- beginning of main
2023-04-04 17:24:18.427 19036-19036 tflite                  pid-19036                            I  STARTING!
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Log parameter values verbosely: [1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min num runs: [50]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min runs duration (seconds): [1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Max runs duration (seconds): [150]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Inter-run delay (seconds): [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Number of prorated runs per second: [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Num threads: [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Use caching: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Benchmark name: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Output prefix: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min warmup runs: [1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min warmup runs duration (seconds): [0.5]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Run w/o invoking kernels: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Report the peak memory footprint: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Memory footprint check interval (ms): [50]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Graph: [/data/local/model/lstm_w_emb_and_dense_3.8b.tflite]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input layers: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input shapes: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input value ranges: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input value files: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Allow fp16: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Require full delegation: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Enable op profiling: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Max initial profiling buffer entries: [1024]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Allow dynamic increase on profiling buffer entries: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  CSV File to export profiling data to: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Print pre-invoke interpreter state: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Print post-invoke interpreter state: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Release dynamic tensor memory: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Optimize memory usage for large tensors: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Disable delegate clustering: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  File path to export outputs layer to: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  print out all supported flags: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  #threads used for CPU inference: [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Max number of delegated partitions: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min nodes per partition: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Index of the first node that could be delegated: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Index of the first node that could be delegated: [2147483647]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Directory for delegate serialization: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Model-specific token/key for delegate serialization.: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  External delegate path: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  External delegate options: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Use gpu: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Allow lower precision in gpu: [1]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Enable running quant models in gpu: [1]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Prefer maximizing the throughput in gpu: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  GPU backend: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Use Hexagon: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Hexagon lib path: [/data/local/tmp]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Hexagon profiling: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Use NNAPI: [1]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  NNAPI execution preference: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Model execution priority in nnapi: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  NNAPI accelerator name: []
2023-04-04 17:24:18.475 19036-19036 Manager                 pid-19036                            I  DeviceManager::DeviceManager
2023-04-04 17:24:18.475 19036-19036 Manager                 pid-19036                            I  findAvailableDevices
2023-04-04 17:24:18.475 19036-19036 ProcessState            pid-19036                            D  Binder ioctl to enable oneway spam detection failed: Invalid argument
2023-04-04 17:24:17.843     0-0     <no-tag>                kernel                               I  c7  19036 binder: 19036:19036 ioctl 40046210 7ff47b1524 returned -22
2023-04-04 17:24:18.476 19036-19036 hw-ProcessState         pid-19036                            D  Binder ioctl to enable oneway spam detection failed: Invalid argument
2023-04-04 17:24:17.844     0-0     <no-tag>                kernel                               I  c6  19036 binder: 19036:19036 ioctl 40046210 7ff47b1484 returned -22
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface qti-default
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface qti-dsp
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface qti-gpu
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface google-edgetpu
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  NNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,google-edgetpu,nnapi-reference]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Disable NNAPI cpu: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Allow fp16 in NNAPI: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Allow dynamic dimensions in NNAPI: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Use burst mode in NNAPI: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Use xnnpack: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Loaded model /data/local/model/lstm_w_emb_and_dense_3.8b.tflite
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Initialized TensorFlow Lite runtime.
2023-04-04 17:24:18.481 19036-19036 tflite                  pid-19036                            I  Created TensorFlow Lite delegate for NNAPI.
2023-04-04 17:24:18.481 19036-19036 tflite                  pid-19036                            I  NNAPI delegate created.
2023-04-04 17:24:18.482 19036-19036 tflite                  pid-19036                            I  Replacing 6 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 1 partitions for the whole graph.
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 2 size 2048512
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  TypeManager::TypeManager
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  NNAPI Vendor extensions enabled: 1
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  Registered extension com.google.edgetpu_precompiled
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 3 size 4
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 0
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 5 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 6 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 7 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 8 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 9 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 10 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 11 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 12 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 13 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 14 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 15 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 16 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 17 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 18 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 19 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 20 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 21 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 24 size 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 25 size 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 8
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 26 size 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 12
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 27 size 1
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 16
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 28 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 29 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 30 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 31 size 0
2023-04-04 17:24:18.483 19036-19036 Operations              pid-19036                            E  NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/operations/UnidirectionalSequenceLSTM.cpp:160): Unsupported input operand type for UNIDIRECTIONAL_SEQUENCE_LSTM op: TENSOR_QUANT8_ASYMM_SIGNED
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            E  Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/operations/UnidirectionalSequenceLSTM.cpp:160): Unsupported input operand type for UNIDIRECTIONAL_SEQUENCE_LSTM op: TENSOR_QUANT8_ASYMM_SIGNED
2023-04-04 17:24:18.483 19036-19036 tflite                  pid-19036                            E  NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.
2023-04-04 17:24:18.483 19036-19036 tflite                  pid-19036                            E  Node number 6 (TfLiteNnapiDelegate) failed to prepare.
2023-04-04 17:24:18.483 19036-19036 tflite                  pid-19036                            E  Restored original execution plan after delegate application failure.
2023-04-04 17:24:18.484 19036-19036 tflite                  pid-19036                            E  Failed to apply NNAPI delegate.
2023-04-04 17:24:18.484 19036-19036 tflite                  pid-19036                            E  Benchmarking failed.
```
</details>",2023-04-05 00:46:36+00:00,"stat:awaiting response, stat:awaiting tensorflower, type:bug, stale, comp:lite, TFLiteConverter, ModelOptimizationToolkit, TFLiteNNAPIDelegate, TF 2.12"
M2 GPU utilization decays from 50% to 10% in non batched inference for huggingface distilbert-base-cased,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tensorflow-macos 2.9, tensorflow-metal 0.5.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 13.3

### Mobile device

_No response_

### Python version

Python 3.10.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

Apple M2 Max (unified memory)

### Current Behaviour?

```shell
MacBook Pro M2 Max 96gb 
macOS 13.3 
tensorflow-macos 2.9.0 
tensorflow-metal 0.5.0

GPU utilization should hold steady when running inference for HuggingFace TFDistilBertForSequenceClassification from pretrained 'distilbert-base-cased'.
Instead utilization dropped steadily from 50% to 10% (and sometimes, below 2%). 
It becomes excruciatingly slow.
```


### Standalone code to reproduce the issue

```shell
# if needed, from HuggingFace
!pip install transformers
!pip install datasets

from transformers import AutoTokenizer, TFDistilBertForSequenceClassification
from datasets import load_dataset
from tqdm import tqdm

imdb = load_dataset('imdb')
sentences = imdb['train']['text'][:500]

tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-cased"")
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased')

for i, sentence in tqdm(enumerate(sentences)):
  inputs = tokenizer(sentence, truncation=True, return_tensors='tf')
  output = model(inputs).logits
  pred = np.argmax(output.numpy(), axis=1)

  if i % 100 == 0:
    print(f""len(input_ids): {inputs['input_ids'].shape[-1]}"")
```


### Relevant log output

```shell
Output from print:

Metal device set to: Apple M2 Max

systemMemory: 96.00 GB
maxCacheSize: 36.00 GB

3it [00:00, 10.87it/s]
len(input_ids): 391
101it [00:13,  6.38it/s]
len(input_ids): 215
201it [00:34,  4.78it/s]
len(input_ids): 237
301it [00:55,  4.26it/s]
len(input_ids): 256
401it [01:54,  1.12it/s]
len(input_ids): 55
500it [03:40,  2.27it/s]
```
</details>",2023-04-08 22:13:45+00:00,"stat:awaiting response, stale, comp:apis, comp:gpu, subtype:macOS, type:performance, TF 2.9"
protobuf have a problem,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11.0

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

I don't know if there is a problem with my download or the version of tensorflow, that is, how to recompile protobuf. Can I download the tensorflow compressed package directly from GitHub and uninstall tensorflow-Intel without uninstalling the gpu version.

### Standalone code to reproduce the issue

```shell
PS E:\Googledownload\RectifiedFlow-main> & D:/conda/envs/reflow/python.exe e:/Googledownload/RectifiedFlow-main/ImageGeneration/1.py
Traceback (most recent call last):
  File ""e:\Googledownload\RectifiedFlow-main\ImageGeneration\1.py"", line 1, in <module>
    import tensorflow as tf
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\python\__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\python\eager\context.py"", line 28, in <module>
    from tensorflow.core.framework import function_pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\function_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\tensor_shape_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File ""D:\conda\envs\reflow\lib\site-packages\google\protobuf\descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```


### Relevant log output

_No response_</details>",2023-04-14 02:06:39+00:00,"stat:awaiting response, type:build/install, type:support, stale, TF 2.11"
"Initial training speed extreemly (~100x) slow for generated data, and become faster and faster as each epoch progress","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS

### Mobile device

_No response_

### Python version

3.9.1

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.1

### GPU model and memory

GeForce GTX TITAN X, 12G

### Current Behaviour?

My code is roughly like this:
```python
  for i in range(data_n):
    if not train_data[i].npz exists:
      data = generate train_data[i].npz and save to disk
    else:
      data = read train_data[i].npz from disk

    model.fit(data)
```

`data_n = 26`,  when `i` less than ~20, everything is normal.
as `i` reach about 20, something strange start to happen:

I found if the data.npz is generated, the initial training speed is *very* slow, e.g ~x100 slower than the normal speed (when directly read from .npz file), and as the training Epoch progress, it become faster and faster, until it eventually runs full power on GPU, e.g.
```
_________________________________________________________________                                                                                                                      
Epoch 1/1000                                                                                                                                                                           
732/732 [==============================] - 1524s 2s/step - loss: 3.0771 - accuracy: 0.2212 - val_loss: 3.0802 - val_accuracy: 0.2265                                                   
Epoch 2/1000                                                                                                                                                                           
732/732 [==============================] - 931s 1s/step - loss: 3.0538 - accuracy: 0.2246 - val_loss: 3.0623 - val_accuracy: 0.2303                                                    
Epoch 3/1000                                                                                                                                                                           
732/732 [==============================] - 833s 1s/step - loss: 3.0434 - accuracy: 0.2261 - val_loss: 3.0663 - val_accuracy: 0.2298                                                    
Epoch 4/1000                                                                                                                                                                           
732/732 [==============================] - 757s 1s/step - loss: 3.0339 - accuracy: 0.2275 - val_loss: 3.0618 - val_accuracy: 0.2309                                                    
Epoch 5/1000                                                                                                                                                                           
732/732 [==============================] - 663s 906ms/step - loss: 3.0316 - accuracy: 0.2279 - val_loss: 3.0692 - val_accuracy: 0.2297                                                 
Epoch 6/1000                                                                                                                                                                           
732/732 [==============================] - 572s 782ms/step - loss: 3.0254 - accuracy: 0.2288 - val_loss: 3.0683 - val_accuracy: 0.2298                                                 
Epoch 7/1000                                                                                                                                                                           
732/732 [==============================] - 406s 555ms/step - loss: 3.0201 - accuracy: 0.2291 - val_loss: 3.0638 - val_accuracy: 0.2305                                                 
Epoch 8/1000                                                                                                                                                                           
732/732 [==============================] - 367s 502ms/step - loss: 3.0148 - accuracy: 0.2301 - val_loss: 3.0595 - val_accuracy: 0.2289                                                 
Epoch 9/1000                                                                                                                                                                           
732/732 [==============================] - 321s 439ms/step - loss: 3.0098 - accuracy: 0.2311 - val_loss: 3.0599 - val_accuracy: 0.2299                                                 
Epoch 10/1000                                                                                                                                                                          
732/732 [==============================] - 304s 416ms/step - loss: 3.0058 - accuracy: 0.2318 - val_loss: 3.0608 - val_accuracy: 0.2301                                                 
Epoch 11/1000                                                                                                                                                                          
732/732 [==============================] - 286s 391ms/step - loss: 3.0053 - accuracy: 0.2317 - val_loss: 3.0562 - val_accuracy: 0.2320                                                 
Epoch 12/1000                                                                                                                                                                          
732/732 [==============================] - 268s 366ms/step - loss: 3.0010 - accuracy: 0.2315 - val_loss: 3.0563 - val_accuracy: 0.2323                                                 
Epoch 13/1000                                                                                                                                                                          
732/732 [==============================] - 252s 345ms/step - loss: 3.0003 - accuracy: 0.2320 - val_loss: 3.0518 - val_accuracy: 0.2332                                                 
Epoch 14/1000                                                                                                                                                                          
732/732 [==============================] - 233s 319ms/step - loss: 2.9963 - accuracy: 0.2321 - val_loss: 3.0488 - val_accuracy: 0.2315                                                 
Epoch 15/1000                                                                                                                                                                          
732/732 [==============================] - 217s 296ms/step - loss: 2.9926 - accuracy: 0.2326 - val_loss: 3.0515 - val_accuracy: 0.2329                                                 
Epoch 16/1000                                                                                                                                                                          
732/732 [==============================] - 198s 271ms/step - loss: 2.9881 - accuracy: 0.2341 - val_loss: 3.0514 - val_accuracy: 0.2327                                                 
Epoch 17/1000                                                                                                                                                                          
732/732 [==============================] - 181s 247ms/step - loss: 2.9883 - accuracy: 0.2326 - val_loss: 3.0527 - val_accuracy: 0.2333                                                 
Epoch 18/1000                                                                                                                                                                          
732/732 [==============================] - 165s 226ms/step - loss: 2.9868 - accuracy: 0.2338 - val_loss: 3.0496 - val_accuracy: 0.2307                                                 
Epoch 19/1000                                                                                                                                                                          
732/732 [==============================] - 153s 209ms/step - loss: 2.9831 - accuracy: 0.2333 - val_loss: 3.0438 - val_accuracy: 0.2336                                                 
Epoch 20/1000                                                                                                                                                                          
732/732 [==============================] - 137s 187ms/step - loss: 2.9817 - accuracy: 0.2338 - val_loss: 3.0442 - val_accuracy: 0.2330                    
Epoch 21/1000                                                                                                                                                                          
732/732 [==============================] - 123s 168ms/step - loss: 2.9779 - accuracy: 0.2340 - val_loss: 3.0434 - val_accuracy: 0.2344                                                 
Epoch 22/1000                                                                                                                                                                          
732/732 [==============================] - 113s 154ms/step - loss: 2.9789 - accuracy: 0.2344 - val_loss: 3.0370 - val_accuracy: 0.2341                                                 
Epoch 23/1000                                                                                                                                                                          
732/732 [==============================] - 102s 139ms/step - loss: 2.9743 - accuracy: 0.2351 - val_loss: 3.0392 - val_accuracy: 0.2356                                                 
Epoch 24/1000                                                                                                                                                                          
732/732 [==============================] - 93s 127ms/step - loss: 2.9777 - accuracy: 0.2349 - val_loss: 3.0517 - val_accuracy: 0.2336                                                  
Epoch 25/1000                                                                                                                                                                          
732/732 [==============================] - 82s 112ms/step - loss: 2.9716 - accuracy: 0.2352 - val_loss: 3.0279 - val_accuracy: 0.2369                                                  
Epoch 26/1000                                                                                                                                                                          
732/732 [==============================] - 79s 107ms/step - loss: 2.9709 - accuracy: 0.2351 - val_loss: 3.0363 - val_accuracy: 0.2348                                                  
Epoch 27/1000                                                                                                                                                                          
732/732 [==============================] - 67s 92ms/step - loss: 2.9672 - accuracy: 0.2354 - val_loss: 3.0344 - val_accuracy: 0.2343                                                   
Epoch 28/1000                                                                                                                                                                          
732/732 [==============================] - 55s 75ms/step - loss: 2.9664 - accuracy: 0.2350 - val_loss: 3.0418 - val_accuracy: 0.2337                                                   
Epoch 29/1000                                                                                                                                                                          
732/732 [==============================] - 45s 62ms/step - loss: 2.9690 - accuracy: 0.2356 - val_loss: 3.0439 - val_accuracy: 0.2340                                                   
Epoch 30/1000                                                                                                                                                                          
732/732 [==============================] - 32s 44ms/step - loss: 2.9639 - accuracy: 0.2364 - val_loss: 3.0333 - val_accuracy: 0.2350                                                   
Epoch 31/1000                                                                                                                                                                          
732/732 [==============================] - 28s 39ms/step - loss: 2.9631 - accuracy: 0.2362 - val_loss: 3.0373 - val_accuracy: 0.2353                                                   
Epoch 32/1000                                                                                                                                                                          
732/732 [==============================] - 28s 38ms/step - loss: 2.9626 - accuracy: 0.2368 - val_loss: 3.0391 - val_accuracy: 0.2343                                                   
Epoch 33/1000                                                                                                                                                                          
732/732 [==============================] - 26s 36ms/step - loss: 2.9598 - accuracy: 0.2355 - val_loss: 3.0339 - val_accuracy: 0.2356                                                   
Epoch 34/1000                                                                                                                                                                          
732/732 [==============================] - 26s 35ms/step - loss: 2.9581 - accuracy: 0.2368 - val_loss: 3.0337 - val_accuracy: 0.2349                                                   
Epoch 35/1000                                                                                                                                                                          
732/732 [==============================] - 25s 35ms/step - loss: 2.9601 - accuracy: 0.2366 - val_loss: 3.0306 - val_accuracy: 0.2363                            
```

As you can see, the 1st epoch took 1524s, while the 35th epoch took 25s; if the data is directly read from .npz an epoch took consistent ~18s.

Could this be a TF memory issue ? or it's a Python issue?

How and where I can debug such issue? or any suggestions I can try to improve the speed?

Thanks.

### Standalone code to reproduce the issue

```python
# My code is roughly like this:

  for i in range(data_n):
    if not train_data[i].npz exists:
      data = generate train_data[i].npz and save to disk
    else:
      data = read train_data[i].npz from disk

    model.fit(data)
```
```


### Relevant log output

_No response_</details>",2023-04-27 17:52:36+00:00,"stat:awaiting response, stale, subtype: ubuntu/linux, type:performance, TF 2.8"
Protocol Buffer error - critical I cannot import tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Bug occurred today when reloading a jupyter notebook.

I previously tried to install cuML
https://docs.rapids.ai/install#pip-install

but the trace error seems unrelated to it, but mention `Protocol Buffers`:



### Standalone code to reproduce the issue

```shell
The error occurs when : 
`import tensorflow`

and consequently any other import of libraries based on tf, such as:
`import umap`
will fail.

I cannot use TF anymore.
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[33], line 1
----> 1 import tensorflow as tf
      2 #import umap

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py:37
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.
     42 from tensorflow.python import data

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:28
     25 from absl import logging
     26 import numpy as np
---> 28 from tensorflow.core.framework import function_pb2
     29 from tensorflow.core.protobuf import config_pb2
     30 from tensorflow.core.protobuf import coordination_config_pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
     17 from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
     18 from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/attr_value_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     17 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2
     20 DESCRIPTOR = _descriptor.FileDescriptor(
     21   name='tensorflow/core/framework/resource_handle.proto',
     22   package='tensorflow',
   (...)
     26   ,
     27   dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36
     13 _sym_db = _symbol_database.Default()
     18 DESCRIPTOR = _descriptor.FileDescriptor(
     19   name='tensorflow/core/framework/tensor_shape.proto',
     20   package='tensorflow',
   (...)
     23   serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\""z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03 \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tB\x87\x01\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\xf8\x01\x01\x62\x06proto3')
     24 )
     29 _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(
     30   name='Dim',
     31   full_name='tensorflow.TensorShapeProto.Dim',
     32   filename=None,
     33   file=DESCRIPTOR,
     34   containing_type=None,
     35   fields=[
---> 36     _descriptor.FieldDescriptor(
     37       name='size', full_name='tensorflow.TensorShapeProto.Dim.size', index=0,
     38       number=1, type=3, cpp_type=2, label=1,
     39       has_default_value=False, default_value=0,
     40       message_type=None, enum_type=None, containing_type=None,
     41       is_extension=False, extension_scope=None,
     42       serialized_options=None, file=DESCRIPTOR),
     43     _descriptor.FieldDescriptor(
     44       name='name', full_name='tensorflow.TensorShapeProto.Dim.name', index=1,
     45       number=2, type=9, cpp_type=9, label=1,
     46       has_default_value=False, default_value=_b("""").decode('utf-8'),
     47       message_type=None, enum_type=None, containing_type=None,
     48       is_extension=False, extension_scope=None,
     49       serialized_options=None, file=DESCRIPTOR),
     50   ],
     51   extensions=[
     52   ],
     53   nested_types=[],
     54   enum_types=[
     55   ],
     56   serialized_options=None,
     57   is_extendable=False,
     58   syntax='proto3',
     59   extension_ranges=[],
     60   oneofs=[
     61   ],
     62   serialized_start=149,
     63   serialized_end=182,
     64 )
     66 _TENSORSHAPEPROTO = _descriptor.Descriptor(
     67   name='TensorShapeProto',
     68   full_name='tensorflow.TensorShapeProto',
   (...)
    100   serialized_end=182,
    101 )
    103 _TENSORSHAPEPROTO_DIM.containing_type = _TENSORSHAPEPROTO

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py:560, in FieldDescriptor.__new__(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)
    554 def __new__(cls, name, full_name, index, number, type, cpp_type, label,
    555             default_value, message_type, enum_type, containing_type,
    556             is_extension, extension_scope, options=None,
    557             serialized_options=None,
    558             has_default_value=True, containing_oneof=None, json_name=None,
    559             file=None, create_key=None):  # pylint: disable=redefined-builtin
--> 560   _message.Message._CheckCalledFromGeneratedFile()
    561   if is_extension:
    562     return _message.default_pool.FindExtensionByName(full_name)

TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
</details>",2023-05-01 20:55:39+00:00,"stat:awaiting response, type:bug, type:build/install, stale, TF 2.11"
Tensorflow 2.11 and 2.14 Memory Issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary  (Lambda Stack)

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Driver Version: 525.105.17   CUDA Version: 12.0   

### GPU model and memory

A6000    System 1TB RAM 2TB Swap

### Current Behaviour?

I have multiple models that have worked fine for months or years under version <= 2.10.

With 2.11, I get this (below).

I have (on another system) tested 2.10 vs 2.11 by switching Docker containers.  The exact same code, data, etc, works with 2.10 but fails with 2.11.


training_cycle =  0


on_train_begin lr =  1.5625000182595272e-09
Epoch 1/100

---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-282-89befc246e8e> in <module>
     13 
     14     if training_validation_split != None:
---> 15         history = midas_model.fit (training_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[tensorboard_cb, dhc_scheduler])
     16     else:
     17         history = midas_model.fit (training_dataset, epochs=num_epochs, callbacks=[tensorboard_cb, dhc_scheduler])

/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

ResourceExhaustedError: Graph execution error:

Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/usr/lib/python3/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/lib/python3/dist-packages/traitlets/config/application.py"", line 664, in launch_instance
      app.start()
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelapp.py"", line 583, in start
      self.io_loop.start()
    File ""/usr/lib/python3/dist-packages/tornado/platform/asyncio.py"", line 132, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
      self._run_once()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
      handle._run()
    File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/lib/python3/dist-packages/tornado/ioloop.py"", line 758, in _run_callback
      ret = callback()
    File ""/usr/lib/python3/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
      return fn(*args, **kwargs)
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1248, in inner
      self.run()
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1162, in run
      yielded = self.gen.send(value)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 381, in dispatch_queue
      yield self.process_one()
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 346, in wrapper
      runner = Runner(result, future, yielded)
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1095, in __init__
      self.run()
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1162, in run
      yielded = self.gen.send(value)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
      self.do_execute(
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/lib/python3/dist-packages/ipykernel/ipkernel.py"", line 300, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/lib/python3/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 2857, in run_cell
      result = self._run_cell(
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 2886, in _run_cell
      return runner(coro)
    File ""/usr/lib/python3/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3062, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3254, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-282-89befc246e8e>"", line 15, in <module>
      history = midas_model.fit (training_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[tensorboard_cb, dhc_scheduler])
    File ""/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1222, in run_step
      outputs = model.train_step(data)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 527, in minimize
      self.apply_gradients(grads_and_vars)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1140, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 634, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1166, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1216, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall'
Out of memory while trying to allocate 6558708800 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:   18.32GiB
              constant allocation:         0B
        maybe_live_out allocation:   12.22GiB
     preallocated temp allocation:         0B
                 total allocation:   18.32GiB
Peak buffers:
	Buffer 1:
		Size: 6.11GiB
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[443156,3700]
		==========================

	Buffer 2:
		Size: 6.11GiB
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[443156,3700]
		==========================

	Buffer 3:
		Size: 6.11GiB
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[443156,3700]
		==========================

	Buffer 4:
		Size: 16B
		Operator: op_type=""AssignAddVariableOp"" op_name=""AssignAddVariableOp"" source_file=""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/sgd.py"" source_line=182
		XLA Label: fusion
		Shape: (f32[443156,3700], f32[443156,3700])
		==========================

	Buffer 5:
		Size: 4B
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[]
		==========================


	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_train_function_30187]



### Standalone code to reproduce the issue

```shell
Here is the model.  I have tried reducing the 3,700 width down to 3,400 but that doesn't help.  This exact model (3,700) works fine with <= 2.10 and has for months.


Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 layer_0 (InputLayer)        [(None, 443156)]          0         
                                                                 
 layer_1 (Dense)             (None, 3700)              1639680900
                                                                 
 layer_2 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_3 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_4 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_5 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_6 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_7 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_8 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_9 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_10 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_11 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_12 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_13 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_14 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_15 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_16 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_17 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_18 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_19 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_20 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_21 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_22 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_23 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_24 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_25 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_26 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_27 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_28 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_29 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_30 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_31 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_32 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_33 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_34 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_35 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_36 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_37 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_38 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_39 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_40 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_41 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_42 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_43 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_44 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_45 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_46 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_47 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_48 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_49 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_50 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_51 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_52 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_53 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_54 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_55 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_56 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_57 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_58 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_59 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_60 (Dense)            (None, 3700)              13693700  

                                                                 
 layer_61 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_62 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_63 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_64 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_65 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_66 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_67 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_68 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_69 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_70 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_71 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_72 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_73 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_74 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_75 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_76 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_77 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_78 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_79 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_80 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_81 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_82 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_83 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_84 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_85 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_86 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_87 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_88 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_89 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_90 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_91 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_92 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_93 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_94 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_95 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_96 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_97 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_98 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_99 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_100 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_101 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_102 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_103 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_104 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_105 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_106 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_107 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_108 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_109 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_110 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_111 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_112 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_113 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_114 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_115 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_116 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_117 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_118 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_119 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_120 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_121 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_122 (Dense)           (None, 3700)              13693700  
                                                                 

 layer_123 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_124 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_125 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_126 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_127 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_128 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_129 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_130 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_131 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_132 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_133 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_134 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_135 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_136 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_137 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_138 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_139 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_140 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_141 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_142 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_143 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_144 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_145 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_146 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_147 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_148 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_149 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_150 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_151 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_152 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_153 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_154 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_155 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_156 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_157 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_158 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_159 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_160 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_161 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_162 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_163 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_164 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_165 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_166 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_167 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_168 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_169 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_170 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_171 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_172 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_173 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_174 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_175 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_176 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_177 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_178 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_179 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_180 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_181 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_182 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_183 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_184 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_185 (Dense)           (None, 3700)              13693700  

                                                                 
 layer_186 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_187 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_188 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_189 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_190 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_191 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_192 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_193 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_194 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_195 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_196 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_197 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_198 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_199 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_200 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_201 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_202 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_203 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_204 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_205 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_206 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_207 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_208 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_209 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_210 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_211 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_212 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_213 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_214 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_215 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_216 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_217 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_218 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_219 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_220 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_221 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_222 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_223 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_224 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_225 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_226 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_227 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_228 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_229 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_230 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_231 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_232 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_233 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_234 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_235 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_236 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_237 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_238 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_239 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_240 (Dense)           (None, 3700)              13693700  
                                                                 
 output_layer (Dense)        (None, 1)                 3701      
                                                                 
=================================================================
Total params: 4,912,478,901
Trainable params: 4,912,478,901
Non-trainable params: 0
_________________________________________________________________

​


midas_model.compile(loss=loss_list, optimizer = the_optimizer)


Here are some other potential items of interest.

the_optimizer = keras.optimizers.SGD (nesterov=True, momentum=0.9, learning_rate=initial_learning_rate)

Loss:  mae = tf.keras.losses.MeanAbsoluteError()
```


### Relevant log output

_No response_</details>",2023-05-02 14:23:56+00:00,"stat:awaiting response, stale, comp:apis, type:performance, TF 2.11"
Significantly higher RAM consumption for Keras Lenet-5 models (especially during inference) on Keras when compared to other frameworks,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Custom
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version**: 2.5
-   **Python version**: 3.7.5
-   **GPU model and memory**:  NVIDIA Quadro RTX 4000 GPU with 8GB of VRAM
-  **CPU model**: Intel(R) Core(TM) i9-9900K CPU operating at 3.60GHz 
- **cuDNN version**: 7.6.5


### Describe the problem
Hello everybody.

I’ve been experimenting with different models and different frameworks, and I’ve noticed that, when using CPU, RAM consumption when using Lenet5 models on the MNIST dataset is significantly higher on Keras compared to the PyTorch and TensorFlow v1.X implementations, with RAM consumption during inference standing out. Note that I artificially increased the size of the testing set of the MNIST dataset by duplicating the set by 100 times for a total of 1 000 000 samples. I've done so so any difference in inference time may be clearer.

This also might be related to the slowdown observed during inference on Keras (https://github.com/tensorflow/tensorflow/issues/60462)

Here are boxplots that showcase the RAM consumption of numerous Lenet5 models:

![Screenshot 2023-05-04 015324 - Copy](https://user-images.githubusercontent.com/132307143/236123835-7c2c46c6-f1f6-4fe7-9b12-bea3750cb3c5.png)


Any ideas on what may be causing this?
",2023-05-04 06:05:59+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.5"
Memory Leak in mkl_graph_util.h,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.1

### GCC/Compiler version

clang 12.0.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In [mkl_graph_util.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/mkl_graph_util.h#L199C53-L200), we create a thread_local hash set using `new`. But it doesn't look like we ever free this memory. AddressSanitizer reports a memory leak when running a program that uses this code. Maybe we can change this to `std::unique_ptr`?

### Standalone code to reproduce the issue

```shell
// compiling with ASAN should leak.

#include ""tensorflow/core/public/session.h""

using namespace ::tensorflow;

int main(int argc, char** argv) {
  GraphDef gdef;
  Scope scope;
  auto op = ops::Add(scope, ops::Const(1), ops::Const(2));
  scope.ToGraphDef(&gdef);
  std::unique_ptr<Session> session = NewSession(SessionOptions());

  std::vector<Tensor> outputs;
  session->Run({}, {}, {}, &outputs);
  return 0;
}
```


### Relevant log output

```shell
Direct leak of 56 byte(s) in 1 object(s) allocated from:
    #0 0x14ffadad in malloc (/home/axlui/.cache/bazel/_bazel_axlui/f4685961e4a033eb3c5c8f3ed28f41d5/execroot/__main__/bazel-out/k8-opt/bin/cc/main_asan+0x14ffadad)
    #1 0x7f23a56450b4 in operator new(unsigned long) (/lib/x86_64-linux-gnu/libstdc++.so.6+0xa60b4)
    #2 0x2a67d4c6 in tensorflow::mkl_op_registry::IsMklOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::DataType) /proc/self/cwd/external/org_tensorflow/tensorflow/core/graph/mkl_graph_util.h:268:10
    #3 0x2a691503 in tensorflow::MklLayoutRewritePass::CheckForNodeRewrite(tensorflow::Node const*) const /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:3832:8
    #4 0x2a694b15 in tensorflow::MklLayoutRewritePass::RunPass(std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:4147:15
    #5 0x2a695b79 in tensorflow::MklLayoutRewritePass::Run(tensorflow::GraphOptimizationPassOptions const&)::$_15::operator()(std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) const /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:4205:5
    #6 0x2a6958f2 in tensorflow::MklLayoutRewritePass::Run(tensorflow::GraphOptimizationPassOptions const&) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:4218:7
    #7 0x2a77721a in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/optimization_registry.cc:73:26
    #8 0x289796a8 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:1729:3
    #9 0x28976dfc in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:1331:3
    #10 0x289722e2 in tensorflow::DirectSession::GetOrCreateExecutors(absl::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:1573:3
    #11 0x2897014e in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tsl::thread::ThreadPoolOptions const&) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:873:3
    #12 0x2896fbd3 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:842:10
    #13 0x2896c1eb in tensorflow::DirectSession::Run(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:472:10
    #14 0x1504aef5 in nn::NNInterface::Infer() /proc/self/cwd/cc/nn/nn_interface.cc:279:3
    #15 0x15049658 in nn::NNInterface::InferLoop() /proc/self/cwd/cc/nn/nn_interface.cc:267:5
    #16 0x1505f32b in void std::__invoke_impl<void, void (nn::NNInterface::*)(), nn::NNInterface*>(std::__invoke_memfun_deref, void (nn::NNInterface::*&&)(), nn::NNInterface*&&) /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:73:14
    #17 0x1505f264 in std::__invoke_result<void (nn::NNInterface::*)(), nn::NNInterface*>::type std::__invoke<void (nn::NNInterface::*)(), nn::NNInterface*>(void (nn::NNInterface::*&&)(), nn::NNInterface*&&) /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:95:14
    #18 0x1505f234 in void std::thread::_Invoker<std::tuple<void (nn::NNInterface::*)(), nn::NNInterface*> >::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/thread:264:13
    #19 0x1505f1f8 in std::thread::_Invoker<std::tuple<void (nn::NNInterface::*)(), nn::NNInterface*> >::operator()() /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/thread:271:11
    #20 0x1505f09c in std::thread::_State_impl<std::thread::_Invoker<std::tuple<void (nn::NNInterface::*)(), nn::NNInterface*> > >::_M_run() /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/thread:215:13
    #21 0x7f23a566decf  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xceecf)
```
</details>",2023-05-05 02:07:29+00:00,"stat:awaiting tensorflower, type:bug, comp:runtime, TF 2.11"
TF-Lite is 4x slower than Tensorflow on MacOS (and 2x slower in Colab),"Problem - Running a model with TFLite is 4x slower than running with tensorflow on my M1 MacOS.  

### 1. System information

- OS Platform and Distribution: **MacOS Ventura 13.2, on Apple M1 Macbook Air**
- TensorFlow installation (pip package or built from source): **2.9.1, from pip**

### 2. Code

Running the code in [this Colab notebook](https://colab.research.google.com/drive/1KFYbdxZicG2sQlw2kD63TM63gGm5VQJC?usp=sharing ), on MacOS, gives:

```
Tensorflow Model execution stats:
  Fraction of time spent detecting: 81%
  Average time per frame (ms): 19
  Average FPS: 52.10
  Median time per frame (ms): 19
  Median FPS: 52.33
TFLite Model execution stats:
  Fraction of time spent detecting: 95%
  Average time per frame (ms): 86
  Average FPS: 11.69
  Median time per frame (ms): 85
  Median FPS: 11.74
  
Tensorflow is 4.46x times faster than TFLite
```
... Clearly TFLite is much slower.

If I run it it in Colab, notebook, TFLite is still about 1.8x slower.

What is going on here?  I would expect TFLite to be faster.  Can this be fixed by adjusting some flags somewhere?

This has been noticed before - [see Stackoverflow question](https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop?rq=2).

**If this slowdown is unavoidable - is there some other way to serialize tensorflow functions so that they can be loaded again without slowdown?**",2023-05-16 22:06:56+00:00,"stat:awaiting response, stale, comp:lite, type:performance, TFLiteConverter, TF 2.9, TF 2.13"
metal delegate memory leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!
I  calls this three function loop on ios,and find memory leak
 use TFLGpuDelegateCreate --》 TfLiteInterpreterModifyGraphWithDelegate --》  TFLGpuDelegateDelete
 
![191225054-cfc30bd2-cc8d-4b42-97c5-131ffdbde9d3](https://github.com/tensorflow/tensorflow/assets/87115287/908eb0f8-ac63-4b4d-8959-8e377f75b3b4)


### Standalone code to reproduce the issue

```shell
From my own investigation it seems that the leak is coming from ModifyGraphWithDelegate, instead of Invoke.Attached is a screenshot of instruments, and a 5MB growth per call。
```


### Relevant log output

_No response_</details>",2023-05-18 08:57:48+00:00,"stat:awaiting tensorflower, type:bug, comp:lite, TFLiteGpuDelegate, TF 2.11"
TFLite model maker object detection training is too slow in colab,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TFlite model maker 0.4.2

### Custom Code

No

### OS Platform and Distribution

colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In colab I have enabled 'fallback runtime version' since with Python 3.10 model maker was not installed successfully.
After enabling this option I am able to install it successfully.
Nut when I started a object detection training with 'efficientnet_lite4', it is showing `35/2122 [..............................] - ETA: 6:08:03 - loss: 0.7095 - accuracy: 0.5509` which is keep increasing.

Please help to solve this issue.


### Standalone code to reproduce the issue

```shell
My training command is as below-
`model = image_classifier.create(train_data, validation_data=validation_data, model_spec=model_spec.get('efficientnet_lite4'), batch_size=32, epochs=50, train_whole_model=True, use_augmentation=False, use_hub_library=False, model_dir='/content/drive/MyDrive/expi')`
```


### Relevant log output

_No response_</details>",2023-05-19 08:09:16+00:00,"stat:awaiting response, comp:lite, type:performance, TF 2.12"
SavedModel: enable dropout & disable batch normalization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am currently training a ResNet model with both batch normalization and dropout layers. My goal is to use monte carlo dropout for uncertainty estimation at evaluation time (i.e. with training=False in my model call).

I'm currently working with tf.functions and the SavedModel format (not eager execution). Eager execution is to slow for my application, and therefore is not an option for me. 

Thus, when I set training=False for my model calls, it disables both the batch normalization and dropout layers. However, when I set training=False I want to keep dropout enabled but disable batch normalization (for the purpose of uncertainty estimation).

How can I achieve this with tf.function and the SavedModel format?

I am using tf-nightly.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>",2023-05-19 19:10:56+00:00,"stat:awaiting response, type:support, stale, comp:keras, TF 2.13"
AdamW optimiser crashes on tf-macos v2.12.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.5.1 (running on M1 pro chip)

### Mobile device

_No response_

### Python version

3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

The problem occurs in tensorflow-macos v2.12.0 when attempting to call model.compile() with the AdamW optimiser. A warning is thrown, telling us that there is a known slowdown when using v2.11+ optimizers, and the backend attempts to fall back to a legacy version. However, AdamW does not exist in legacy versions, which eventually propagates through to an ""unknown optimizer"" error.

Expected behaviour: tf.keras.Model object is compiled using the AdamW optimiser, either using the ""tf v2.11+"" optimiser class with known slowdown, or falling back to an implementation compatible with legacy keras optimisers

Note: problem occurs in tf-macos regardless of whether we are using tf-metal to access the GPU.

### Standalone code to reproduce the issue

```shell
##
##  Imports
##

import sys

import tensorflow as tf

from tensorflow.keras.models     import Model
from tensorflow.keras.layers     import Input, Dense
from tensorflow.keras.optimizers import AdamW

##
##  Report versions
##

print(f""Python version is: {sys.version}"")
##  -->  Python version is: 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]

print(f""TF version is: {tf.__version__}"")
##  -->  TF version is: 2.12.0

print(f""Keras version is: {tf.keras.__version__}"")
##  -->  Keras version is: 2.12.0


##
##  Create a very simple model
##

x_in  = Input(1)
x     = Dense(10)(x_in)
model = Model(x_in, x)

##
##  Compile model with AdamW optimizer
##
model.compile(optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-2))
```


### Relevant log output

```shell
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdamW`.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[4], line 2
      1 ##  Compile model with AdamW optimizer
----> 2 model.compile(optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-2))

File ~/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/saving/legacy/serialization.py:368, in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    364 cls = object_registration.get_registered_object(
    365     class_name, custom_objects, module_objects
    366 )
    367 if cls is None:
--> 368     raise ValueError(
    369         f""Unknown {printable_module_name}: '{class_name}'. ""
    370         ""Please ensure you are using a `keras.utils.custom_object_scope` ""
    371         ""and that this object is included in the scope. See ""
    372         ""https://www.tensorflow.org/guide/keras/save_and_serialize""
    373         ""#registering_the_custom_object for details.""
    374     )
    376 cls_config = config[""config""]
    377 # Check if `cls_config` is a list. If it is a list, return the class and the
    378 # associated class configs for recursively deserialization. This case will
    379 # happen on the old version of sequential model (e.g. `keras_version` ==
    380 # ""2.0.6""), which is serialized in a different structure, for example
    381 # ""{'class_name': 'Sequential',
    382 #   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}"".

ValueError: Unknown optimizer: 'adamw'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.
```
</details>",2023-05-21 13:25:34+00:00,"stat:awaiting response, type:bug, comp:keras, TF 2.12"
Memory leak in model fit with dataset from generator,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10, 3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

When creating a dataset from the generator, using Tensorflow V2.12.0 with Python 3.11, some memory is not released after training on each batch, leading to a linear increase in memory usage during the model fit step.

### Standalone code to reproduce the issue

```shell
This is the code I used to generate random data on the fly to be used by the generator, and a simple model to fit this data on. 


batch_size =32
train_datapoints_count = 1000000
val_datapoints_count = 200000

def generate_random_dataset(input_size):
    for _ in range(input_size):
        x = np.random.rand(batch_size, 1, 10).astype(np.float32)
        y = np.random.rand(batch_size, 1, 1).astype(np.float32)
        yield x, y

dataset_train = tf.data.Dataset.from_generator(
    generator=generate_random_dataset,
    args=[train_datapoints_count],
    name=""random_ds_train"",
    output_signature=(
        tf.TensorSpec(shape=(None, 1, 10), dtype=np.float32),
        tf.TensorSpec(shape=(None, 1, 1), dtype=np.float32),
    ),
)

dataset_val = tf.data.Dataset.from_generator(
    generator=generate_random_dataset,
    args = [val_datapoints_count],
    name=""random_ds_val"",
    output_signature=(
        tf.TensorSpec(shape=(None, 1, 10), dtype=np.float32),
        tf.TensorSpec(shape=(None, 1, 1), dtype=np.float32),
    ),
)

model = tf.keras.Sequential(
    [
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units=batch_size, activation=""relu""),
        tf.keras.layers.Dense(units=batch_size, activation=""relu""),
        tf.keras.layers.Dense(units=1),
    ]
)

model.compile(
    loss=tf.losses.MeanSquaredError(),
    optimizer=tf.optimizers.Adam(),
    metrics=[tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError()],
)
model.fit(
    dataset_train,
    epochs=2,
    validation_data=dataset_val,
)
```

I then ran the code in a container and logged the memory usage.
```
FROM python:3.10 
#also 3.11

WORKDIR /app

RUN pip install tensorflow==2.12.0
COPY train.py .

ENTRYPOINT [""python"", ""train.py""]
```
```


### Relevant log output

```shell
With Python 3.11, I see an increment in memory usage over time, while it remains the same for Python 3.10. I am allocating the exact same resources to both of the containers.
The columns are respectively: Seconds into the fitting step, memory usage tf_2.12_python_3.10, memory usage tf_2.12_python_3.11. 

5	254.9	288.9
10	254.9	291
15	254.9	293.1
20	254.9	294.8
25	254.9	296.4
30	254.9	298.5
35	254.9	300.3
40	254.9	301.8
45	254.9	304
50	254.9	306.2
55	254.9	308.3
60	254.9	310.6
65	254.9	312.5
70	254.9	314.8
75	254.9	316.5
80	254.9	317.9
85	254.6	319.5
90	254.2	320.4
95	253.8	321.5
100	253	323.1
```
</details>",2023-05-22 16:10:35+00:00,"stat:awaiting response, stale, type:performance, TF 2.12"
AdamW optimizer crashes on Model.fit() for tensorflow-macos v2.14.0-dev20230518,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230518

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.5.1 running on ARM architecture [M1 Pro chip]

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When calling Model.compile() with the AdamW optimizer, a warning is thrown saying that v2.11+ optimizers have a known slowdown on M1/M2 devices, and so the backend attempts to fallback to a legacy version. However, no legacy version of the AdamW optimizer exists. In a previous tf-macos version 2.12, this lead to an error during Model.compile() [see issue https://github.com/tensorflow/tensorflow/issues/60652]. In the current nightly, this error is not thrown - however, after calling model.compile(), the attribute model.optimizer is set to string 'adamw' instead of an optimizer object.

Later, when we call model.fit(), this leads to an AttributeError, because model.optimizer.minimize() does not exist when model.optimizer is a string.

Expected behaviour: correctly compile the model with either a v2.11+ optimiser without slowdown, or a legacy-compatible implementation of the AdamW optimizer. I could attempt to contribute this - but there may be a steep learning curve! Then the model will train correctly with a valid AdamW optimizer when calling model.fit().

Note: a warning message suggests using the optimizer located at `tf.keras.optimizers.legacy.AdamW`, but this does not exist

### Standalone code to reproduce the issue

```shell
##===========##
##  Imports  ##
##===========##

import sys

import tensorflow as tf

import numpy as np

from tensorflow.keras.models     import Model
from tensorflow.keras.layers     import Input, Dense
from tensorflow.keras.optimizers import AdamW

##===================##
##  Report versions  ##
##===================##
#
# Expected outputs:
# Python version is: 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]
# TF version is: 2.14.0-dev20230518
# Numpy version is: 1.23.2
#

print(f""Python version is: {sys.version}"")
print(f""TF version is: {tf.__version__}"")
print(f""Numpy version is: {np.__version__}"")

##==============================##
##  Create a very simple model  ##
##==============================##
#
# Expected outputs:
# Model: ""model_1""
# _________________________________________________________________
#  Layer (type)                Output Shape              Param #   
# =================================================================
#  Layer_in (InputLayer)       [(None, 2)]               0         
#                                                                 
#  Layer_hidden (Dense)        (None, 10)                30        
#                                                                 
#  Layer_out (Dense)           (None, 2)                 22        
#                                                                 
# =================================================================
# Total params: 52 (208.00 Byte)
# Trainable params: 52 (208.00 Byte)
# Non-trainable params: 0 (0.00 Byte)
# _________________________________________________________________
#

x_in  = Input(2 , dtype=tf.float32, name=""Layer_in""    )
x     = x_in
x     = Dense(10, dtype=tf.float32, name=""Layer_hidden"", activation=""relu""  )(x)
x     = Dense(2 , dtype=tf.float32, name=""Layer_out""   , activation=""linear"")(x)
model = Model(x_in, x)
model.summary()

##===================================================##
##  Compile model with MSE loss and AdamW optimizer  ##
##===================================================##
#
# Expected outputs:
# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.
# WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdamW`.
#

model.compile(
    loss      = ""mse"", 
    optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-2)
)

##===========================##
##  Generate some fake data  ##
##===========================##
#
# Expected outputs:
# X shape is (100, 2), Y shape is (100, 2)
#

dataset_size = 100
X = np.random.normal(size=(dataset_size, 2))
X = tf.constant(X, dtype=tf.float32)
Y = np.random.normal(size=(dataset_size, 2))
Y = tf.constant(Y, dtype=tf.float32)

print(f""X shape is {X.shape}, Y shape is {Y.shape}"")

##===================================##
##  Fit model to data for one epoch  ##
##===================================##
#
# Expected outputs:
# ---------------------------------------------------------------------------
# AttributeError                            Traceback (most recent call last)
# Cell In[9], line 51
#       1 ##===================================##
#       2 ##  Fit model to data for one epoch  ##
#       3 ##===================================##
#    (...)
#      48 #       • mask=None
#      49 #
# ---> 51 model.fit(X, Y, epochs=1)

# File ~/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
#      67     filtered_tb = _process_traceback_frames(e.__traceback__)
#      68     # To get the full stack trace, call:
#      69     # `tf.debugging.disable_traceback_filtering()`
# ---> 70     raise e.with_traceback(filtered_tb) from None
#      71 finally:
#      72     del filtered_tb

# File /var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/__autograph_generated_filezzqv9k36.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
#      13 try:
#      14     do_return = True
# ---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
#      16 except:
#      17     do_return = False

# AttributeError: in user code:

#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
#         return step_function(self, iterator)
#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1322, in step_function  **
#         outputs = model.distribute_strategy.run(run_step, args=(data,))
#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1303, in run_step  **
#         outputs = model.train_step(data)
#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1084, in train_step
#         self.optimizer.minimize(loss, self.trainable_variables, tape=tape)

#     AttributeError: 'str' object has no attribute 'minimize'

model.fit(X, Y, epochs=1)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[9], line 51
      1 ##===================================##
      2 ##  Fit model to data for one epoch  ##
      3 ##===================================##
   (...)
     48 #       • mask=None
     49 #
---> 51 model.fit(X, Y, epochs=1)

File ~/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/__autograph_generated_filezzqv9k36.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

AttributeError: in user code:

    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1084, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)

    AttributeError: 'str' object has no attribute 'minimize'
```
</details>",2023-05-23 13:38:07+00:00,"stat:awaiting response, type:bug, stale, comp:keras"
Tensorflow Object Detection Project,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf1.x

### Custom Code

No

### OS Platform and Distribution

Macos Ventura

### Mobile device

Macbook air 2020 i3

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

So i am making an object detection projetc for school and i need help whenever i run this code this is the error that pops up. Please help it is due in a few days

I have installed all neccesary modules, i think

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python
# coding: utf-8
""""""
Detect Objects Using Your Webcam
================================
""""""

# %%
# This demo will take you through the steps of running an ""out-of-the-box"" detection model to
# detect objects in the video stream extracted from your camera.

# %%
# Create the data directory
# ~~~~~~~~~~~~~~~~~~~~~~~~~
# The snippet shown below will create the ``data`` directory where all our data will be stored. The
# code will create a directory structure as shown bellow:
#
# .. code-block:: bash
#
#     data
#     └── models
#
# where the ``models`` folder will will contain the downloaded models.
import os
#os.chdir( '/Users/akulthota/Desktop/Object Detection' )

DATA_DIR = os.path.join(os.getcwd(), 'data')
MODELS_DIR = os.path.join(DATA_DIR, 'models')
for dir in [DATA_DIR, MODELS_DIR]:
    if not os.path.exists(dir):
        os.mkdir(dir)

# %%
# Download the model
# ~~~~~~~~~~~~~~~~~~
# The code snippet shown below is used to download the object detection model checkpoint file,
# as well as the labels file (.pbtxt) which contains a list of strings used to add the correct
# label to each detection (e.g. person).
#
# The particular detection algorithm we will use is the `SSD ResNet101 V1 FPN 640x640`. More
# models can be found in the `TensorFlow 2 Detection Model Zoo <https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md>`_.
# To use a different model you will need the URL name of the specific model. This can be done as
# follows:
#
# 1. Right click on the `Model name` of the model you would like to use;
# 2. Click on `Copy link address` to copy the download link of the model;
# 3. Paste the link in a text editor of your choice. You should observe a link similar to ``download.tensorflow.org/models/object_detection/tf2/YYYYYYYY/XXXXXXXXX.tar.gz``;
# 4. Copy the ``XXXXXXXXX`` part of the link and use it to replace the value of the ``MODEL_NAME`` variable in the code shown below;
# 5. Copy the ``YYYYYYYY`` part of the link and use it to replace the value of the ``MODEL_DATE`` variable in the code shown below.
#
# For example, the download link for the model used below is: ``download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz``
import tarfile
import urllib.request

# Download and extract model
MODEL_DATE = '20200711'
MODEL_NAME = 'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8'
MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'
MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'
MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME
PATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)
PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))
PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))
if not os.path.exists(PATH_TO_CKPT):
    print('Downloading model. This may take a while... ', end='')
    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)
    tar_file = tarfile.open(PATH_TO_MODEL_TAR)
    tar_file.extractall(MODELS_DIR)
    tar_file.close()
    os.remove(PATH_TO_MODEL_TAR)
    print('Done')

# Download labels file
LABEL_FILENAME = 'mscoco_label_map.pbtxt'
LABELS_DOWNLOAD_BASE = \
    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'
PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))
if not os.path.exists(PATH_TO_LABELS):
    print('Downloading label file... ', end='')
    import ssl
    ssl._create_default_https_context = ssl._create_unverified_context
    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)
    print('Done')

# %%
# Load the model
# ~~~~~~~~~~~~~~
# Next we load the downloaded model

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging
import tensorflow as tf
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder

tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)

# Enable GPU dynamic memory allocation
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)

# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()

@tf.function
def detect_fn(image):
    """"""Detect objects in image.""""""

    image, shapes = detection_model.preprocess(image)
    prediction_dict = detection_model.predict(image, shapes)
    detections = detection_model.postprocess(prediction_dict, shapes)

    return detections, prediction_dict, tf.reshape(shapes, [-1])


# %%
# Load label map data (for plotting)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Label maps correspond index numbers to category names, so that when our convolution network
# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility
# functions, but anything that returns a dictionary mapping integers to appropriate string labels
# would be fine.
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,
                                                                    use_display_name=True)

# %%
# Define the video stream
# ~~~~~~~~~~~~~~~~~~~~~~~
# We will use `OpenCV <https://pypi.org/project/opencv-python/>`_ to capture the video stream
# generated by our webcam. For more information you can refer to the `OpenCV-Python Tutorials <https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html#capture-video-from-camera>`_
import cv2

cap = cv2.VideoCapture(0)

# %%
# Putting everything together
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The code shown below loads an image, runs it through the detection model and visualizes the
# detection results, including the keypoints.
#
# Note that this will take a long time (several minutes) the first time you run this code due to
# tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be
# faster.
#
# Here are some simple things to try out if you are curious:
#
# * Modify some of the input images and see if detection still works. Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).
# * Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).
# * Set ``min_score_thresh`` to other values (between 0 and 1) to allow more detections in or to filter out more detections.
import numpy as np

while True:
    # Read frame from camera
    ret, image_np = cap.read()

    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
    image_np_expanded = np.expand_dims(image_np, axis=0)

    # Things to try:
    # Flip horizontally
    # image_np = np.fliplr(image_np).copy()

    # Convert image to grayscale
    # image_np = np.tile(
    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)

    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
    detections, predictions_dict, shapes = detect_fn(input_tensor)

    label_id_offset = 1
    image_np_with_detections = image_np.copy()

    viz_utils.visualize_boxes_and_labels_on_image_array(
          image_np_with_detections,
          detections['detection_boxes'][0].numpy(),
          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),
          detections['detection_scores'][0].numpy(),
          category_index,
          use_normalized_coordinates=True,
          max_boxes_to_draw=200,
          min_score_thresh=.30,
          agnostic_mode=False)

    # Display output
    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))

    if cv2.waitKey(25) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/akulthota/Desktop/Object/object_detection_camera.py"", line 92, in <module>
    from object_detection.utils import label_map_util
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/object_detection/utils/label_map_util.py"", line 21, in <module>
    from object_detection.protos import string_int_label_map_pb2
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/object_detection/protos/string_int_label_map_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
</details>",2023-05-23 22:52:30+00:00,"stat:awaiting response, type:support, stale, comp:model, TF 1.5.0"
CUDNN failed to allocate the scratch space for the runner,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug


### Source

source

### Tensorflow Version

Tensorlfow 2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.2


### Python version

3.9


### CUDA/cuDNN version

Cuda 11.8,cuDNN  8.6.0

### GPU model and memory

GTX 4070, Vram 11178/12282Mib

### Current Behaviour?

In the middle of training I suddenly get
`Node: 'gradient_tape/model/conv3d_20/Conv3D/Conv3DBackpropFilterV2'
CUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.`

I am running 3d Unet segmentation, my dataset is custom generator going through Dataset.I am using multiprocessing. Exact same code and model ran without this error in windows. It was slow so I moved to linux. It is not easy replicating the issue as it sometimes happens so many epochs after.




",2023-06-05 17:33:00+00:00,"stat:awaiting response, type:bug, stale, comp:gpu, TF 2.12"
"Minimal TfLite program generates Valgrind ""still reachable"" messages","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Linux64 Ubuntu 22.10

### Mobile device

_No response_

### Python version

3.10.7 (irrelevant for the problem)

### Bazel version

_No response_

### GCC/Compiler version

gcc 12.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Minimal TfLite (2.10) program generates Valgrind ""still reachable"" message. See accompanied code and Valgrind log. The network cannot be shared (company intellectual property), but this should not matter to reproduce the problem.

You can get rid off the ""still reachable"" message from `BuildFromFile` by using:
`delete model->error_reporter();`
Still, shouldn't TfLite take care of this? The use of smart pointers suggests that the user does not need to manage TfLite's memory.

My question:
How to get rid off the ""still reachable"" messages from `AllocateTensors`?

What I tried (but which does _not_ get rid off the ""still reachable"" messages):
Manually delete the model and interpreter at the end:
`interpreter.reset(nullptr);`
`model.reset(nullptr);`
Clear two vectors of `BuiltinOpResolver`:
`op_resolver.GetDelegateCreators().clear();`
`op_resolver.GetOpaqueDelegateCreators().clear();`

What I also tried is replace `BuiltinOpResolver` with `BuiltinOpResolverWithoutDefaultDelegates`. Then you _do_ get rid off ""still reachable"" messages from `AllocateTensors`. With `BuiltinOpResolverWithoutDefaultDelegates` the XNNPACK backend is not employed, so seemingly the ""still reachable"" messages have to do with XNNPACK.
XNNPACK documentation says: ""To avoid memory and resource leaks, users must call xnn_deinitialize once for each successful xnn_initialize call."" `xnn_deinitialize` is called in function `TfLiteXNNPackWeightsCacheDelete` (tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc). But `TfLiteXNNPackWeightsCacheDelete` is never called. I believe this causes the memory problem, TfLite never seems to call `xnn_deinitialize`.

My company always keeps a clean Valgrind report for their software, so even if these Valgrind messages are not harmful, I still would like to get rid off them.

### Standalone code to reproduce the issue

```shell
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/interpreter.h>

using namespace tflite;
using namespace std;

int main() {
	unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""classifier.tflite"");  //This line generates one ""still reachable"" message
	const tflite::ops::builtin::BuiltinOpResolver op_resolver;
	tflite::InterpreterBuilder interpreter_builder(*model, op_resolver);
	interpreter_builder.SetNumThreads(1);

	unique_ptr<tflite::Interpreter> interpreter;
	interpreter_builder(&interpreter);
	interpreter->AllocateTensors();  //This line generates many ""still reachable"" messages

	//delete model->error_reporter();  //This will get rid off the first ""still reachable"" message
	return EXIT_SUCCESS;
}
```


### Relevant log output

```shell
$ valgrind --leak-check=full --show-reachable=yes ./app
==988== Memcheck, a memory error detector
==988== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==988== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
==988== Command: ./app
==988==
==988== error calling PR_SET_PTRACER, vgdb might block
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
==988==
==988== HEAP SUMMARY:
==988==     in use at exit: 704 bytes in 11 blocks
==988==   total heap usage: 2,144 allocs, 2,133 frees, 4,481,261 bytes allocated
==988==
==988== 8 bytes in 1 blocks are still reachable in loss record 1 of 11
==988==    at 0x4845013: operator new(unsigned long) (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x1F6CF9: tflite::DefaultErrorReporter() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FC7C: main (Main.cpp:9)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 2 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A454: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 3 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A46F: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 4 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55B06F: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 5 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55B017: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 6 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55AFCF: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 7 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55AF96: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 56 bytes in 1 blocks are still reachable in loss record 8 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A491: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 56 bytes in 1 blocks are still reachable in loss record 9 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A4B3: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPaeckDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 72 bytes in 1 blocks are still reachable in loss record 10 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A4D5: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 320 bytes in 1 blocks are still reachable in loss record 11 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A11E: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== LEAK SUMMARY:
==988==    definitely lost: 0 bytes in 0 blocks
==988==    indirectly lost: 0 bytes in 0 blocks
==988==      possibly lost: 0 bytes in 0 blocks
==988==    still reachable: 704 bytes in 11 blocks
==988==         suppressed: 0 bytes in 0 blocks
==988==
==988== For lists of detected and suppressed errors, rerun with: -s
==988== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```
</details>",2023-06-06 10:38:45+00:00,"stat:awaiting tensorflower, type:bug, comp:lite, TF 2.10"
TF >= 2.7 slowdown tf.data API,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

installed from pip

### Tensorflow Version

tf >= 2.7

### Custom Code

No

### OS Platform and Distribution

Linux, CentOS Linux 7

### Mobile device

No

### Python version

3.8, 3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Since TF >= 2.7, I observed a significant slowness when running `ds_iter.get_next()` where `ds_iter` is an iterator of tf.data.Dataset defined as `dataset.shuffle(...).cache().repeat(...).batch(..., drop_remainder=True).prefetch(tf.data.AUTOTUNE)`. The same code runs much faster on the same machine using TF <= 2.6. A reproducer is included below.

Looking at the [change log from TF2.8](https://www.exxactcorp.com/blog/Deep-Learning/tensorflow-2-8-0-released), I see the following related to Dataset API `(since v2.7) Stateful ops used in tf.data.Dataset`. However I am not entirely sure if there is a workaround in higher TF version to reach the same speed.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import time

X = tf.random.uniform(shape=[21000, 373])
Labels = tf.random.uniform(shape=[21000, 1])
batchsize = 8192
n_shuffles = 500000
n_samples = 21000
ds = tf.data.Dataset.from_tensor_slices((X, Labels))
ds = ds.shuffle(buffer_size=n_samples).cache().repeat(n_shuffles).batch(batchsize, drop_remainder=True).prefetch(tf.data.AUTOTUNE)
ds_iter = iter(ds)

num_iter = 100
loop_start = time.time()
for i in range(num_iter):
    ds_iter.get_next()
loop_stop  = time.time()
print('loop time', f'{loop_stop-loop_start : .4f}')
```


### Relevant log output

```shell
output of the reproducer:
- with TF >= 2.7: loop time  15.8203
- with TF <= 2.6: loop time  1.0519

device to run the above two tests:
NVIDIA A100-PCIE-40GB NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1

$$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Thu_Jan_28_19:32:09_PST_2021
Cuda compilation tools, release 11.2, V11.2.142
Build cuda_11.2.r11.2/compiler.29558016_0
```
</details>",2023-06-08 21:56:06+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.7"
W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) 

### Bazel version

bazel 5.3.2

### GCC/Compiler version

gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0

### CUDA/cuDNN version

11.2/8 in conda env

### GPU model and memory

laptop 3080 RTX

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
I have tensorflow 2 installed and also from the code below I see cudnn 8 is found. 


(samurai) mona@ard-gpu-01:~/samurai$ cat cudnn_test.py 
import tensorflow as tf

sys_details = tf.sysconfig.get_build_info()
cuda_version = sys_details[""cuda_version""]
print(cuda_version)

cudnn_version = sys_details[""cudnn_version""]
print(cudnn_version)


cuda_compute_capabilities = sys_details[""cuda_compute_capabilities""]
print(cuda_compute_capabilities)
(samurai) mona@ard-gpu-01:~/samurai$ python cudnn_test.py 
11.2
8
['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']
```

However, when I run the following command, I get an error that cudnn 8 is not found.

```
(samurai) mona@ard-gpu-01:~/samurai$ python train_samurai.py --config configs/samurai/samurai.txt --datadir data/duck/ --basedir . --expname duck_test --gpu 0
Namespace(config=None, basedir='.', expname='duck_test', batch_size=1024, learning_rate=0.0001, epochs=150, steps_per_epoch=2000, gpu='0', tpu=None, debug=False, profile=False, perturb=1.0, raw_noise_std=0.0, coarse_samples=64, linear_disparity_sampling=False, fine_samples=128, fourier_frequency=10, direction_fourier_frequency=4, random_encoding_offsets=True, fine_net_width=128, fine_net_depth=8, coarse_net_width=128, coarse_net_depth=6, appearance_latent_dim=32, diffuse_latent_dim=24, fix_diffuse=True, camera_distribution='sphere', use_fully_random_cameras=False, random_cameras_per_view=4, min_softmax_scaler=1.0, max_softmax_scaler=10.0, camera_weight_update_lr=0.3, camera_weight_update_momentum=0.75, bounding_size=0.5, resolution_factor=4, advanced_loss_done=80000, network_gradient_norm_clipping=0.1, camera_gradient_norm_clipping=-1, not_learn_r=False, not_learn_t=False, not_learn_f=False, edge_align_step=200, num_edge_align_steps=50, pretrained_camera_poses_folder=None, start_f_optimization=90000, start_fourier_anneal=0, finish_fourier_anneal=50000, slow_scheduler_decay=100000, brdf_schedule_decay=40000, lambda_smoothness=0.01, smoothness_bound_dividier=200, coarse_distortion_lambda=0.001, fine_distortion_lambda=0, normal_direction_lambda=0.005, mlp_normal_direction_lambda=0.0003, disable_posterior_scaling=False, disable_mask_uncertainty=True, lambda_brdf_decoder_smoothness=0.1, lambda_brdf_decoder_sparsity=0.01, camera_lr=0.003, camera_lr_decay=70, camera_regularization=0.1, aim_center_regularization=10.0, camera_rotation='lookat', learn_camera_offsets=True, basecolor_metallic=True, skip_decomposition=False, compose_on_white=True, rotating_object=False, single_env=False, brdf_preintegration_path='data/neural_pil/BRDFLut.hdr', illumination_network_path='data/neural_pil/illumination-network', datadir='data/duck/', max_resolution_dimension=400, test_holdout=16, dataset='samurai', load_gt_poses=False, canonical_pose=0, log_step=100, weights_epoch=5, validation_epoch=5, testset_epoch=150, video_epoch=50, lrate_decay=300, render_only=False)
2023-06-13 15:35:10.002485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-06-13 15:35:10.022702: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/home/mona/MVTec/HALCON-23.05-Progress//lib/x64-linux:/usr/local/cuda-11.7/lib64:/home/mona/onnx-tensorrt/build:
2023-06-13 15:35:10.022715: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Utilizing 0 GPUs for training.
2023-06-13 15:35:11.092766: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(70, 3)
Model: ""sequential_12""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 MappingNetwork/Layer_0 (Den  (None, 128)              16512     
 se)                                                             
                                                                 
 MappingNetwork/Layer_1 (Den  (None, 128)              16512     
 se)                                                             
                                                                 
 MappingNetwork/Final (Dense  (None, 768)              99072     
 )                                                               
                                                                 
 reshape_1 (Reshape)         (None, 2, 3, 128)         0         
                                                                 
=================================================================
Total params: 132,096
Trainable params: 132,096
Non-trainable params: 0
_________________________________________________________________
Model: ""sequential_13""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 ConditionalNetwork/Dense1 (  (None, 32)               192       
 Dense)                                                          
                                                                 
 ConditionalNetwork/DenseFin  (None, 256)              8448      
 al (Dense)                                                      
                                                                 
 reshape_2 (Reshape)         (None, 2, 128)            0         
                                                                 
=================================================================
Total params: 8,640
Trainable params: 8,640
Non-trainable params: 0
_________________________________________________________________
Found ckpts []
Starting training in epoch 0 at step 0
Start Training...
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
  25/2000 [..............................] - ETA: 42:41 - loss: 1.8824 - loss_camera: 7.2076 - fine_loss: 1.8019   

```
```


### Relevant log output

```shell
(samurai) mona@ard-gpu-01:~/samurai$ lsb_release -a
LSB Version:	core-11.1.0ubuntu4-noarch:security-11.1.0ubuntu4-noarch
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04.2 LTS
Release:	22.04
Codename:	jammy
(samurai) mona@ard-gpu-01:~/samurai$ uname -a
Linux ard-gpu-01 5.19.0-43-generic #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon May 22 13:39:36 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

```
(samurai) mona@ard-gpu-01:~/samurai$ nvidia-smi
Tue Jun 13 15:38:44 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3080 L...    On | 00000000:01:00.0 Off |                  N/A |
| N/A   49C    P8               17W /  90W|    102MiB / 16384MiB |     21%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      2549      G   /usr/lib/xorg/Xorg                           95MiB |
|    0   N/A  N/A      2983      G   ...libexec/gnome-remote-desktop-daemon        3MiB |
+---------------------------------------------------------------------------------------+
```
```
(samurai) mona@ard-gpu-01:~/samurai$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Jun__8_16:49:14_PDT_2022
Cuda compilation tools, release 11.7, V11.7.99
Build cuda_11.7.r11.7/compiler.31442593_0

```


The code is from this repo: https://github.com/google/samurai
```
</details>",2023-06-13 19:41:41+00:00,"type:bug, type:build/install, TF 2.8"
test,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-06-16 07:58:46+00:00,"invalid, TFLiteConverter"
FFT produces wrong results when using multiple GPUs with MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0 2.14.0-dev20230619

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8.0 / 8.6.0.163

### GPU model and memory

_No response_

### Current Behaviour?

Using TensorFlow FFT in a Keras model will produce incorrect results when using MirroredStrategy and multiple GPUs.
This is not an accuracy issue. The results of consecutive calls seem to be either correct or garbage.

I created a test Keras model that has one layer that does FFT. There is also a reference model using a DFT layer that is used to verify that incorrect behavior only happens when using tf.signal.fft.
Attached is a test application that runs both models in different combinations of MirroredStrategy/default strategy and eager/graph execution.
MirroredStrategy and graph execution is the combination that produces the error. At least two GPUs are required to reproduce the problem.
The output MAE loss is around 6.5, which translates to 650% error. (The absolute value of each entry in the correct output is 1.0.)

I think it's not a user error, but if it is, there should be an error or warning instead of incorrect results.

I was able to reproduce the issue with all TF fft variants (tf.signal.fft, tf.signal.rfft, tf.signal.stft, tf.signal.fft2d)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from scipy.linalg import dft
from math import sqrt

# Layer that does tf.signal.fft operation
class FFTLayer(tf.keras.layers.Layer):

    def call(self, x):
      fx = tf.signal.fft(x)
      return fx

# Layer that returns same results as tf.signal.fft op, but
# uses slower direct computation of DFT, implemented as matrix multiply.
class MatrixDFTLayer(tf.keras.layers.Layer):
    def __init__(self):
      super().__init__()
      self.dft = tf.cast(dft(1024), tf.complex64)

    def call(self, x):
        fx = self.dft @ tf.transpose(x)
        return tf.transpose(fx)


def create_model(use_mirrored_strategy: bool = True,
                              run_eagerly: bool = True,
                              layer_to_use: tf.keras.layers.Layer = FFTLayer) -> None:
    print(f""\ncreate model with: use_mirrored_strategy: {use_mirrored_strategy}, "",
          f""run_eagerly: {run_eagerly}, "",
          f""layer_to_use: {layer_to_use}"")

    if use_mirrored_strategy:
        distribution_strategy = tf.distribute.MirroredStrategy()
    else:
        distribution_strategy = tf.distribute.get_strategy()
    with distribution_strategy.scope():

        ins = tf.keras.layers.Input([1024], dtype=tf.complex64)
        x = layer_to_use()(ins)
        model = tf.keras.Model(inputs=ins, outputs=x)

        model.compile(
            loss=tf.keras.losses.MeanAbsoluteError(),
            run_eagerly=run_eagerly
        )
    return model


def create_data(fft_size, batch_size, num_steps):
    num_examples = num_steps * batch_size

    # y data is a complex vector of all (1/sqrt(2), (1/sqrt(2)j)
    train_y = np.ones([fft_size], np.float32)
    train_y = (1/sqrt(2))*train_y + (1/sqrt(2))*1j*train_y
    # abs mean is 1 -> MAE magnitude should be compared to 1
    print(""train_y mean: "", tf.reduce_mean(tf.abs(train_y)))

    # use inverse transform to create input data
    # fft(train_x) will produce train_y
    train_x = tf.signal.ifft(train_y)

    # clone data to get larger training set
    train_y = train_y[tf.newaxis, ...]
    train_x = train_x[tf.newaxis, ...]

    train_x = tf.tile(train_x, [num_examples, 1])
    train_y = tf.tile(train_y, [num_examples, 1])
    
    return train_x, train_y


fft_size = 1024
batch_size = 9
num_steps = 100
train_x, train_y = create_data(fft_size, batch_size, num_steps)

# Test cases with MatrixDFTLayer
# These are all ok, MAE close to 0.0
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=False, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=True, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=True, run_eagerly=False, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")

# Test Cases using TF FFT. These fail when using MirroredStrategy.
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=False, layer_to_use=FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=True, layer_to_use=FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# fail, 
model = create_model(use_mirrored_strategy=True, run_eagerly=False,layer_to_use= FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
```


### Relevant log output

```shell
train_y mean:  tf.Tensor(1.0, shape=(), dtype=float32)

create model with: use_mirrored_strategy: False,  run_eagerly: False,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: True,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: True,  run_eagerly: False,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: False,  layer_to_use: <class '__main__.FFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: True,  layer_to_use: <class '__main__.FFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: True,  run_eagerly: False,  layer_to_use: <class '__main__.FFTLayer'>
loss: 6.451958656311035
```
</details>",2023-06-19 19:28:28+00:00,"stat:awaiting tensorflower, type:bug, comp:dist-strat, TF 2.12"
"Lack of Documentation for GPU Use, Especially in Metal GPUs in MacBook M1, M1 Max and M2 Chips","Hi,

Whenever I tried to use a GPU on MPS with MacBook M1, I generally fail to use the GPU and whenever I tried to reach out to documentation for help, it doesn't provide much help. In addition to the documentation issue, there's a slowdown on M1, M1 Max and M2 chips when I use TensorFlow 2.11+

WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5
2023-06-27 14:57:34.718668: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
517/517 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.82332023-06-27 14:58:57.433849: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.",2023-06-27 12:04:56+00:00,"stat:awaiting response, comp:gpu, type:docs-feature"
TensorFlow GPU,"New versions of TensorFlow no longer support Windows native Gpus.

If built in WSL2

Is there a shortcut to read data from Windows directory in WSL2?

Because I need to use TensorFlow GPU to compute a lot of data.
Copying to WSL2 is slow",2023-07-09 17:05:54+00:00,"type:support, wsl2"
site/en/guide/create_op.md example code has memory leak,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.6.5

### Custom code

Yes

### OS platform and distribution

linux centos 7.6

### Mobile device

linux centos 7.6

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?


In the demo code, Tensor* **output_tensor** was allocated but the memory was not free

`#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output_flat(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output_flat(0) = input(0);
  }
};`

### Standalone code to reproduce the issue

```shell
Repeatedly call the ZeroOutOp. You'll see the memory continue to increase
```


### Relevant log output

_No response_",2023-07-10 14:20:47+00:00,"type:docs-bug, type:bug"
Memory out of bounds in compiled tflite with emscripten.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have compiled tflite using cmake (without XNNPACK support) and emscripten (both latest 3.1.42 and 3.1.10). 
When trying to perform inference at the browser with my model I get the following error:

vmt.wasm:0x31cff Uncaught RuntimeError: memory access out of bounds
    at vmt.wasm:0x31cff
    at vmt.wasm:0x1f7a94
    at vmt.wasm:0x3c4910
    at vmt.wasm:0x65ace
    at vmt.wasm:0x231c3e
    at vmt.wasm:0x458a49
    at vmt.wasm:0x517c60
    at img.onload (index.html:772:28)

This happens with all of my models at the very first operation (pad). When inspecting the .wasm file using chrome dev tools I see that the error happens at a ""memory.fill"" operation.

### Standalone code to reproduce the issue

```shell
I have compiled tflite with the following emcmake command:

cmake -DCMAKE_CXX_FLAGS=""-lpthread -pthread -lpthread -s USE_PTHREADS"" -DTFLITE_ENABLE_MMAP=OFF -DTFLITE_ENABLE_NNAPI=OFF -DTFLITE_ENABLE_RUY=ON -DTFLITE_ENABLE_XNNPACK=OFF ..

while when compiling my project with emscripten (including the above resulting libraries) I use the following flags:

	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s INITIAL_MEMORY=512MB"")
	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s ALLOW_MEMORY_GROWTH=1"")
	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s ALLOW_TABLE_GROWTH=1"")
```


### Relevant log output

```shell
This is the output of PrintInterpreterState right before the first inference.

[WASM] === Pre-invoke Interpreter State ===
pre-vmt.js:11 [WASM] Interpreter has 1 subgraphs.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] -----------Subgraph-0 has 134 tensors and 49 nodes------------
pre-vmt.js:11 [WASM] 1 Inputs: [0] -> 602112B (0.57MB)
pre-vmt.js:11 [WASM] 1 Outputs: [122] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Tensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  
pre-vmt.js:11 [WASM] Tensor   0  ��ʻ䯻9:󺂶*򨓮.. kTfLiteFloat32  kTfLiteArenaRw     602112   / 0.57 [1,224,224,3] [0, 602112)
pre-vmt.js:11 [WASM] Tensor   1 騅:��񛻺󿰻��m... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [690960, 691024)
pre-vmt.js:11 [WASM] Tensor   2 r畼��匹��t;��... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [690864, 690928)
pre-vmt.js:11 [WASM] Tensor   3 jԐ;��4i;⦄;Q;箮. kTfLiteFloat32  kTfLiteMmapRo      160      / 0.00 [40] [690688, 690848)
pre-vmt.js:11 [WASM] Tensor   4 究򑃷çc6ԯ#6ߗ<׹... kTfLiteFloat32  kTfLiteMmapRo      160      / 0.00 [40] [690512, 690672)
pre-vmt.js:11 [WASM] Tensor   5 :#��
𯫿��7tU��... kTfLiteFloat32  kTfLiteMmapRo      224      / 0.00 [56] [690272, 690496)
pre-vmt.js:11 [WASM] Tensor   6 ƙl��7򣡷/򷕈4��.. kTfLiteFloat32  kTfLiteMmapRo      224      / 0.00 [56] [690032, 690256)
pre-vmt.js:11 [WASM] Tensor   7 ����뻪��3z}��.. kTfLiteFloat32  kTfLiteMmapRo      256      / 0.00 [64] [689760, 690016)
pre-vmt.js:11 [WASM] Tensor   8 ᡁ������땐6ԝ... kTfLiteFloat32  kTfLiteMmapRo      256      / 0.00 [64] [689488, 689744)
pre-vmt.js:11 [WASM] Tensor   9 􌶄��5.8^L𷆄򷳱... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [688896, 689472)
pre-vmt.js:11 [WASM] Tensor  10 ��""ۀ7��Ce����... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [688304, 688880)
pre-vmt.js:11 [WASM] Tensor  11 ģ
pre-vmt.js:11 [WASM] 7C^J𐬣𶓃6񪤷׾... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [687712, 688288)
pre-vmt.js:11 [WASM] Tensor  12 ��ȏ6띓����8h... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [687120, 687696)
pre-vmt.js:11 [WASM] Tensor  13 ��A򪶚윶��䞏��.. kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686816, 687104)
pre-vmt.js:11 [WASM] Tensor  14 ԙõ`򫷾F6򓕶O򙷳c... kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686512, 686800)
pre-vmt.js:11 [WASM] Tensor  15 ū쵄񉸻罷{\ѵfW𶯜... kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686208, 686496)
pre-vmt.js:11 [WASM] Tensor  16 &ꋷ𷏸ᛸ6��ꮮ. kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [685904, 686192)
pre-vmt.js:11 [WASM] Tensor  17 |㗶񏍷򛀷��\Y7... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [685312, 685888)
pre-vmt.js:11 [WASM] Tensor  18 蕷ΒU7��`&÷勸f+... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [684720, 685296)
pre-vmt.js:11 [WASM] Tensor  19 w춃֣74ٿ𣔝����.. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [683552, 684704)
pre-vmt.js:11 [WASM] Tensor  20 ;""6!򑶷膷ڽ𷞮��... kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [682384, 683536)
pre-vmt.js:11 [WASM] Tensor  21 󠼷YPQ������75... kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [681216, 682368)
pre-vmt.js:11 [WASM] Tensor  22 ��󰳳IB}������.. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [680048, 681200)
pre-vmt.js:11 [WASM] Tensor  23 򳤷񲂷wް5��^	8􊮮. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [678880, 680032)
pre-vmt.js:11 [WASM] Tensor  24 _��m󷟝򵳂󶬾𼾮.. kTfLiteFloat32  kTfLiteMmapRo      32       / 0.00 [8] [678832, 678864)
pre-vmt.js:11 [WASM] Tensor  25 P��&;𶿮����7#׮.. kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [678752, 678816)
pre-vmt.js:11 [WASM] Tensor  26 ��5񺉷a꨷ᘑ��.. kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [678672, 678736)
pre-vmt.js:11 [WASM] Tensor  27 卞7󯉷򨮷��󠚷��. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678560, 678656)
pre-vmt.js:11 [WASM] Tensor  28 FR񷀃󷴫d������.. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678448, 678544)
pre-vmt.js:11 [WASM] Tensor  29 򺜵W7𽒷񪶷̒#��   kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678336, 678432)
pre-vmt.js:11 [WASM] Tensor  30 $뀷��R𷸙u쭌7󿮮. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678224, 678320)
pre-vmt.js:11 [WASM] Tensor  31 \~��ط$i򷮼
6҄*觮.. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678112, 678208)
pre-vmt.js:11 [WASM] Tensor  32 ��0��涂񖷝f5��. kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677904, 678096)
pre-vmt.js:11 [WASM] Tensor  33 ؉h������7˜𶠺... kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677696, 677888)
pre-vmt.js:11 [WASM] Tensor  34 󚫷\7͛$♩��7[... kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677488, 677680)
pre-vmt.js:11 [WASM] Tensor  35 9X����������.. kTfLiteFloat32  kTfLiteMmapRo      1728     / 0.00 [16,3,3,3] [675744, 677472)
pre-vmt.js:11 [WASM] Tensor  36 jť<箯󧬍󘴙;Љ&󿈮.. kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [1,3,3,16] [675152, 675728)
pre-vmt.js:11 [WASM] Tensor  37 􅻻λ""<����?<ɓ... kTfLiteFloat32  kTfLiteMmapRo      512      / 0.00 [8,1,1,16] [674624, 675136)
pre-vmt.js:11 [WASM] Tensor  38 ޛȻx׃<򁻝ě<u+��... kTfLiteFloat32  kTfLiteMmapRo      1280     / 0.00 [40,1,1,8] [673328, 674608)
pre-vmt.js:11 [WASM] Tensor  39 ��Ԍ󼲍׹󵏼Ϩ��... kTfLiteFloat32  kTfLiteMmapRo      1440     / 0.00 [1,3,3,40] [671872, 673312)
pre-vmt.js:11 [WASM] Tensor  40 ?ȑ򄏱;3d1󞻳��       kTfLiteFloat32  kTfLiteMmapRo      2560     / 0.00 [16,1,1,40] [669296, 671856)
pre-vmt.js:11 [WASM] Tensor  41 Cһ;��ٻ:뼩攻ŷ... kTfLiteFloat32  kTfLiteMmapRo      3584     / 0.00 [56,1,1,16] [665696, 669280)
pre-vmt.js:11 [WASM] Tensor  42 3������򥅡;b... kTfLiteFloat32  kTfLiteMmapRo      2016     / 0.00 [1,3,3,56] [663664, 665680)
pre-vmt.js:11 [WASM] Tensor  43 Mٚ<啤<j��8|��쮮. kTfLiteFloat32  kTfLiteMmapRo      3584     / 0.00 [16,1,1,56] [660064, 663648)
pre-vmt.js:11 [WASM] Tensor  44 ᦺ��򦖶:晼4櫻k... kTfLiteFloat32  kTfLiteMmapRo      4096     / 0.00 [64,1,1,16] [655952, 660048)
pre-vmt.js:11 [WASM] Tensor  45 轼\<j4<Ӯ:𡻻ug... kTfLiteFloat32  kTfLiteMmapRo      2304     / 0.00 [1,3,3,64] [653632, 655936)
pre-vmt.js:11 [WASM] Tensor  46 ��৐��󶜿󄲢𠧮.. kTfLiteFloat32  kTfLiteMmapRo      6144     / 0.01 [24,1,1,64] [647472, 653616)
pre-vmt.js:11 [WASM] Tensor  47 ��񒘼Ѡj󫈞󭲐<�� kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [633632, 647456)
pre-vmt.js:11 [WASM] Tensor  48 '����󼰛��󋍮.. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [628432, 633616)
pre-vmt.js:11 [WASM] Tensor  49 `��𼢈<񗋻          kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [24,1,1,144] [614592, 628416)
pre-vmt.js:11 [WASM] Tensor  50 ا""󖗽򫋺<󀮹͑x󪁮.. kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [600752, 614576)
pre-vmt.js:11 [WASM] Tensor  51 ȷk󒏷:󁌝��󦦮.. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [595552, 600736)
pre-vmt.js:11 [WASM] Tensor  52 t=򜧨;6򌼾)𻀫��... kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [24,1,1,144] [581712, 595536)
pre-vmt.js:11 [WASM] Tensor  53 򚪼f񒻯��=弯P... kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [72,1,1,24] [574784, 581696)
pre-vmt.js:11 [WASM] Tensor  54 (����nۻ:,��󛥮.. kTfLiteFloat32  kTfLiteMmapRo      2592     / 0.00 [1,3,3,72] [572176, 574768)
pre-vmt.js:11 [WASM] Tensor  55 ��뙿��󭩄;��?
pre-vmt.js:11 [WASM] 􄠫TfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [24,1,1,72] [565248, 572160)
pre-vmt.js:11 [WASM] Tensor  56 ��̘<9D򆼇*t<'��... kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [72,1,1,24] [558320, 565232)
pre-vmt.js:11 [WASM] Tensor  57 𳼑sD󠺬;󋽦􊼠R... kTfLiteFloat32  kTfLiteMmapRo      2592     / 0.00 [1,3,3,72] [555712, 558304)
pre-vmt.js:11 [WASM] Tensor  58 mX��h:%4k<̫'<(ç<񝮮. kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [24,1,1,72] [548784, 555696)
pre-vmt.js:11 [WASM] Tensor  59 o󻛄񷖡��<𽤼񁮮. kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [534944, 548768)
pre-vmt.js:11 [WASM] Tensor  60 Ⱥּ󏉼򘄼]��d_;�. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [529744, 534928)
pre-vmt.js:11 [WASM] Tensor  61 ֭C򢊛;󥙻怙<貦��.. kTfLiteFloat32  kTfLiteMmapRo      27648    / 0.03 [48,1,1,144] [502080, 529728)
pre-vmt.js:11 [WASM] Tensor  62 ��􊥺*?""��󸸔��.. kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [446768, 502064)
pre-vmt.js:11 [WASM] Tensor  63 򶼼1n⻗��-Ի񛹪�� kTfLiteFloat32  kTfLiteMmapRo      10368    / 0.01 [1,3,3,288] [436384, 446752)
pre-vmt.js:11 [WASM] Tensor  64 ��ֻ𲰼􅛻o𣼣�� kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [48,1,1,288] [381072, 436368)
pre-vmt.js:11 [WASM] Tensor  65 V;潛򠛮<ʻ㻅S;򮮮 kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [325760, 381056)
pre-vmt.js:11 [WASM] Tensor  66 ¿,<
pre-vmt.js:11 [WASM] 񏻥E����ݻE񮮮 kTfLiteFloat32  kTfLiteMmapRo      10368    / 0.01 [1,3,3,288] [315376, 325744)
pre-vmt.js:11 [WASM] Tensor  67 dճ<󋺑񢼍����Ү.. kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [48,1,1,288] [260064, 315360)
pre-vmt.js:11 [WASM] Tensor  68 ����𣺼d򻍪~;�� kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [204752, 260048)
pre-vmt.js:11 [WASM] Tensor  69 冻񍖼𣄼𲿼󲎽��. kTfLiteInt32    kTfLiteMmapRo      8        / 0.00 [2] [204720, 204728)
pre-vmt.js:11 [WASM] Tensor  70 ~&��<��`<ݺNϘ;𫮮. kTfLiteInt32    kTfLiteMmapRo      12       / 0.00 [3] [204688, 204700)
pre-vmt.js:11 [WASM] Tensor  71 ��%ɻ	`޻󴵹ϛ;t񮮮 kTfLiteFloat32  kTfLiteMmapRo      708      / 0.00 [177] [203968, 204676)
pre-vmt.js:11 [WASM] Tensor  72 <񌹂L��;ԅ򻬰A󾫮.. kTfLiteInt32    kTfLiteMmapRo      32       / 0.00 [4,2] [203920, 203952)
pre-vmt.js:11 [WASM] Tensor  73 Ё򻙃ٻ:
ԺV۪;��
pre-vmt.js:11 [WASM] ;Ŗ... kTfLiteFloat32  kTfLiteMmapRo      203904   / 0.19 [177,288] [0, 203904)
pre-vmt.js:11 [WASM] Tensor  74 2
pre-vmt.js:11 [WASM] 纝S
󊺅򚋟��<... kTfLiteFloat32  kTfLiteArenaRw     612912   / 0.58 [1,226,226,3] [2759680, 3372592)
pre-vmt.js:11 [WASM] Tensor  75 ⻒����S󁟮󼘮.. kTfLiteFloat32  kTfLiteArenaRw     802816   / 0.77 [1,112,112,16] [1956864, 2759680)
pre-vmt.js:11 [WASM] Tensor  76 z򛼷x0<ʬ𠻗��߮.. kTfLiteFloat32  kTfLiteArenaRw     831744   / 0.79 [1,114,114,16] [602112, 1433856)
pre-vmt.js:11 [WASM] Tensor  77 󥺗��Ї􆛁����.. kTfLiteFloat32  kTfLiteArenaRw     200704   / 0.19 [1,56,56,16] [1433856, 1634560)
pre-vmt.js:11 [WASM] Tensor  78 ½ẇ                     kTfLiteFloat32  kTfLiteArenaRw     100352   / 0.10 [1,56,56,8] [602112, 702464)
pre-vmt.js:11 [WASM] Tensor  79 𖒽񅬽Ɓּ򽮨=ڝ... kTfLiteFloat32  kTfLiteArenaRw     501760   / 0.48 [1,56,56,40] [1140352, 1642112)
pre-vmt.js:11 [WASM] Tensor  80 󦜼󅯻牪=𞁼{Jk<󯮮. kTfLiteFloat32  kTfLiteArenaRw     538240   / 0.51 [1,58,58,40] [602112, 1140352)
pre-vmt.js:11 [WASM] Tensor  81 )a˼q󀀀
pre-vmt.js:11 [WASM] ������<蚮.. kTfLiteFloat32  kTfLiteArenaRw     125440   / 0.12 [1,28,28,40] [1140352, 1265792)
pre-vmt.js:11 [WASM] Tensor  82 ⰾ󝔦<񃼄h��8��.. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [953344, 1003520)
pre-vmt.js:11 [WASM] Tensor  83 ��į<򑁼Q󬽴߼6)... kTfLiteFloat32  kTfLiteArenaRw     175616   / 0.17 [1,28,28,56] [602112, 777728)
pre-vmt.js:11 [WASM] Tensor  84 ��s��񌽌𱽬��*... kTfLiteFloat32  kTfLiteArenaRw     175616   / 0.17 [1,28,28,56] [777728, 953344)
pre-vmt.js:11 [WASM] Tensor  85 ��La޻��4<zL󼚨... kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [602112, 652288)
pre-vmt.js:11 [WASM] Tensor  86 <򔼌��󰽊V-��<��. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [652288, 702464)
pre-vmt.js:11 [WASM] Tensor  87 DZQ��󿒇<;zZ����.. kTfLiteFloat32  kTfLiteArenaRw     200704   / 0.19 [1,28,28,64] [832512, 1033216)
pre-vmt.js:11 [WASM] Tensor  88 ^��j􌧁;E����... kTfLiteFloat32  kTfLiteArenaRw     230400   / 0.22 [1,30,30,64] [602112, 832512)
pre-vmt.js:11 [WASM] Tensor  89 ��󛶼+)������.. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,14,14,64] [832512, 882688)
pre-vmt.js:11 [WASM] Tensor  90 ༟<cdͻYϑ������.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [882688, 901504)
pre-vmt.js:11 [WASM] Tensor  91 ;��༕󗼫}<~_.��.. kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [602112, 715008)
pre-vmt.js:11 [WASM] Tensor  92 덒;T@ë<ˠ��v<5�� kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [715008, 827904)
pre-vmt.js:11 [WASM] Tensor  93 򣀼򧂼4񥻬SN����.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [827904, 846720)
pre-vmt.js:11 [WASM] Tensor  94 ��6𢺊$@<󶢺T>ֺ舮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [846720, 865536)
pre-vmt.js:11 [WASM] Tensor  95 i粼**&<ļ򅫈;B򻤺... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [602112, 715008)
pre-vmt.js:11 [WASM] Tensor  96 S{.ZR����<9��1... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [715008, 827904)
pre-vmt.js:11 [WASM] Tensor  97 Pw􀗃􈨉��㢥󧂮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [827904, 846720)
pre-vmt.js:11 [WASM] Tensor  98 ߴüs��9󢠠           kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [715008, 733824)
pre-vmt.js:11 [WASM] Tensor  99 t#򼇟񼻺<􊚻󣫚򡋮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 100 ��ݿ������9\r... kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 101 ��ڛ��𼮲��N􏕮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [602112, 620928)
pre-vmt.js:11 [WASM] Tensor 102 򝷺iܨ<<<򾃆;󵮮. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [733824, 752640)
pre-vmt.js:11 [WASM] Tensor 103 ��hԂ<o񉼿hἑݎ<f�� kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 104 󻦨𼯄[<��W~J󗩮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 105 0»Lv<ǡF򟻸;jV��... kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [715008, 733824)
pre-vmt.js:11 [WASM] Tensor 106 s	��W򈤋񊗺9fZ��.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [602112, 620928)
pre-vmt.js:11 [WASM] Tensor 107 wŔ;􃋼��~<��<ڷ... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [749568, 862464)
pre-vmt.js:11 [WASM] Tensor 108 KԽ;㚹֛!;񬻠         kTfLiteFloat32  kTfLiteArenaRw     147456   / 0.14 [1,16,16,144] [602112, 749568)
pre-vmt.js:11 [WASM] Tensor 109 𜭼艺<ﻨ9刼;
pre-vmt.js:11 [WASM] *<􉮮. kTfLiteFloat32  kTfLiteArenaRw     28224    / 0.03 [1,7,7,144] [749568, 777792)
pre-vmt.js:11 [WASM] Tensor 110 0��ĺ;ۚ<��ϭܻI󮮮 kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [715008, 724416)
pre-vmt.js:11 [WASM] Tensor 111 sl$􀍋;OQֺ<��𻺔ޮ.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 112 2\𼀄��;ဥ;��Ȉ... kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 113 󂆅��􎙧<񶧻񍮮. kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [602112, 611520)
pre-vmt.js:11 [WASM] Tensor 114 ꥩ8^񑻢����         kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [724416, 733824)
pre-vmt.js:11 [WASM] Tensor 115 𛁺����[񁺼��߮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 116 /ӎ􀀀
pre-vmt.js:11 [WASM] 껵򀼏��B�� kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 117 ϣ󺕠����;f򜺹
pre-vmt.js:11 [WASM] ... kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [715008, 724416)
pre-vmt.js:11 [WASM] Tensor 118 ᳠:$p溓l��㻂	𺩯... kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [658560, 667968)
pre-vmt.js:11 [WASM] Tensor 119 ��򭶹��Cျꬓ:��. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 120 SW��Y��;��e��... kTfLiteFloat32  kTfLiteArenaRw     1152     / 0.00 [1,1,1,288] [659712, 660864)
pre-vmt.js:11 [WASM] Tensor 121 ��Ę;ᑩ򘯱󨞇󎓮.. kTfLiteFloat32  kTfLiteArenaRw     708      / 0.00 [1,177] [602112, 602820)
pre-vmt.js:11 [WASM] Tensor 122 긟𔎴;ڗs񡮠����.. kTfLiteFloat32  kTfLiteArenaRw     708      / 0.00 [1,59,3] [602880, 603588)
pre-vmt.js:11 [WASM] Tensor 123 (nil)                     kTfLiteInt32    kTfLiteArenaRw     16       / 0.00 [4] [660864, 660880)
pre-vmt.js:11 [WASM] Tensor 124 (nil)                     kTfLiteInt32    kTfLiteArenaRw     8        / 0.00 [2] [660928, 660936)
pre-vmt.js:11 [WASM] Tensor 125 (nil)                     kTfLiteFloat32  kTfLiteArenaRw     1152     / 0.00 [288] [658560, 659712)
pre-vmt.js:11 [WASM] Tensor 126 (nil)                     kTfLiteInt32    kTfLiteDynamic     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 127 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 128 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 129 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 130 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 131 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 132 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 133 (nil)                     kTfLiteFloat32  kTfLiteArenaRw     1354752  / 1.29 [1,112,112,27] [602112, 1956864)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteArenaRw Info: 
pre-vmt.js:11 [WASM] Tensor 133 has the max size 1354752 bytes (1.292 MB).
pre-vmt.js:11 [WASM] This memory arena is estimated as[0x15e33b0, 0x12abd80), taking 3372592 bytes (3.216 MB).
pre-vmt.js:11 [WASM] One possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:
pre-vmt.js:11 [WASM] Tensor 0 -> 133 -> 75 -> 74.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteArenaRwPersistent Info: not holding any allocation.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteMmapRo Info: 
pre-vmt.js:11 [WASM] Tensor 73 has the max size 203904 bytes (0.194 MB).
pre-vmt.js:11 [WASM] This memory arena is estimated as[0x10dde70, 0x1035320), taking 691024 bytes (0.659 MB).
pre-vmt.js:11 [WASM] One possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:
pre-vmt.js:11 [WASM] Tensor 73 -> 72 -> 71 -> 70 -> 69 -> 68 -> 67 -> 66 -> 65 -> 64 -> 63 -> 62 -> 61 -> 60 -> 59 -> 58 -> 57 -> 56 -> 55 -> 54 -> 53 -> 52 -> 51 -> 50 -> 49 -> 48 -> 47 -> 46 -> 45 -> 44 -> 43 -> 42 -> 41 -> 40 -> 39 -> 38 -> 37 -> 36 -> 35 -> 34 -> 33 -> 32 -> 31 -> 30 -> 29 -> 28 -> 27 -> 26 -> 25 -> 24 -> 23 -> 22 -> 21 -> 20 -> 19 -> 18 -> 17 -> 16 -> 15 -> 14 -> 13 -> 12 -> 11 -> 10 -> 9 -> 8 -> 7 -> 6 -> 5 -> 4 -> 3 -> 2 -> 1.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteDynamic Info: not holding any allocation.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Node   0 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[0,72] -> 602144B (0.57MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[74] -> 612912B (0.58MB)
pre-vmt.js:11 [WASM] Node   1 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[74,35,1] -> 614704B (0.59MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[75] -> 802816B (0.77MB)
pre-vmt.js:11 [WASM]   1 Temporary Tensors:[133] -> 1354752B (1.29MB)
pre-vmt.js:11 [WASM] Node   2 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[75,72] -> 802848B (0.77MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[76] -> 831744B (0.79MB)
pre-vmt.js:11 [WASM] Node   3 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[76,36,2] -> 832384B (0.79MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[77] -> 200704B (0.19MB)
pre-vmt.js:11 [WASM] Node   4 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[77,37,24] -> 201248B (0.19MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[78] -> 100352B (0.10MB)
pre-vmt.js:11 [WASM] Node   5 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[78,38,3] -> 101792B (0.10MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[79] -> 501760B (0.48MB)
pre-vmt.js:11 [WASM] Node   6 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[79,72] -> 501792B (0.48MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[80] -> 538240B (0.51MB)
pre-vmt.js:11 [WASM] Node   7 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[80,39,4] -> 539840B (0.51MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[81] -> 125440B (0.12MB)
pre-vmt.js:11 [WASM] Node   8 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[81,40,25] -> 128064B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[82] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node   9 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[82,41,5] -> 53984B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[83] -> 175616B (0.17MB)
pre-vmt.js:11 [WASM] Node  10 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[83,42,6] -> 177856B (0.17MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[84] -> 175616B (0.17MB)
pre-vmt.js:11 [WASM] Node  11 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[84,43,26] -> 179264B (0.17MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[85] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  12 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[85,82] -> 100352B (0.10MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[86] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  13 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[86,44,7] -> 54528B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[87] -> 200704B (0.19MB)
pre-vmt.js:11 [WASM] Node  14 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[87,72] -> 200736B (0.19MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[88] -> 230400B (0.22MB)
pre-vmt.js:11 [WASM] Node  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[88,45,8] -> 232960B (0.22MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[89] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  16 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[89,46,27] -> 56416B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[90] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  17 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[90,47,9] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[91] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  18 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[91,48,10] -> 118656B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[92] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  19 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[92,49,28] -> 126816B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[93] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  20 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[93,90] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[94] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  21 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[94,50,11] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[95] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  22 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[95,51,12] -> 118656B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[96] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  23 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[96,52,29] -> 126816B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[97] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  24 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[97,94] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[98] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  25 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[98,53,13] -> 26016B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[99] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  26 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[99,54,14] -> 59328B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[100] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  27 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[100,55,30] -> 63456B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[101] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  28 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[101,98] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[102] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  29 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[102,56,15] -> 26016B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[103] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  30 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[103,57,16] -> 59328B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[104] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  31 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[104,58,31] -> 63456B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[105] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  32 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[105,102] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[106] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  33 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[106,59,17] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[107] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  34 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[107,72] -> 112928B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[108] -> 147456B (0.14MB)
pre-vmt.js:11 [WASM] Node  35 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[108,60,18] -> 153216B (0.15MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[109] -> 28224B (0.03MB)
pre-vmt.js:11 [WASM] Node  36 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[109,61,32] -> 56064B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[110] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  37 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[110,62,19] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[111] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  38 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[111,63,20] -> 67968B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[112] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  39 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[112,64,33] -> 111936B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[113] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  40 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[113,110] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[114] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  41 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[114,65,21] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[115] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  42 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[115,66,22] -> 67968B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[116] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  43 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[116,67,34] -> 111936B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[117] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  44 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[117,114] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[118] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  45 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[118,68,23] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[119] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  46 Operator Builtin Code  40 MEAN (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[119,69] -> 56456B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[120] -> 1152B (0.00MB)
pre-vmt.js:11 [WASM]   4 Temporary Tensors:[123-126] -> 1176B (0.00MB)
pre-vmt.js:11 [WASM] Node  47 Operator Builtin Code   9 FULLY_CONNECTED (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[120,73,71] -> 205764B (0.20MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[121] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] Node  48 Operator Builtin Code  22 RESHAPE (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[121,70] -> 720B (0.00MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[122] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Execution plan as the list of 49 nodes invoked in-order: [0-48]
pre-vmt.js:11 [WASM] --------------Subgraph-0 dump has completed--------------
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] --------------Memory Arena Status Start--------------
pre-vmt.js:11 [WASM] Total memory usage: 3372848 bytes (3.217 MB)
pre-vmt.js:11 [WASM] - Total arena memory usage: 3372848 bytes (3.217 MB)
pre-vmt.js:11 [WASM] - Total dynamic memory usage: 0 bytes (0.000 MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Subgraph#0   Arena (Normal)        3372720 (100.00%)
pre-vmt.js:11 [WASM] Subgraph#0   Arena (Persistent)        128 (0.00%)
pre-vmt.js:11 [WASM] --------------Memory Arena Status End--------------


vmt.wasm:0x31cff Uncaught RuntimeError: memory access out of bounds
    at vmt.wasm:0x31cff
    at vmt.wasm:0x1f7a94
    at vmt.wasm:0x3c4910
    at vmt.wasm:0x65ace
    at vmt.wasm:0x231c3e
    at vmt.wasm:0x458a49
    at vmt.wasm:0x517c60
    at Module._landmarkDetection (vmt.js:6100:85)
    at VmtHelper.cycleForSingleImage (vmt-helper.js:115:29)
    at img.onload (index.html:772:28)
```
",2023-07-10 18:41:30+00:00,"stat:awaiting response, type:bug, stale, comp:lite, type:performance, TF 2.13"
tf.data parallel filter dataset for each class and interleave not giving expected speedup ,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

_No response_

### Current behavior?

I've tried to get some help on [Tensorflow Forum](https://discuss.tensorflow.org/t/dataset-with-tf-data-with-batches-of-randomly-n-chosen-speakers-and-their-m-utterances/17824) but I've got no response so I decided to post an issue here. Post on TFRecords Forum holds extensive description of my problem (tf.data dataloader for GE2E loss), but I'll try to trim it down only to technical matter.

I'm working on implementation of dataloader, where I want to get (n_speakers x n_utters) samples in every batch. I've already prepared dataset as 10 sharded TFRecord (as suggested in docs) and I wanted to build pairing mechanism using tf.data API. I've menaged to do so with `tf.data.Dataset.choose_from_datasets`, but this requires me to have a list of datasets, where each dataset contains only samples from one speaker. To prepare such dataset I've filtered my one big TFRecords Dataset with each speaker_id. My implementation feels criminal, because it iterates through whole dataset as many times as there are speakers. I've tried to help it with parallel calls (mapping the filter function with list of speakers), but there was no speedup. I've also tried using interleave (setting only `num_parallel_calls` to `AUTOTUNE`) on my dataset parsing, but all I got was slower execution time.

Is if there is any easy and reasonable solution to this filtering issue (to filter whole dataset by each class in one pass)? My only idea to skip filtering at all is to separate each speaker in different TFRecord at the moment of dataset creation. Also how should I use interleave with my code? 

Currently it takes a long time to make a first pass over dataset (~10min for 10GB of TFRecords, next iters when filtered datasets get cached its 2s) - it can be an issue with training with bigger dataset I suppose.

I'm attaching the relevant code but it's not standalone unfortunately, as I can not attach used data. Also some parsing code is missing, but I think it's not that relevant - my examples are stored as melspectrograms with speaker_id label. I could mock some data if someone was interested in helping me.




### Standalone code to reproduce the issue

```shell
dataset = tf.data.Dataset.from_tensor_slices(files)
tfrecords_reader = functools.partial(
    self._tfrecords_reader,
    speaker_id=self._speaker_id,
    speaker_gender=self._speaker_gender,
    speaker_id_key=self._speaker_id_key,
    speaker_gender_key=self._speaker_gender_key,
)

dataset = tf.data.TFRecordDataset(dataset)
dataset = dataset.map(tfrecords_reader, num_parallel_calls=AUTOTUNE)

# NOT PARALLEL WAY - surprisingly it takes as long as parallel method...
# datasets = []
# for speaker_id in self._speaker_list:
#     datasets.append(
#         dataset.filter(lambda x, y: y[""speaker_id""] == speaker_id)
#         .cache()
#         .shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)
#         .repeat()
#     )

# Filtering for speakers
speakers = tf.data.Dataset.from_tensor_slices(self._speaker_list)
dataset = speakers.map(
    lambda speaker_id: dataset.filter(lambda x, y: y[""speaker_id""] == speaker_id)
    .cache()
    .shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)
    .repeat(),
    num_parallel_calls=AUTOTUNE,
)

datasets_idx = tf.data.Dataset.range(len(self._speaker_list))
# Define those values statically for uniform_candidate_sampler which requires static int, even tho it looks silly
len_speakers = len(speakers)
len_pairs = len_speakers - 1

def pair_speakers(x):
    """"""Pair all speakers in dataset.
    
    Pairing is done in a way where each speaker is taken at least once.
    Also we can guarantee that those pairs are being re-generated each time.
    """"""
    to_pair = tf.reshape(tf.where(tf.range(len(speakers)) != x), [1, -1])  # Exclude speaker x from being drawed
    pairs = tf.random.uniform_candidate_sampler(to_pair, len_pairs, self._num_speakers - 1, True, len_speakers)[
        0
    ]  # sample n_speakers-1 pairings without replacement from len(speakers)-1 speakers
    paired = tf.concat([[x], pairs], 0)  # concat speaker x with its pairs
    return tf.repeat(
        paired, tf.repeat(self._num_utters, self._num_speakers)
    )  # repeat each speaker count by selected number of utterances

choice_dataset = datasets_idx.map(
pair_speakers, num_parallel_calls=AUTOTUNE
)  # [mapping of len(speakers) datasets, each num_speakers*num_utters]
choice_dataset = choice_dataset.flat_map(
    tf.data.Dataset.from_tensor_slices
)  # [single dataset of len(speakers)*num_speakers*num_utters,]

# Deterministicly takes samples from filtered datasets with order specified in choice dataset
dataset = tf.data.Dataset.choose_from_datasets(dataset, choice_dataset).batch(
    self._num_speakers * self._num_utters
)

# Apply transforms to batched dataset spectrograms. Faster transforms than on single examples
dataset.map(
    lambda x, y: (self._transform(x), y),
    num_parallel_calls=tf.data.experimental.AUTOTUNE,
)

dataset = dataset.prefetch(AUTOTUNE)
total_steps = len(self._speaker_list)

return dataset, total_steps
```


### Relevant log output

_No response_",2023-07-11 15:47:21+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.12"
GPU Usage in tensorflow,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Pro 22H2

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.6.0.163

### GPU model and memory

Nvidia Geforce GTX 1050TI

### Current behavior?

Use Tensorflow With Gpu and not cpu...

1.- I have followed tutorial from this page https://www.tensorflow.org/install/gpu?hl=es-419
2.- I Have Installed cuda_11.2.0_460.89_win10.exe from https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal
3.- I Have Downloaded cuDNN from https://developer.nvidia.com/rdp/cudnn-archive , the version 8.6.0 (is the version that documentation says tensorflow supports
4.- Later i installed the cuda, and in the path of cuda in my case is C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2 , and i have unzip the cuDNN file un format .zip and copied the 3 folder in my cuda path
5.- I have executed those commands modifyng the version of cuda with my current version:
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\extras\CUPTI\lib64;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\include;%PATH%
SET PATH=C:\tools\cuda\bin;%PATH%
setx PATH ""%PATH%;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\libnvvp""
6.- I have restarted the pc and checked with CMD Admin Mode the next command nvcc --version
and i have received fine the output:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0

7.- I Have Tried the tutorial from this page too https://www.codingforentrepreneurs.com/blog/install-tensorflow-gpu-windows-cuda-cudnn/ , but i continue without be abble to run tensorflow in gpu mode , i have tried follow the documentation too have tried 4 times, reinstalling and doing again, but didnt worked.
8.- I dont know if i'm doing something bad or i skipped something, if someone can help me i would appreciate it very much..

### Standalone code to reproduce the issue

```shell
# my code is the next
from tensorflow.python.client import device_lib 
print(device_lib.list_local_devices())

import tensorflow as tf

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        print(""GPU:"", gpu)
else:
    print(""No Gpu for Tensorflow"")

if tf.test.is_built_with_cuda():
    print(""Current Tensorflow version support gpu"")
else:
    print(""Current Version dont support gpu"")
```


### Relevant log output

```shell
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 15862100159162906098
xla_global_id: -1
]
No Gpu for Tensorflow
Current Version dont support gpu
```
",2023-07-16 09:06:06+00:00,"stat:awaiting response, type:build/install, stale, subtype:windows, TF 2.13"
 RuntimeError: Quantization to 16x8-bit not yet supported for op: 'FLOOR_MOD'   ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
google colab
- TensorFlow installation (pip package or built from source):
- pip
- TensorFlow library (version, if pip package or github SHA, if built from source):
- 2.12.0

### 2. Code
I have following model.
```python
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0


# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0


# Define a simple sequential model

model_infrence = tf.keras.Sequential([
    MyDense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10)
  ])

model_infrence.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])



# Create a basic model instance


# Display the model's architecture
model_infrence.summary()

# Create a basic model instance


# Display the model's architecture
model_infrence.summary()
```

Then I custom one of the dense layers like so 
```python
# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Contains the Dense layer.""""""


import tensorflow.compat.v2 as tf

from keras import activations
from keras import backend
from keras import constraints
from keras import initializers
from keras import regularizers
from keras.dtensor import utils
from keras.engine.base_layer import Layer
from keras.engine.input_spec import InputSpec

# isort: off
from tensorflow.python.util.tf_export import keras_export


@keras_export(""keras.layers.Dense"")
class MyDense(Layer):
    """"""Just your regular densely-connected NN layer.

    `Dense` implements the operation:
    `output = activation(dot(input, kernel) + bias)`
    where `activation` is the element-wise activation function
    passed as the `activation` argument, `kernel` is a weights matrix
    created by the layer, and `bias` is a bias vector created by the layer
    (only applicable if `use_bias` is `True`). These are all attributes of
    `Dense`.

    Note: If the input to the layer has a rank greater than 2, then `Dense`
    computes the dot product between the `inputs` and the `kernel` along the
    last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).
    For example, if input has dimensions `(batch_size, d0, d1)`, then we create
    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2
    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are
    `batch_size * d0` such sub-tensors).  The output in this case will have
    shape `(batch_size, d0, units)`.

    Besides, layer attributes cannot be modified after the layer has been called
    once (except the `trainable` attribute).
    When a popular kwarg `input_shape` is passed, then keras will create
    an input layer to insert before the current layer. This can be treated
    equivalent to explicitly defining an `InputLayer`.

    Example:

    >>> # Create a `Sequential` model and add a Dense layer as the first layer.
    >>> model = tf.keras.models.Sequential()
    >>> model.add(tf.keras.Input(shape=(16,)))
    >>> model.add(tf.keras.layers.Dense(32, activation='relu'))
    >>> # Now the model will take as input arrays of shape (None, 16)
    >>> # and output arrays of shape (None, 32).
    >>> # Note that after the first layer, you don't need to specify
    >>> # the size of the input anymore:
    >>> model.add(tf.keras.layers.Dense(32))
    >>> model.output_shape
    (None, 32)

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            If you don't specify anything, no activation is applied
            (ie. ""linear"" activation: `a(x) = x`).
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix.
        bias_initializer: Initializer for the bias vector.
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix.
        bias_regularizer: Regularizer function applied to the bias vector.
        activity_regularizer: Regularizer function applied to
            the output of the layer (its ""activation"").
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix.
        bias_constraint: Constraint function applied to the bias vector.

    Input shape:
        N-D tensor with shape: `(batch_size, ..., input_dim)`.
        The most common situation would be
        a 2D input with shape `(batch_size, input_dim)`.

    Output shape:
        N-D tensor with shape: `(batch_size, ..., units)`.
        For instance, for a 2D input with shape `(batch_size, input_dim)`,
        the output would have shape `(batch_size, units)`.
    """"""

    @utils.allow_initializer_layout
    def __init__(
        self,
        units,
        activation=None,
        use_bias=True,
        kernel_initializer=""glorot_uniform"",
        bias_initializer=""zeros"",
        kernel_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        bias_constraint=None,
        **kwargs,
    ):
        super().__init__(activity_regularizer=activity_regularizer, **kwargs)

        self.units = int(units) if not isinstance(units, int) else units
        if self.units < 0:
            raise ValueError(
                ""Received an invalid value for `units`, expected ""
                f""a positive integer. Received: units={units}""
            )
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

        self.input_spec = InputSpec(min_ndim=2)
        self.supports_masking = True

    def build(self, input_shape):
        dtype = tf.as_dtype(self.dtype or backend.floatx())
        if not (dtype.is_floating or dtype.is_complex):
            raise TypeError(
                ""A Dense layer can only be built with a floating-point ""
                f""dtype. Received: dtype={dtype}""
            )

        input_shape = tf.TensorShape(input_shape)
        last_dim = tf.compat.dimension_value(input_shape[-1])
        if last_dim is None:
            raise ValueError(
                ""The last dimension of the inputs to a Dense layer ""
                ""should be defined. Found None. ""
                f""Full input shape received: {input_shape}""
            )
        self.input_spec = InputSpec(min_ndim=2, axes={-1: last_dim})
        self.kernel = self.add_weight(
            ""kernel"",
            shape=[last_dim, self.units],
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            dtype=self.dtype,
            trainable=True,
        )
        if self.use_bias:
            self.bias = self.add_weight(
                ""bias"",
                shape=[
                    self.units,
                ],
                initializer=self.bias_initializer,
                regularizer=self.bias_regularizer,
                constraint=self.bias_constraint,
                dtype=self.dtype,
                trainable=True,
            )
        else:
            self.bias = None
        self.built = True

    def call(self, inputs):
        if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype:
            inputs = tf.cast(inputs, dtype=self._compute_dtype_object)

        is_ragged = isinstance(inputs, tf.RaggedTensor)
        if is_ragged:
            # In case we encounter a RaggedTensor with a fixed last dimension
            # (last dimension not ragged), we can flatten the input and restore
            # the ragged dimensions at the end.
            if tf.compat.dimension_value(inputs.shape[-1]) is None:
                raise ValueError(
                    ""Dense layer only supports RaggedTensors when the ""
                    ""innermost dimension is non-ragged. Received: ""
                    f""inputs.shape={inputs.shape}.""
                )
            original_inputs = inputs
            if inputs.flat_values.shape.rank > 1:
                inputs = inputs.flat_values
            else:
                # Innermost partition is encoded using uniform_row_length.
                # (This is unusual, but we can handle it.)
                if inputs.shape.rank == 2:
                    inputs = inputs.to_tensor()
                    is_ragged = False
                else:
                    for _ in range(original_inputs.ragged_rank - 1):
                        inputs = inputs.values
                    inputs = inputs.to_tensor()
                    original_inputs = tf.RaggedTensor.from_nested_row_splits(
                        inputs, original_inputs.nested_row_splits[:-1]
                    )

        rank = inputs.shape.rank
        if rank == 2 or rank is None:
            # We use embedding_lookup_sparse as a more efficient matmul
            # operation for large sparse input tensors. The op will result in a
            # sparse gradient, as opposed to
            # sparse_ops.sparse_tensor_dense_matmul which results in dense
            # gradients. This can lead to sigfinicant speedups, see b/171762937.
            if isinstance(inputs, tf.SparseTensor):
                # We need to fill empty rows, as the op assumes at least one id
                # per row.
                inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)
                # We need to do some munging of our input to use the embedding
                # lookup as a matrix multiply. We split our input matrix into
                # separate ids and weights tensors. The values of the ids tensor
                # should be the column indices of our input matrix and the
                # values of the weights tensor can continue to the actual matrix
                # weights.  The column arrangement of ids and weights will be
                # summed over and does not matter. See the documentation for
                # sparse_ops.sparse_tensor_dense_matmul a more detailed
                # explanation of the inputs to both ops.
                ids = tf.SparseTensor(
                    indices=inputs.indices,
                    values=inputs.indices[:, 1],
                    dense_shape=inputs.dense_shape,
                )
                weights = inputs
                outputs = tf.nn.embedding_lookup_sparse(
                    self.kernel, ids, weights, combiner=""sum""
                )
            else:
                

                  print(inputs)
                  quotient, x = divmod(inputs, (2**n))
                  #x = inputs % (2**n);
                  quotient1, x1 = divmod(inputs, (2**n - 1))
                  #x1 = inputs % (2**n - 1);
                  quotient2, x2 = divmod(inputs, (2**n + 1))
                  #x2 = inputs % (2**n + 1);
                  # w =  self.w % (2**n);
                  quotient3, w = divmod(self.w, (2**n))
                  # w1 = self.w % (2**n - 1);
                  quotient4, w1 = divmod(self.w, (2**n - 1))
                  # w2 = self.w % (2**n + 1)
                  quotient5, w2 = divmod(self.w, (2**n + 1))
                  quotient6, z = divmod((tf.matmul(x, w) + self.b), (2**n))
                  # z = (tf.matmul(x, w) + self.b) % (2**n)
                  quotient7, z1 = divmod((tf.matmul(x, w) + self.b), (2**n - 1))
                  # z1 = (tf.matmul(x1, w1) + self.b) % (2**n - 1)
                  quotient8, z2 = divmod((tf.matmul(x, w) + self.b), (2**n + 1))
                  # z2 = tf.matmul(x2, w2) + self.b % (2**n + 1)

                  Dm = (2**n) * (2**n - 1) * (2**n + 1);
                  m1 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n));
                  m2 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n - 1));
                  m3 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n + 1));

                  outputs = rns_to_decimal(Dm, z, z1, z2, m1, m2, m3, n)
                  print(outputs)
        # Broadcast kernel to inputs.
        else:
            outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])
            # Reshape the output back to the original ndim of the input.
            if not tf.executing_eagerly():
                shape = inputs.shape.as_list()
                output_shape = shape[:-1] + [self.kernel.shape[-1]]
                outputs.set_shape(output_shape)

        # if self.use_bias:
        #     outputs = tf.nn.bias_add(outputs, self.bias)

        if self.activation is not None:
            outputs = self.activation(outputs)

        if is_ragged:
            outputs = original_inputs.with_flat_values(outputs)

        return outputs

    def compute_output_shape(self, input_shape):
        input_shape = tf.TensorShape(input_shape)
        input_shape = input_shape.with_rank_at_least(2)
        if tf.compat.dimension_value(input_shape[-1]) is None:
            raise ValueError(
                ""The last dimension of the input shape of a Dense layer ""
                ""should be defined. Found None. ""
                f""Received: input_shape={input_shape}""
            )
        return input_shape[:-1].concatenate(self.units)

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                ""units"": self.units,
                ""activation"": activations.serialize(self.activation),
                ""use_bias"": self.use_bias,
                ""kernel_initializer"": initializers.serialize(
                    self.kernel_initializer
                ),
                ""bias_initializer"": initializers.serialize(
                    self.bias_initializer
                ),
                ""kernel_regularizer"": regularizers.serialize(
                    self.kernel_regularizer
                ),
                ""bias_regularizer"": regularizers.serialize(
                    self.bias_regularizer
                ),
                ""activity_regularizer"": regularizers.serialize(
                    self.activity_regularizer
                ),
                ""kernel_constraint"": constraints.serialize(
                    self.kernel_constraint
                ),
                ""bias_constraint"": constraints.serialize(self.bias_constraint),
            }
        )
        return config
    def rns_to_decimal(dm, z1, z2, z3, m1, m2, m3, n = 6):
        M1 = 2**n
        M2 = 2**n - 1
        M3 = 2**n + 1
        x1 = 1
        x2 = 1
        x3 = 1
        quotient, mm1 = divmod(m1, M1)
       # mm1 = m1 % M1
        for i in range(1, M1):
            quotient1, X = divmod((i * mm1), M1)
            if X == 1:
                x1 = i
        quotient2, mm2 = divmod(m2, M2)
       # mm2 = m2 % M2
        for i in range(1, M2):
            quotient3, X1 = divmod((i * mm2), M2)
            if X1 == 1:
                x2 = i
       # mm3 = m3 % M3
        quotient4, mm3 = divmod(m3, M3)
        for i in range(1, M3):
            quotient5, X2 = divmod((i * mm3), M3)
            if X2 == 1:
                x3 = i
        quotient5, num = divmod((z1 * m1 * x1 + z2 * m2 * x2 + z3 * m3 * x3), dm)
       # num = (z1 * m1 * x1 + z2 * m2 * x2 + z3 * m3 * x3) % dm
        return num
```

The only change to original layer is following code:

```python
                  quotient, x = divmod(inputs, (2**n))
                  #x = inputs % (2**n);
                  quotient1, x1 = divmod(inputs, (2**n - 1))
                  #x1 = inputs % (2**n - 1);
                  quotient2, x2 = divmod(inputs, (2**n + 1))
                  #x2 = inputs % (2**n + 1);
                  # w =  self.w % (2**n);
                  quotient3, w = divmod(self.w, (2**n))
                  # w1 = self.w % (2**n - 1);
                  quotient4, w1 = divmod(self.w, (2**n - 1))
                  # w2 = self.w % (2**n + 1)
                  quotient5, w2 = divmod(self.w, (2**n + 1))
                  quotient6, z = divmod((tf.matmul(x, w) + self.b), (2**n))
                  # z = (tf.matmul(x, w) + self.b) % (2**n)
                  quotient7, z1 = divmod((tf.matmul(x, w) + self.b), (2**n - 1))
                  # z1 = (tf.matmul(x1, w1) + self.b) % (2**n - 1)
                  quotient8, z2 = divmod((tf.matmul(x, w) + self.b), (2**n + 1))
                  # z2 = tf.matmul(x2, w2) + self.b % (2**n + 1)

                  Dm = (2**n) * (2**n - 1) * (2**n + 1);
                  m1 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n));
                  m2 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n - 1));
                  m3 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n + 1));

                  outputs = rns_to_decimal(Dm, z, z1, z2, m1, m2, m3, n)
```

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-07-20 15:43:03+00:00,"stat:awaiting response, stale, type:performance, TFLiteConverter, TF 2.12"
Need Help with TensorFlow Lite Model Running on GPU - Output Interpretation Issue (Android Studio Kotlin),"Hello. I have created an Android application in Android Studio that uses a tflite model. Its implementation works without any issues and looks as follows:

val model = Ssd.newInstance(context)

// Creates inputs for reference.
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.UINT8)
inputFeature0.loadBuffer(byteBuffer)

// Runs model inference and gets result.
val outputs = model.process(inputFeature0)
val outputFeature0 = outputs.outputFeature0AsTensorBuffer
val outputFeature1 = outputs.outputFeature1AsTensorBuffer
val outputFeature2 = outputs.outputFeature2AsTensorBuffer
val outputFeature3 = outputs.outputFeature3AsTensorBuffer

// Releases model resources if no longer used.
model.close()

However, the application is running slowly, and I would like to perform the model computations on the GPU.

I am facing an issue with the input and output parts.

I couldn't find any information about it anywhere. The current code looks like this:

val options = Interpreter.Options().apply {
    if(compatList.isDelegateSupportedOnThisDevice) {
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        this.setNumThreads(4)
    }
}
interpreter = Interpreter(loadModelFile(assets,""Ssd.tflite""), options)
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

Then, I should create the input.buffer for the main line:

interpreter.run(inputFeature0.buffer, outputs.buffer)

I tried doing some adjustments, but the outputs.buffer I got as a result was something I couldn't interpret. Has anyone encountered a similar problem? If so, please, I would appreciate your help.

",2023-07-22 11:56:03+00:00,"stat:awaiting response, type:support, stale, comp:lite, TFLiteGpuDelegate, Android"
converter issue ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-07-24 07:36:16+00:00,"stat:awaiting response, stale, TFLiteConverter"
Performance drop with tensorflow 2.13,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Debian GNU/Linux 12 (bookworm)

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I notice a big performance drop between tensorflow 2.12 (CPU) and tensorflow 2.13 (CPU). With the last release (and also with tf-nightly '2.14.0-dev20230724') it takes *4 times* longer to perform a simple sum. It is between keras inputs though some I am not sure if this is directly related to tensorflow or if it comes from keras.

See code below for a very simple example. Note that this is with tensorflow CPU only.
The timings are:
- for tensorflow 2.12.1 + keras 2.12.0: **5.3 s**
- for tensorflow 2.13.0 + keras 2.13.1: **24 s**
- for tensorflow 2.14.0-dev20230724 + keras  2.14.0.dev2023072407: 26 s

### Standalone code to reproduce the issue

```shell
import time

from tensorflow.keras import Input


number_of_executions = 3000

x = Input((1,))
y = Input((1,))

start = time.time()
for i in range(number_of_executions):
    x + y
duration = time.time() - start
print(f""Duration: {duration:.1f}"")
```


### Relevant log output

_No response_",2023-07-24 08:47:46+00:00,"stat:awaiting response, stale, comp:keras, type:performance, TF 2.13"
Converter issue,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-08-04 04:59:40+00:00,"stat:awaiting response, stale, TFLiteConverter"
tf.math.floormod performance issue,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

20.04.1-Ubuntu

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 11.6

### GPU model and memory

Tesla V100

### Current behavior?

Dear Tensorflow support, 

I found that the tf.math.floormod function is very slow for the tensors of float dtype, even if the size of the tensor is very small, in my case, only 6. The platform is a powerful server with GPU acceleration, but the performance is much slower than an old laptop.

Output on server, 20.04.1-Ubuntu, GPU accelerated, TensorFlow version 2.13.0

> tf.math.floormod float cost: 1.0610415935516357 seconds
> tf.math.floormod int cost: 0.005458354949951172 seconds

Output on laptop, Windows 10, cpu-only, TensorFlow version 2.10.0 :

> tf.math.floormod float cost: 0.002992391586303711 seconds
> tf.math.floormod int cost: 0.006021261215209961 seconds

Could you please check this issue? 
Thanks!



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import time
PI = 3.141592653589793

random_tensor = tf.random.uniform(shape=(1, 6), minval=-PI*2, maxval=PI*2, dtype=tf.float32)
start_time = time.time()
random_tensor = tf.math.floormod(random_tensor+PI, 2*PI)-PI
print(f""tf.math.floormod float cost: {time.time() - start_time} seconds"")

random_tensor = tf.random.uniform(shape=(1, 6), minval=-200, maxval=200, dtype=tf.int32)
start_time = time.time()
random_tensor = tf.math.floormod(random_tensor+100, 2*100)-100
print(f""tf.math.floormod int cost: {time.time() - start_time} seconds"")
```


### Relevant log output

_No response_",2023-08-11 07:59:42+00:00,"stat:awaiting response, stale, comp:ops, type:performance, TF 2.13"
"Error in PredictCost() for the op: op: ""CropAndResize"" attr","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have a model that used to run on python 3.7 + tensorflow 2.4.1. When upgrading to tensorflow 2.13.0, we're seeing this error message, which seems to correlate with the degradation of our model performance. Any idea what might be leading to this error and what could be changed? 

It does seem like when I remove `tf.function` decorator, this error message goes away. So, I'm assuming it has something to do with tracing, but not entirely sure why it's erroring out in tf 2.13 and not in 2.4.1...

```
Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_FLOAT shape { dim { size: -569 } dim { size: 128 } dim { size: 224 } dim { size: 2 } } } inputs { dtype: DT_FLOAT shape { dim { size: -16 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -16 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } tensor_content: ""\030\000\000\000\020\000\000\000"" } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""Tesla V100-SXM2-16GB"" frequency: 1530 num_cores: 80 environment { key: ""architecture"" value: ""7.0"" } environment { key: ""cuda"" value: ""11080"" } environment { key: ""cudnn"" value: ""8600"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 6291456 shared_memory_size_per_multiprocessor: 98304 memory_size: 11614814208 bandwidth: 898048000 } outputs { dtype: DT_FLOAT shape { dim { size: -16 } dim { size: 24 } dim { size: 16 } dim { size: 2 } } }
```

### Standalone code to reproduce the issue

```shell
I don't have a standalone code to reproduce the issue, but would just like some feedback on what might be causing the above issue...
```


### Relevant log output

_No response_",2023-08-21 16:50:23+00:00,"stat:awaiting response, type:others, comp:ops, TF 2.13"
How to limit GPU memory usage when only prediction in c++?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.4

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.0 / 8.1.0

### GPU model and memory

RTX 3090 / 24GB

### Current behavior?

I loaded the saved model using the already compiled tensorflow-gpu 2.4.0.
When this model was used for prediction, it was confirmed that all available memory of the gpu was used.
I've seen limiting using the growing method in python, but I don't know how to use it in c++. could you please tell me how?



### Standalone code to reproduce the issue

```shell
#include <stdlib.h>
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <vector>
void NoOpDeallocator(void* data, size_t a, void* b) {}

int main() {
    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    const char* saved_model_dir = ""H:\\my_model\\""; // Path of the model
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\n"");
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }
}
```


### Relevant log output

_No response_",2023-08-28 07:20:19+00:00,"stat:awaiting response, type:feature, comp:runtime"
incompatible shapes error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to train a model to do binary classification.  I am getting an error about incompatible shapes, but my data seems to be in the correct shape. 









































































































































































































































































































































































### Standalone code to reproduce the issue

```shell
!pip install transformers

import tensorflow as tf
import tensorflow_hub as hub
from transformers import LongformerTokenizer, TFLongformerForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import LearningRateScheduler
import math
import numpy as np

positive_sentences = [
    ""The weather is absolutely gorgeous today."",
    ""I'm so grateful for the support of my friends and family."",
    ""I achieved my personal best in the race!"",
    ""The new cafe in town serves amazing coffee."",
    ""I love spending time with my adorable pets."",
    ""I received a surprise gift from a dear friend."",
    ""The sunrise this morning was breathtaking."",
    ""I'm excited about the upcoming vacation."",
    ""The concert last night was incredibly entertaining."",
    ""I'm proud of my hard work paying off."",
    ""The park is a peaceful place to relax."",
    ""I found a great book that I can't put down."",
    ""The team's collaboration led to a successful project."",
    ""I'm enjoying learning a new skill."",
    ""Spending time with loved ones always brightens my day."",
    ""I got a promotion at work, and it's a fantastic feeling."",
    ""The movie I watched last night was heartwarming."",
    ""I'm making positive changes in my daily routine."",
    ""The delicious aroma of home-cooked food fills the air."",
    ""I'm surrounded by inspiring and supportive people.""
]

negative_sentences = [
    ""The constant rain is making me feel gloomy."",
    ""I'm disappointed that my plans got canceled."",
    ""The traffic was horrendous this morning."",
    ""I made a mistake on the important presentation."",
    ""I'm feeling overwhelmed with work and tasks."",
    ""The internet connection is frustratingly slow."",
    ""The food I ordered was cold and tasteless."",
    ""I'm exhausted after a long and stressful day."",
    ""My phone battery died at the worst time."",
    ""The store was out of stock of the item I needed."",
    ""I lost my wallet and it's been a hassle."",
    ""The loud construction noise is giving me a headache."",
    ""I'm struggling to meet my deadlines."",
    ""The movie I was looking forward to was disappointing."",
    ""I'm not feeling well and it's affecting my mood."",
    ""My computer crashed and I lost my unsaved work."",
    ""The rude customer service ruined my experience."",
    ""I'm frustrated with the constant delays."",
    ""The rainy weather is putting me in a bad mood."",
    ""I'm stressed about the upcoming exams.""
]

sentences = positive_sentences + negative_sentences

labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
    ,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
]

df = pd.DataFrame({'sentences':sentences,'labels':labels})

data = df.copy()

display(data.head(5))

tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
longformer_model = TFLongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096')

X_train, X_test, y_train, y_test = train_test_split(df['sentences'],df['labels'], stratify=df['labels'])
X_train.head(4)
display(y_train)

x_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=""tf"")
x_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=""tf"")

y_train_encoded = np.array(y_train)
y_test_encoded = np.array(y_test)

display(x_train_tokens)

input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
outputs = longformer_model(input_ids, attention_mask=attention_mask)[0]
output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name=""output"")(outputs)

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[output_layer])

def lr_schedule(epoch):
    initial_lr = 0.001  # Set your initial learning rate here
    drop = 0.75
    epochs_drop = 5  # Adjust this value based on your preference
    lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return lr

lr_scheduler = LearningRateScheduler(lr_schedule)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    [x_train_tokens['input_ids'], x_train_tokens['attention_mask']]
    , y_train
    , epochs=5
    , batch_size=5
    , validation_data=([x_test_tokens['input_ids'], x_test_tokens['attention_mask']], y_test)
    , callbacks=[lr_scheduler]
)
```


### Relevant log output

```shell
Epoch 1/5

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-12-24f08ed0005f> in <cell line: 3>()
      1 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
      2 
----> 3 history = model.fit(
      4     [x_train_tokens['input_ids'], x_train_tokens['attention_mask']]
      5     , y_train

1 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:

Detected at node 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-24f08ed0005f>"", line 3, in <cell line: 3>
      history = model.fit(
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1'
Detected at node 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-24f08ed0005f>"", line 3, in <cell line: 3>
      history = model.fit(
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1'
2 root error(s) found.
  (0) INVALID_ARGUMENT:  Incompatible shapes: [5,512,12,514] vs. [5,512,12,513]
	 [[{{node gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1}}]]
	 [[model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._6/attention/self/cond_2/pivot_t/_676/_1027]]
  (1) INVALID_ARGUMENT:  Incompatible shapes: [5,512,12,514] vs. [5,512,12,513]
	 [[{{node gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_131688]
```
",2023-08-29 17:14:09+00:00,"stat:awaiting response, type:bug, stale, comp:model, TF 2.13"
Memory leak using SavedModel format,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12 / 2.13

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Simply, I did

- Build the model (`model`)
- Save it using (a). `model.save(my_model)` , (b). `model.save_weights(my_model.h5)`.

Next, 


## Case 1

1. Using `model.load_weight(my_model.h5)`, and pass `jit_compile=True` followed by `model.fit`. It works, though 11GB consumed out of 16GB. 
2. Using `tf.keras.load_model(``my_model`), and pass `jit_compile=True`, followed by `model.fit`. It doesn't work, simply extremly slow and consumed 15.8GB instantly. Sudden jump from 3GB to 15.8GB. But without passing `jit_compile`, it runs normally but memory consumption remains same. 

## Case 2

1. If I initialize model within `with strategy.scope():`, and try to save the weight (by `model.save_weight`), it gives `OSError: Unable to create link (name already exists)` but without strategy scope, this error doesn't occur.

### Standalone code to reproduce the issue

```shell
to do.
```


### Relevant log output

_No response_",2023-08-30 05:51:06+00:00,"stat:awaiting tensorflower, type:bug, comp:keras, TF 2.13"
import tensorflow as tf delegate = tf.lite.experimental.load_delegate('/content/drive/MyDrive/test_delegate/libtensorflowlite_gpu_delegate.so')#with this we ca get faster predictions OSError: libEGL.so: cannot open shared object file: No such file or directory,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2023-09-05 08:39:49+00:00,"type:build/install, comp:lite, TFLiteConverter, TFLiteGpuDelegate"
Memory leak when using tf.Model and tf.Model.fit() in a loop. clear_session() does not help,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes (tf-nightly = ""2.15.0.dev20230904"")

### Source

source

### TensorFlow version

2.13, 2.12, 2.11

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.1 LTS (GNU/Linux 5.16.10 x86_64)

### Mobile device

_No response_

### Python version

3.10.0, 3.9.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN 8600

### GPU model and memory

NVIDIA RTX A5000, 24GB

### Current behavior?

Memory usage steadily increases when using tf.keras.Model and tf.keras.Model.fit() in a loop, and leads to Out Of Memory exception saturating the memory eventually. clear_session() does not help. The same code with TF version == 2.9.2 has an almost constant memory usage instead, and works as expected.
I've also opened [this issue](https://github.com/keras-team/tf-keras/issues/286) on Keras's GitHub, months ago, with no solutions from the Keras team.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf # same issue with tf-nightly = ""2.15.0.dev20230904""
import time
import gc
import psutil # psutil == ""5.9.5""
import subprocess as sp

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(
        device=gpu, enable=True
    )


def get_cpu_memory():
    memory_info = psutil.virtual_memory()
    # you can have the percentage of used RAM
    memory_percent = 100.0 - memory_info.percent
    memory_free_values = memory_info.available / (1024 * 1024)  # in MB
    # you can calculate percentage of available memory
    return memory_free_values, memory_percent


def get_gpu_memory():
    command = ""nvidia-smi --query-gpu=memory.free --format=csv""
    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\n')[:-1][1:]
    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)][0]
    memory_percent = (memory_free_values / 24564) * 100  # my gpu has 24564 MB of memory
    return memory_free_values, memory_percent


class MyModel(tf.keras.Model):

    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(1000, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(10000, activation=tf.nn.relu)
        self.dense3 = tf.keras.layers.Dense(10000, activation=tf.nn.relu)
        self.dense4 = tf.keras.layers.Dense(1000, activation=tf.nn.softmax)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        x = self.dense4(x)
        return x


if __name__ == '__main__':
    print(f""Starting.."")
    memory_free_val_initial, memory_perc_initial = get_cpu_memory()
    print(f""[Memory monitoring] Free memory CPU {memory_free_val_initial} MB, {memory_perc_initial} %."")
    memory_free_val_initial_gpu, memory_perc_initial_gpu = get_gpu_memory()
    print(f""[Memory monitoring] Free memory GPU {memory_free_val_initial_gpu} MB, {memory_perc_initial_gpu} %."")

    for r in range(0, 1000):
        model = MyModel()
        # ds = tf.data.Dataset.from_tensor_slices((tf.random.uniform((64*4, 1000)), tf.ones((64*4))))
        ds = (lambda: tf.data.Dataset.from_tensor_slices((tf.random.uniform((64 * 20, 1000)), tf.ones((64 * 20)))))
        model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy())

        model.fit(ds().batch(64), verbose=0)
        model.evaluate(ds().batch(64), verbose=0)
        tf.keras.backend.clear_session()

        if r % 5 == 0:
            # print every 5 model.fit
            print(f""Round: {r}"")
            memory_free_val, memory_perc = get_cpu_memory()
            print(f""[Memory monitoring] Free memory CPU {memory_free_val} MB, {memory_perc} %."")
            memory_free_val_gpu, memory_perc_gpu = get_gpu_memory()
            print(f""[Memory monitoring] Free memory GPU {memory_free_val_gpu} MB, {memory_perc_gpu} %."")
            if r == 0:
                memory_free_first = memory_free_val
                memory_free_first_gpu = memory_free_val_gpu
            # time.sleep(2)

        del model
        gc.collect()
        del ds

    print(f""[Memory monitoring CPU] Memory usage increased by {memory_free_first - memory_free_val} MB, ""
          ""during the process."")
    print(f""[Memory monitoring GPU] Memory usage increased by {memory_free_first_gpu - memory_free_val_gpu} MB, ""
          ""during the process."")
```


### Relevant log output

```shell
Round: 0
[Memory monitoring] Free memory CPU 56633.48046875 MB, 88.5 %.
[Memory monitoring] Free memory GPU 21180 MB, 86.2237420615535 %.


Round: 995
[Memory monitoring] Free memory CPU 49866.3046875 MB, 77.9 %.
[Memory monitoring] Free memory GPU 21156 MB, 86.12603810454324 %.

[Memory monitoring CPU] Memory usage increased by 6767.17578125 MB, during the process.
[Memory monitoring GPU] Memory usage increased by 24 MB, during the process.
```
",2023-09-05 08:54:21+00:00,"stat:awaiting response, type:bug, stale, comp:keras, TF 2.13"
DenseFeatures Feature column combine order,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have this code to create DenseFeatures layer.

 ```
       # Define the input for each feature column.
        inputs = {}
        for col in feature_columns:
            if type(col) == type(tf.feature_column.numeric_column(""temp"")):
                dtype = tf.float32
                key = col.key
            else:
                dtype = tf.int64
                key = col.categorical_column.key
            inputs[key] = tf.keras.layers.Input(name=key, shape=(), dtype=dtype)

        // Now use a DenseFeatures layer to combine them
        x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)
```

-----------------

Now, I am trying to implement the output of the DenseFeatures layer for inference. I am facing a situation where I cannot use the TensorFlow model due to latency constraints.

However, my issue is that the manner in which DenseFeatures combines the inputs does not follow the order specified in feature_columns, nor is it sorted based on the feature names.

Is there a way I can determine the order in which these feature columns are combined within the DenseFeatures layer?

As I understand, in most places it is mentioned that it should follow the same order as the feature columns, but this is not what I am observing. I am using TensorFlow 2.13.

### Standalone code to reproduce the issue

```shell
# Define the input for each feature column.
        inputs = {}
        for col in feature_columns:
            if type(col) == type(tf.feature_column.numeric_column(""temp"")):
                dtype = tf.float32
                key = col.key
            else:
                dtype = tf.int64
                key = col.categorical_column.key
            inputs[key] = tf.keras.layers.Input(name=key, shape=(), dtype=dtype)

        // Now use a DenseFeatures layer to combine them
        x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)
```


### Relevant log output

_No response_",2023-09-19 21:00:19+00:00,"stat:awaiting response, type:support, stale, comp:apis, TF 2.13"
can't convert keras model to tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0 or tf-nightly

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```
import tensorflow as tf

keras_model = tf.keras.models.load_model(keras_model_filename)
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()

file = open(tflite_model_filename, 'wb‘）
file.write(tflite_model)

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://github.com/tensorflow/tensorflow/assets/68658008/dc88001c-02c2-42c6-8c74-8de447c5a776)
",2023-09-28 04:11:06+00:00,"comp:lite, TFLiteConverter, TF2.14"
TFlite memory allocation thrashing with FlexDelegates,"### 1. System information

- OS Platform and Distribution macOS, iOS 16.6.1 iPhone 13 Pro:
- TensorFlow installation (pip package or built from source): 2.13.0
- TensorFlow library (version, if pip package or github SHA, if built from source): tflite built from 2.13.0 tag

### 2. Code

I have a model that converts correctly and flags the following flex delegates:

```python
  tf.AddV2(tensor<1x33xcomplex<f32>>, tensor<complex<f32>>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.AddV2(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.Complex(tensor<1x33xf32>, tensor<1x33xf32>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.Complex(tensor<f32>, tensor<f32>) -> (tensor<complex<f32>>) : {device = """"}
  tf.ConcatV2(tensor<1x1x1xcomplex<f32>>, tensor<1x1x64xcomplex<f32>>, tensor<i32>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
  tf.GatherV2(tensor<1x1x33xcomplex<f32>>, tensor<i32>, tensor<i32>) -> (tensor<1x33xcomplex<f32>>) : {batch_dims = 0 : i64}
  tf.Pow(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.RealDiv(tensor<1x33xcomplex<f32>>, tensor<1x33xcomplex<f32>>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.SelectV2(tensor<1x1x1xi1>, tensor<1x1x33xcomplex<f32>>, tensor<1x1x33xcomplex<f32>>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
  tf.StridedSlice(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<1x1x1xcomplex<f32>>) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
  tf.StridedSlice(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<1x1x64xcomplex<f32>>) : {begin_mask = 3 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
  tf.Sub(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.Transpose(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
```

### 3. Failure after conversion
When running this model on iOS, and profiling the model, I can see the application spending a lot of time (30%) in `posix_memalign` that are emitted from the flex library which leads to poor performance of the model executing (highlighted in following image):

![image](https://github.com/tensorflow/tensorflow/assets/192171/f4950106-1b7f-47cc-8025-d3fc9818e643)

Is there a way to:
- figure out which flex delegate is causing this? (If I compile the flex library with symbols, I can't seem to turn on optimization which results in an unusable library for me)
- modify the memory allocation/de-allocation strategy to re-ruse existing memory? I suspect this issue is a side-effect of tflite <-> tensorflow interop but that is only a suspicion
- set something like max allowed persistent memory to prevent de-allocation? I also have a suspicion that it might not be tied to a single delegate but by the allocator itself",2023-09-29 10:49:50+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TFLiteConverter, TF 2.13"
"TFRecordWriter stuck (or very slow) while serializing data, depending on feature transformation type","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

ubuntu 22.4

### Mobile device

_No response_

### Python version

3.8.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?


**tf.io.TFRecordWriter** freeze when preprocessing features with **scikit-learn** (**Dask-ML**) **QuantileTranformer**, whereas its working (writing out few k samples within seconds) when using **StandardScaler**.

How might QuantileTransformer produce output data shaped in a way it breaks TFRecords serialization? Might dtype precision influence serialization performance in such criticality?

### Standalone code to reproduce the issue

```shell
(reduced pseudo code)

# fit scalers
#### NOTE Option 1: using this scaler breaks TF records writer!
feat_standardizer = dask_QuantileTransformer(output_distribution=standardizer_distribution, 
                                                n_quantiles=n_feat_standardizer_quantils, 
                                                subsample=n_fit_samples, copy=False)


# NOTE Option 2: Using this one works for TF records writer
feat_standardizer = dask_StandardScaler(copy=False)


## from here proceed the same way until TF data serialization as TFRecord files

x_train_future = dask_client.scatter(x_train_arr) 
feat_standardizer = dask_client.submit(feat_standardizer.fit, x_train_future).result()
x_train_preproc_future = dask_client.submit(feat_standardizer.transform, x_train_future)
x_train_dask_arr = dask_client.gather(x_train_preproc_future)

....

# Transform training data (in chuncks of subsets of the total training dataset)
X_train_future = dask_client.scatter(x_train_arr_transform_batch)
X_train_future = dask_client.submit(feat_standardizer.transform, X_train_future)
X_train_future = dask_client.submit(feat_normalizer.transform, X_train_future)
x_train_arr_transform_batch  = dask_client.gather(X_train_future)
    
# reshape features: (sample * time, feat) -> (sample, time, feat)
X_train = x_train_arr_transform_batch.reshape((n_train_samples, n_steps, n_feat))

# cast numpy default precision float64 -> TF float32                               
X_train = X_train.astype(np.float32)
y_train = y_train.astype(np.int64)

def array_to_tfrecords(X, y):
        feature_dict = {
            'X': tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten())),
            'y': tf.train.Feature(int64_list=tf.train.Int64List(value=y.flatten()))
        }
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict)) 
        return example.SerializeToString()

tf.io.TFRecordWriter(tfrecords_file_path, options=tf.io.TFRecordOptions(compression_type='ZLIB', compression_level=7)) as writer:
        for x, y in tqdm(zip(X_train, y_train)):         # <---- no progress visible here when using QuantileTransformer!
            serialized = array_to_tfrecords(x, y)
            writer.write(serialized)
```


### Relevant log output

```shell
No output visible. The script just never continues and freeze while pretending to serialize data
```
",2023-10-11 11:29:35+00:00,type:bug
[XLA] using auto-clustering with tf_xla_auto_jit causes multiple compilation with variable seq. len.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a somewhat complex model that deals with variable length tensors as input. The feature dimension remains the same, only one of the dimension changes (time). If I fix this by giving always the same batch of features, I can use tf_xla_auto_jit and see that the training speed improves quite a lot. However, as soon as I throw the for `batch in train_dataset` in the mix, everything becomes infinitely slow and, looking at the warning, it feels like XLA keeps recompiling ... 

I don't really know what to expect here? 

### Standalone code to reproduce the issue

```shell
Impossible to share unfortunately.
```


### Relevant log output

_No response_",2023-10-23 15:22:16+00:00,"stat:awaiting response, type:bug, stale, comp:xla, TF 2.13"
tf.random.normal() causes RAM usage to keep growing,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS Venture 13.1 (22C65)

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**Issue** 
I'm getting what looks like a memory leak from running tf.random.normal as below, in eager mode. 
I have not encountered this issue with others versions, e.g. 2.12.1

**Context** 
I'm running RL algorithms using a TF-addon [NoisyDense](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/NoisyDense) layer, which exposes a function that does the below random sampling. Since it's being executed millions of times, it causes exploding RAM.

**Notes**
Wrapping the random sampling inside a function decorated with tf.function seems to avoid the issue.

### Standalone code to reproduce the issue

```shell
import os, psutil
process = psutil.Process()
print(process.memory_info().rss / 1000000)  # Ram usage in MB

import tensorflow as tf
print(tf.__version__)

for _ in range(10):
    print(process.memory_info().rss / 1000000)
    for _ in range(5000):
        x = tf.random.normal(shape=(10000, 1))  # Causing growing RAM
```


### Relevant log output

```shell
382.963712
2.13.0
382.963712
385.220608
387.21536
388.927488
390.926336
392.392704
394.338304
396.374016
398.323712
400.273408
```
",2023-10-24 04:06:40+00:00,"stat:awaiting response, type:bug, stale, comp:ops, TF 2.13"
Tensorflow weird GPU memory usage universal-sentence-encoder,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.14.0-rc1-21-g4dacf3f368e 2.14.0

### Custom code

Yes

### OS platform and distribution

ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Hi I am new to TensorFlow, GPU,Models everything :)

I am using the below code to generate text embeddings

```
texts = [ ""This is the first sentence."",""Another sentence for embedding."",""Embeddings are useful for NLP.""]
import tensorflow as tf
import numpy as np

# Path to the saved USE model directory
use_model_path = ""universal-sentence-encoder""
# Configure GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
        
use_model = tf.saved_model.load(use_model_path)

embeddings_list = []
embeddings = use_model(texts)
embeddings_list = embeddings.numpy().tolist()
#clear session
tf.keras.backend.clear_session()


print(embeddings_list)
```
If I execute this in a for loop with a list of 512kb texts, GPU memory increases exponentially. Even after adding tf.keras.backend.clear_session() in every iteration, I see the same behavior.

I am expecting it to release memory on each iteration.
 I know we can do this in batches but I am planning to us this as a REST API.
how can I optimize this?

### Standalone code to reproduce the issue

```shell
texts = [ ""This is the first sentence."",""Another sentence for embedding."",""Embeddings are useful for NLP.""]
import tensorflow as tf
import numpy as np

# Path to the saved USE model directory
use_model_path = ""universal-sentence-encoder""
# Configure GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
        
use_model = tf.saved_model.load(use_model_path)

embeddings_list = []
embeddings = use_model(texts)
embeddings_list = embeddings.numpy().tolist()
#clear session
tf.keras.backend.clear_session()


print(embeddings_list)
```


### Relevant log output

_No response_",2023-11-10 16:19:25+00:00,"stat:awaiting response, stale, comp:gpu, type:performance, TF2.14"
Least squares is 50% slower when activating JIT compile,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Activating JIT compile makes tf.linalg.lstsq up to 100% slower. 

If we benchmark `tf.linalg.lstsq` on a matrix of shape (2**18, 5), we see considerable slowdown when activating JIT compile. Expectation would be that JIT actually speeds up solving the least squares problem, but seeing the opposite across multiple platforms and setups. JIT version is consistently slower, taking up to twice as much depending on the platform.

### Standalone code to reproduce the issue


See here:

https://colab.research.google.com/drive/1MIBRIYik2ZVpy1eIIalokMQxE2GA2G9D#scrollTo=7YrOyhUaZD24

or equivalently:

```shell
import tensorflow as tf
@tf.function(jit_compile=False)
def lstsq_no_jit(X,y):
  return tf.linalg.lstsq(X,y)

@tf.function(jit_compile=True)
def lstsq_jit(X,y):
  return tf.linalg.lstsq(X,y)

m, n, k = (2**18, 5, 1)
shape_X = (m, n)
shape_y = (m, k)

X = tf.random.normal(shape_X)
y = tf.random.normal(shape_y)

# Run once to avoid compilation overhead.
lstsq_no_jit(X,y)
lstsq_jit(X,y)
```
At this point, just time
```shell
lstsq_no_jit(X,y)
```
vs 
```shell
lstsq_jit(X,y)
```


### Relevant log output
In the colab version, see the following:
* Running lstsq with no JIT: 6.01 ms ± 401 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
* Running lstsq with JIT: 11.4 ms ± 3.45 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)

",2023-11-12 17:01:11+00:00,"stat:awaiting response, stale, comp:xla, type:performance, TF2.14"
Tensorflow terribly slow on Mac Studio M1 Ultra - problem with tensorflow-metal,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

Mac OS 13.5.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a Mac Studio M1 Ultra and I am trying to train a simple RNN to forecast time series.
When I run the code, tensorflow takes ~140s per epoch or 670ms/step.

If I uninstall tensorflow-metal, it takes only 20ms/step. But in this way I won't be able to use the GPU.

Why is this happening?

### Standalone code to reproduce the issue

```shell
import numpy as np
import keras 
import tensorflow as tf

def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise
    return series[..., np.newaxis].astype(np.float32)

np.random.seed(42)
n_steps = 50
series = generate_time_series(10000, n_steps + 1)

X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]

# Implementing simple RRN
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([keras.layers.SimpleRNN(1, input_shape=[None, 1])])

optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.005)
model.compile(loss=""mse"", optimizer=optimizer)
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid))
```


### Relevant log output

```shell
Epoch 1/20

2023-11-14 20:58:05.842329: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Ultra
2023-11-14 20:58:05.842356: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB
2023-11-14 20:58:05.842361: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB
2023-11-14 20:58:05.842396: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2023-11-14 20:58:05.842410: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2023-11-14 20:58:06.172278: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.

219/219 [==============================] - 148s 676ms/step - loss: 0.4310 - val_loss: 0.2155
Epoch 2/20
219/219 [==============================] - 143s 651ms/step - loss: 0.1627 - val_loss: 0.1514
Epoch 3/20
219/219 [==============================] - 142s 649ms/step - loss: 0.1462 - val_loss: 0.1488
Epoch 4/20
219/219 [==============================] - 141s 644ms/step - loss: 0.1474 - val_loss: 0.1475
Epoch 5/20
219/219 [==============================] - 142s 649ms/step - loss: 0.1477 - val_loss: 0.1508
Epoch 6/20
219/219 [==============================] - 142s 649ms/step - loss: 0.1006 - val_loss: 0.0617
...
```
",2023-11-14 19:58:41+00:00,"stat:awaiting response, type:build/install, stale, subtype:macOS, type:performance, TF 2.13"
tf.keras.layers.MaxPooling3D can not still work for bfloat 16 in tensorflow 2.12.0: No OpKernel was registered to support Op ‘MaxPool3D’ ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

mac ventura 13.6.1

### Mobile device

_No response_

### Python version

3.10.

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

No GPU machine

### Current behavior?

I’m a researcher in the filed of neuroscience using MRI techniques.

I have the same question raised by Dr. Roger ( [Tensorflow question (MaxPool3d, MaxPool3D, MaxPooling3D)!) 1](https://discuss.tensorflow.org/t/tensorflow-question-maxpool3d-maxpool3d-maxpooling3d/11291)). I have the same trouble "" No OpKernel was registered to support Op ‘MaxPool3D’"".

In my understanding ([ModelCheckpoint callback fails when mixed precision is enabled in TF 2.11.0 · Issue #349 · keras-team/tf-keras · GitHub](https://github.com/keras-team/tf-keras/issues/349)), bfloat 16 did not work for bfloat 16 in the tf.keras.layers.MaxPooling3D one year ago. But the keras team fixed this issue in the verion of  TF 2.12.0 or further newer verions. Is that correct? Please tell me if my understanding is correct or not. If this issue is fixed, any other problems??

### Standalone code to reproduce the issue

```shell
tf.keras.layers.MaxPooling3D in the version of tensorflow 2.12.0 did not compute bfloat 16 data.
```


### Relevant log output

```shell
The code history as follows:

input the folliwing:
python3 /Users/username/Downloads/SHIVA_PVS/predict_one_file.py -i /Users/username/mri/young_healthy/MRI_resliced.nii -m /Users/username/Downloads/SHIVA_PVS/PVS/v1/T1.PVS/20211030-162753_Unet3Dv2-10.7.2-1.8-T1.VRS_fold_1x6_pi_fold_0_model.h5 -b /Users/username/mri/young_healthy/brain_reslice.nii -o /Users/username/mri/young_healthy/pv_MRI_resliced --verbose --gpu -1

terminal returns:
2023-11-16 07:55:41.891278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Trying to run inference on GPU 0
WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING
The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.
If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once
INFO : Predicting fold : 20211030-162753_Unet3Dv2-10.7.2-1.8-T1.VRS_fold_1x6_pi_fold_0_model
Traceback (most recent call last):
File “/Users/username/Downloads/SHIVA_PVS/predict_one_file.py”, line 125, in
prediction = model.predict(
File “/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py”, line 70, in error_handler
raise e.with_traceback(filtered_tb) from None
File “/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py”, line 53, in quick_execute
tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op ‘MaxPool3D’ used by {{node model/Enc_Max_D7/MaxPool3D}} with these attrs: [T=DT_HALF, data_format=“NDHWC”, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding=“VALID”]
Registered devices: [CPU]
Registered kernels:
device=‘XLA_CPU_JIT’; T in [DT_FLOAT, DT_BFLOAT16, DT_HALF]
device=‘CPU’; T in [DT_FLOAT]
device=‘CPU’; T in [DT_BFLOAT16]
```
",2023-12-09 07:18:54+00:00,"stat:awaiting response, type:bug, stale, comp:keras, TF 2.12"
Golang  TensorFlow2.5.0  have a memory leak on reloading model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.5.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am a user of tfgo, and I encountered a bug in tfgo. When the model is reloaded, there is a memory leak, But when I see tfgo source code, I find this bug is from tf, Because I find TF C.TF_DeleteSession is not work

### Standalone code to reproduce the issue

```shell
I find tf lib C.TF_DeleteSession(s.c, status.c) is not work
```


### Relevant log output

_No response_",2023-12-20 06:58:08+00:00,"stat:awaiting response, type:bug, stale, comp:apis, TF 2.5"
Tensorflow Java 2.10.1 memory leak issue on constant creation,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.1

### Custom code

No

### OS platform and distribution

debian aarch64

### Mobile device

Raspberry pi

### Python version

_No response_

### Bazel version

5.1.1

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current behavior?

I've built tensorflow java for raspberry and aarch64 architecture with following command on Ubuntu 22.04
```
bazel build --config=elinux_aarch64 \
--copt=-std=gnu11 \
--copt=-O2 \
--config=monolithic \
--define tensorflow_mkldnn_contraction_kernel=0 \
--verbose_failures \
//tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
```
When the constant is created in a code, it seems it doesn't free a memory properly. The memory consumption reaches 8Gb in 8 seconds.
I've identified that the problem is in this invocation which is invoked in Constant class
https://github.com/tensorflow/tensorflow/blob/v2.10.1/tensorflow/java/src/main/java/org/tensorflow/op/core/Constant.java#L642-L647
However I stuck here. Unfortunately, I can't build the most recent tensorflow 2.15.0  because it fails with some strange error.
Could you please help me to identify what I'm doing wrong?

### Standalone code to reproduce the issue

```shell
public static void main(String[] args) {
        Ops ops = Ops.create();
        System.out.println(""Ops created"");
        int capacity = 480 * 640 * 3;
        ByteBuffer byteBuffer = ByteBuffer.allocateDirect(capacity);
        for (int i = 0; i < capacity; i++) {
            byteBuffer.put((byte) 200);
        }
        byteBuffer.rewind();
        while (!Thread.interrupted()) {
            byteBuffer.rewind();
            long[] imageShape = new long[]{480, 640, 3};
            Constant<UInt8> constant = ops.constant(UInt8.class, imageShape, byteBuffer);
            System.out.println(constant);
        }
    }
```


### Relevant log output

_No response_",2023-12-23 22:00:44+00:00,"stat:awaiting tensorflower, type:support, comp:core, TF 2.10"
Failure in building MLIR dialects and utilities from source,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04.3 LTS
- TensorFlow installation (pip package or built from source): build from source
- TensorFlow library (version, if pip package or github SHA, if built from source):  commit id 2454fa808a6

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
$ bazel build  -j tensorflow/compiler/mlir/...
...
ERROR: /home/.../tensorflow/tensorflow/core/tfrt/graph_executor/BUILD:27:11: Compiling tensorflow/core/tfrt/graph_executor/graph_execution_options.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/core/tfrt/graph_executor:graph_execution_options) /usr/lib/llvm-14/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 ... (remaining 241 arguments skipped)
In file included from tensorflow/core/tfrt/graph_executor/graph_execution_options.cc:19:
external/com_google_absl/absl/log/log.h:199:9: warning: 'LOG' macro redefined [-Wmacro-redefined]
#define LOG(severity) ABSL_LOG_INTERNAL_LOG_IMPL(_##severity)
        ^
external/local_tsl/tsl/platform/default/logging.h:165:9: note: previous definition is here
#define LOG(severity) _TF_LOG_##severity
        ^
In file included from tensorflow/core/tfrt/graph_executor/graph_execution_options.cc:19:
external/com_google_absl/absl/log/log.h:237:9: warning: 'LOG_EVERY_N' macro redefined [-Wmacro-redefined]
#define LOG_EVERY_N(severity, n) \
        ^
external/local_tsl/tsl/platform/default/logging.h:278:9: note: previous definition is here
#define LOG_EVERY_N(severity, n)                       \
        ^
In file included from tensorflow/core/tfrt/graph_executor/graph_execution_options.cc:19:
external/com_google_absl/absl/log/log.h:245:9: warning: 'LOG_FIRST_N' macro redefined [-Wmacro-redefined]
#define LOG_FIRST_N(severity, n) \
        ^
external/local_tsl/tsl/platform/default/logging.h:284:9: note: previous definition is here
#define LOG_FIRST_N(severity, n)                       \
        ^
In file included from tensorflow/core/tfrt/graph_executor/graph_execution_options.cc:19:
external/com_google_absl/absl/log/log.h:253:9: warning: 'LOG_EVERY_POW_2' macro redefined [-Wmacro-redefined]
#define LOG_EVERY_POW_2(severity) \
        ^
external/local_tsl/tsl/platform/default/logging.h:290:9: note: previous definition is here
#define LOG_EVERY_POW_2(severity)                         \
        ^
In file included from tensorflow/core/tfrt/graph_executor/graph_execution_options.cc:19:
external/com_google_absl/absl/log/log.h:265:9: warning: 'LOG_EVERY_N_SEC' macro redefined [-Wmacro-redefined]
#define LOG_EVERY_N_SEC(severity, n_seconds) \
        ^
external/local_tsl/tsl/platform/default/logging.h:300:9: note: previous definition is here
#define LOG_EVERY_N_SEC(severity, n_seconds)                      \
        ^
tensorflow/core/tfrt/graph_executor/graph_execution_options.cc:131:16: error: no matching function for call to 'StrCat'
            << absl::StrCat(options.model_metadata)
               ^~~~~~~~~~~~
external/com_google_absl/absl/strings/str_cat.h:451:41: note: candidate function not viable: no known conversion from 'const tensorflow::SessionMetadata' to 'const absl::AlphaNum' for 1st argument
ABSL_MUST_USE_RESULT inline std::string StrCat(const AlphaNum& a) {
                                        ^
external/com_google_absl/absl/strings/str_cat.h:449:41: note: candidate function not viable: requires 0 arguments, but 1 was provided
ABSL_MUST_USE_RESULT inline std::string StrCat() { return std::string(); }
                                        ^
external/com_google_absl/absl/strings/str_cat.h:455:34: note: candidate function not viable: requires 2 arguments, but 1 was provided
ABSL_MUST_USE_RESULT std::string StrCat(const AlphaNum& a, const AlphaNum& b);
                                 ^
external/com_google_absl/absl/strings/str_cat.h:456:34: note: candidate function not viable: requires 3 arguments, but 1 was provided
ABSL_MUST_USE_RESULT std::string StrCat(const AlphaNum& a, const AlphaNum& b,
                                 ^
external/com_google_absl/absl/strings/str_cat.h:458:34: note: candidate function not viable: requires 4 arguments, but 1 was provided
ABSL_MUST_USE_RESULT std::string StrCat(const AlphaNum& a, const AlphaNum& b,
                                 ^
external/com_google_absl/absl/strings/str_cat.h:463:41: note: candidate function template not viable: requires at least 5 arguments, but 1 was provided
ABSL_MUST_USE_RESULT inline std::string StrCat(
                                        ^
5 warnings and 1 error generated.
INFO: Elapsed time: 27.129s, Critical Path: 24.43s
INFO: 1098 processes: 701 internal, 397 local.
FAILED: Build did NOT complete successfully
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2024-01-03 18:13:22+00:00,"stat:awaiting response, type:build/install, stale, comp:lite, TFLiteConverter"
[TensorFlow Lite Delegates] iOS run laggy on first inference,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

'TensorFlowLiteSwift', '~> 0.0.1-nightly'

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

iPhone 15 Pro MAX

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The first inference should have nearly the same latency at other inferences.

### Standalone code to reproduce the issue

```shell
https://github.com/isl-org/MiDaS/tree/master/mobile/ios

delegates = [MetalDelegate()]
interpreter = try Interpreter(modelPath: modelPath, options: options, delegates: delegates)
try interpreter.allocateTensors()
inputTensor = try interpreter.input(at: 0)
outputTensor = try interpreter.output(at: 0)

do {
  try interpreter.copy(data, toInputAt: 0)
  try interpreter.invoke()

  // Get the output `Tensor` to process the inference results.
  outputTensor = try interpreter.output(at: 0)
} catch let error {
  os_log(
    ""Failed to invoke the interpreter with error: %s"", type: .error,
    error.localizedDescription)
  return
}
```


### Relevant log output

```shell
inference index,CPU,GPU,ANE,
1,83.08,123.78,20.00,
2,66.86,35.96,28.76,
3,67.70,19.67,24.16,
4,78.89,19.11,24.29,
5,69.42,19.15,24.85,
6,69.58,19.35,24.16,
7,68.47,19.29,26.29,
8,69.42,19.86,24.36,
9,69.24,19.49,24.91,
10,73.53,20.40,24.03,
11,70.91,19.29,24.62,
12,70.52,18.94,24.71,
13,71.66,19.35,24.30,
14,70.40,19.15,24.09,
15,71.73,19.37,23.74,
16,72.06,20.41,24.79,
17,70.98,19.28,23.81,
18,71.97,19.52,25.02,
19,72.19,20.14,23.95,
20,71.55,19.28,24.05,
21,73.17,19.00,24.26,
22,73.02,19.47,24.35,
23,71.86,20.24,23.90,
24,73.81,20.58,24.00,
25,72.03,19.51,24.55,
26,72.37,20.75,24.30,
27,73.91,19.23,23.61,
28,72.73,19.16,23.46,
29,72.86,19.36,24.23,
30,73.44,19.52,24.02,
31,72.76,20.09,24.99,
32,73.49,19.51,24.24,
33,76.49,20.70,23.92,
34,73.78,19.53,23.98,
35,73.45,19.01,23.92,
36,74.01,19.53,24.03,
37,74.03,20.48,23.95,
38,73.22,19.78,24.13,
39,74.21,20.78,23.45,
40,74.64,19.48,24.41,
41,73.77,19.36,23.42,
42,75.36,19.85,23.76,
43,75.76,20.30,23.87,
44,74.97,19.84,24.27,
45,74.44,19.83,23.75,
46,76.28,20.96,24.07,
47,76.44,19.36,23.70,
48,75.56,19.42,24.21,
49,74.88,20.82,24.74,
50,76.20,20.10,24.32,
51,76.29,19.62,23.08,
52,75.45,20.93,23.53,
53,75.03,19.33,23.05,
54,76.22,19.32,23.25,
55,76.57,19.69,23.67,
56,76.05,20.14,24.51,
57,75.14,19.58,23.07,
58,76.09,20.45,23.58,
59,76.01,19.37,23.57,
60,76.39,19.26,24.53,
61,76.18,19.21,23.38,
62,75.79,19.60,23.24,
63,76.43,19.75,23.20,
64,77.32,19.63,23.84,
65,77.05,19.45,22.99,
66,76.45,19.90,23.32,
67,76.25,20.06,23.38,
68,76.99,20.11,23.55,
69,77.57,20.86,23.28,
70,77.42,19.61,23.37,
71,76.84,19.92,24.12,
72,76.49,21.05,23.25,
73,76.61,20.25,23.84,
74,78.08,20.95,23.44,
75,77.66,19.91,23.47,
76,76.42,19.52,23.76,
77,77.00,19.92,23.38,
78,77.56,19.62,23.69,
79,78.40,20.31,23.65,
80,78.56,19.67,23.30,
81,77.59,19.21,24.01,
82,77.83,20.21,24.67,
83,77.07,19.80,21.67,
84,77.55,20.08,28.61,
85,77.73,19.68,31.18,
86,78.05,19.10,31.83,
87,78.54,19.70,30.31,
88,77.76,20.37,29.35,
89,77.24,19.58,27.23,
90,77.72,19.92,28.56,
91,77.66,19.38,24.71,
92,78.45,20.20,24.32,
93,79.30,20.01,25.32,
94,78.52,19.99,24.10,
95,76.68,19.76,23.12,
96,77.93,19.42,22.87,
97,77.74,20.54,22.79,
98,77.39,19.82,22.46,
99,78.75,20.26,21.99,
100,79.28,19.93,22.60,
101,78.99,19.94,20.58,
102,79.06,20.11,21.44,
103,78.65,20.09,21.25,
104,78.57,19.77,21.29,
105,77.44,19.84,20.99
```
",2024-01-05 02:13:02+00:00,"stat:awaiting response, comp:lite, type:performance"
Having Problems While Doing Digit Recognition in OpenCV Using Tensorflow Mnist,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

MacOS Sonoma 14.2.1

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M2 Max

### Current behavior?

Currently I'm learning OpenCV and decided to make a project to practice my skills. In my project, I'm aiming to read a sudoku table, divide it into individual cells, and recognize the numbers. The rest is mostly sudoku solving.

Right now, I'm stuck on recognizing the numbers part. I decided to use mnist dataset to train the model. For a reason that I couldn't figure out, the program always recognizes the digit wrong. 

[First form of my Image](https://i.stack.imgur.com/9bHhp.png)
[My image after rescaling](https://i.stack.imgur.com/qOb0C.png)

I thought that the problem was with the way that I trained the model. Because I transformed my huge image to 28x28, it blurred the image a lot. I thought that that was the problem. The code that I added below trains the model in 300x300 form. It's definitely slower but I thought that It's more safe. Unfortunately, the result didn't change. 

The code below also outputs the probability for every class. I input an image (which I gave the link to) with the number 6, but it doesn't work. and the model guesses it as 8. I don't know to source of the problem. I hope that you can help me.

### Standalone code to reproduce the issue

```shell
sample = splitted_blocks[0][0][6]
size = 300
(ds_train, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)


def resize_img(image, label):
    return tf.image.resize(image, (size, size)), label


ds_train = ds_train.map(resize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.map(resize_img, num_parallel_calls=tf.data.AUTOTUNE)


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255., label


ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(size, size, 1)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
model.compile(
    optimizer=tf.keras.optimizers.legacy.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

model.fit(
    ds_train,
    epochs=6,
    validation_data=ds_test,
)
sample = cv.cvtColor(sample, cv.COLOR_BGR2GRAY)
sample = cv.resize(sample, (size, size))

view_image(sample)
sample = np.invert(np.array([sample]))

prediction = model.predict(sample)
print(np.argmax(prediction))

for class_index, prob in enumerate(prediction[0]):
    print(f'Class {class_index}: Probability {prob}')

sample = np.invert(np.array([sample])).reshape((size, size, 1))
cv.imshow(""test"", sample)
cv.waitKey(0)
```


### Relevant log output

```shell
Epoch 1/6
469/469 [==============================] - 20s 36ms/step - loss: 0.3859 - sparse_categorical_accuracy: 0.9090 - val_loss: 0.1917 - val_sparse_categorical_accuracy: 0.9432
Epoch 2/6
469/469 [==============================] - 17s 37ms/step - loss: 0.1866 - sparse_categorical_accuracy: 0.9472 - val_loss: 0.1665 - val_sparse_categorical_accuracy: 0.9530
Epoch 3/6
469/469 [==============================] - 17s 37ms/step - loss: 0.1414 - sparse_categorical_accuracy: 0.9585 - val_loss: 0.1636 - val_sparse_categorical_accuracy: 0.9548
Epoch 4/6
469/469 [==============================] - 17s 37ms/step - loss: 0.1215 - sparse_categorical_accuracy: 0.9642 - val_loss: 0.1735 - val_sparse_categorical_accuracy: 0.9563
Epoch 5/6
469/469 [==============================] - 17s 37ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9676 - val_loss: 0.1444 - val_sparse_categorical_accuracy: 0.9615
Epoch 6/6
469/469 [==============================] - 17s 37ms/step - loss: 0.1040 - sparse_categorical_accuracy: 0.9688 - val_loss: 0.1410 - val_sparse_categorical_accuracy: 0.9630
1/1 [==============================] - 0s 59ms/step
8
Class 0: Probability 144.3349151611328
Class 1: Probability 2138.992431640625
Class 2: Probability 4751.95068359375
Class 3: Probability 4140.23583984375
Class 4: Probability -2975.758544921875
Class 5: Probability 2814.78662109375
Class 6: Probability 1327.113525390625
Class 7: Probability 2143.239990234375
Class 8: Probability 4821.7548828125
Class 9: Probability -3011.857666015625
```
",2024-01-16 00:56:16+00:00,"stat:awaiting response, type:support, stale, comp:keras, TF 2.15"
"Running quantized tflite model in ""full"" tensorflow","I quantized a full tensorflow model and got a smaller tflite file

This runs well using tf.lite.Interpreter. However, on my machine, tflite can only use CPU, while ""full"" tensorflow is much faster as it can use GPU as well 
Because of CPU-only execution, the smaller quantized model runs slower in tf.lite.Interpreter than the larger/original model on ""full"" tensorflow

So I would like to know if there's any way to run the quantized tflite model (which is saved as a .tflite file on disk) in ""full"" tensorflow (via load_model for example) so that GPU acceleration can be used

thanks",2024-01-19 10:28:45+00:00,"stat:awaiting response, type:support, stale, comp:lite"
"When convert from keras to tflite, the output is different from what it shoud be.","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

https://github.com/Z-yq/TensorflowASR/blob/v2/asr/models/chunk_conformer_blocks.py this file CTCdecoder is not correct,but converted tflite of conformerblock work well, and I convert it using https://github.com/TensorSpeech/TensorFlowASR . And I have control input shape when load weight. It complex me a long time. maybe because its dynamic train?
### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.


",2024-01-23 08:00:58+00:00,"stat:awaiting response, stale, comp:lite, TFLiteConverter"
Significant difference in RSS memory usage between TF1 and TF2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.13.1

### Custom code

Yes

### OS platform and distribution

Redhat Enterprise Linux 8.9

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

5.4.0

### GCC/compiler version

10.4

### CUDA/cuDNN version

CUDA 12.2, cuDNN 8.9.5

### GPU model and memory

A100 80GB

### Current behavior?

When running the same keras workload on TF1 vs. TF2, I'm seeing a significant increase in memory utilization per epoch. This happens when using both CPUs/GPUs. The utilization for an epoch climbs up significantly after every epoch. See below for TF1 vs. TF2:

```
# For TF2

Memory usage after epoch 0 [mem_usage =  3.41 GB] 
Memory usage after epoch 1 [mem_usage =  3.88 GB] 
Memory usage after epoch 2 [mem_usage =  3.88 GB] 
Memory usage after epoch 3 [mem_usage =  4.32 GB] 
Memory usage after epoch 4 [mem_usage =  4.81 GB] 
Memory usage after epoch 5 [mem_usage =  5.26 GB] 
Memory usage after epoch 6 [mem_usage =  5.70 GB] 
Memory usage after epoch 7 [mem_usage =  6.14 GB] 
Memory usage after epoch 8 [mem_usage =  6.70 GB] 
Memory usage after epoch 9 [mem_usage =  7.15 GB] 
Memory usage after epoch 10 [mem_usage =  7.36 GB]
Memory usage after epoch 11 [mem_usage =  7.36 GB]
Memory usage after epoch 12 [mem_usage =  7.36 GB]
Memory usage after epoch 13 [mem_usage =  7.37 GB]
Memory usage after epoch 14 [mem_usage =  7.37 GB]
Memory usage after epoch 15 [mem_usage =  7.37 GB]
Memory usage after epoch 16 [mem_usage =  7.37 GB]
Memory usage after epoch 17 [mem_usage =  7.59 GB]
Memory usage after epoch 18 [mem_usage =  7.81 GB]
Memory usage after epoch 19 [mem_usage =  7.81 GB]

# For TF1

Memory usage after epoch 0 [mem_usage =  5.13 GB] 
Memory usage after epoch 1 [mem_usage =  5.14 GB] 
Memory usage after epoch 2 [mem_usage =  5.14 GB] 
Memory usage after epoch 3 [mem_usage =  5.15 GB] 
Memory usage after epoch 4 [mem_usage =  5.15 GB] 
Memory usage after epoch 5 [mem_usage =  5.15 GB] 
Memory usage after epoch 6 [mem_usage =  5.15 GB] 
Memory usage after epoch 7 [mem_usage =  5.15 GB] 
Memory usage after epoch 8 [mem_usage =  5.15 GB] 
Memory usage after epoch 9 [mem_usage =  5.15 GB] 
Memory usage after epoch 10 [mem_usage =  5.15 GB]
Memory usage after epoch 11 [mem_usage =  5.15 GB]
Memory usage after epoch 12 [mem_usage =  5.15 GB]
Memory usage after epoch 13 [mem_usage =  5.15 GB]
Memory usage after epoch 14 [mem_usage =  5.15 GB]
Memory usage after epoch 15 [mem_usage =  5.15 GB]
Memory usage after epoch 16 [mem_usage =  5.15 GB]
Memory usage after epoch 17 [mem_usage =  5.15 GB]
Memory usage after epoch 18 [mem_usage =  5.15 GB]
Memory usage after epoch 19 [mem_usage =  5.15 GB]
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import psutil
import time
import os

def mem_usage_str():
    process = psutil.Process(os.getpid())
    gb =  process.memory_info().rss / (1024.**3)
    return ' [mem_usage = {:5.2f} GB]'.format(gb)

if int(tf.__version__.split('.')[0]) < 2:
    """"""
    Patch to fix TF/numpy1.20 compatibility issue
    """"""
    from   tensorflow.math          import reduce_prod
    from   tensorflow.python.ops    import array_ops

    def _constant_if_small(value, shape, dtype, name):
        try:
            if reduce_prod(shape) < 1000:  # monkey patch
                return array_ops.constant(value, shape=shape, dtype=dtype,
                                          name=name)
        except TypeError:
            # Happens when shape is a Tensor, list with Tensor elements, etc.
            pass
        return None

    array_ops._constant_if_small = _constant_if_small
    """"""
    End of patch
    """"""

def build_model():
    inputs = [tf.keras.layers.Input(shape=(300, 6), name='input_layer')]
    current_layer = inputs[0]

    current_layer = tf.keras.layers.LSTM(
        50,
        dropout=0.1,
        recurrent_dropout=0.1,
        return_sequences=False,
        name='lstm',
    )(current_layer)

    current_layer = tf.keras.layers.Dense(1)(current_layer)
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    model = tf.keras.models.Model(inputs=inputs, outputs=current_layer)
    model.compile(loss='mse', optimizer=optimizer)

    return model

def run(model, X, y, n_epochs):
    tot_time = 0.

    print(f'Memory usage before training' + mem_usage_str())
    for i in range(n_epochs):
        start = time.time()
        model.fit(X, y, epochs=1, batch_size=4096, verbose=0)
        tot_time += time.time() - start
        print(f'Memory usage after epoch {i}' + mem_usage_str())

    print(f'Avg. time = {tot_time / n_epochs} seconds')

def run_example(p, n_epochs):
    import numpy as np

    model = build_model()
    X = np.random.randn(2 ** p, 300, 6)
    y = np.random.randn(2 ** p)

    run(model, X, y, n_epochs)

def main():
    run_example(
        16, # 2 ** 16 samples
        20, # 10 epochs
    )


# ------------------------------------------------------------------------------

if __name__ == ""__main__"":
    main()
```


### Relevant log output

_No response_",2024-01-24 10:19:10+00:00,"stat:awaiting response, type:bug, stale, type:performance, TF 2.13"
"Method: ""Eigen::SpatialConvolutionBackwardKernel"" in ""tensorflow/core/kernels/eigen_backward_spatial_convolutions.h"" is painfully slow !","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.13.1

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

---

### Python version

---

### Bazel version

---

### GCC/compiler version

Clang 16

### CUDA/cuDNN version

---

### GPU model and memory

---

### Current behavior?

I've ""borrowed"" this method in order to use it in own C++ NN project. It is consistently 3x slower than a simple naive nested for loop implementation on moderately sized inputs: eg. convolution with filter(64,64,5,5), stride 1 and no inflation. This implementation takes 10 seconds on 64x800x800 input and 64x800x800 backward output, meanwhile naive loop takes only 3 seconds, so there is a huge difference !
In reference, all the other spatial convolutions (Eigen::SpatialConvolution and Eigen::SpatialConvolutionBackwardInput) easily beat naive implementations by more than 30%. 

### Standalone code to reproduce the issue
Here is a very broad implementation of algorithm that beats your implementation (dimensions, stride and inflation are handled externally), but that is besides the point.
I also know that your implementation has to work with a large range of inputs, but surely there has to be a way to make it faster, at least for most commonly used kernels in image processing. The current contraction method does seem very inefficient.
```shell


using Tensor = Eigen::Tensor<float, 3, Eigen::ColMajor, int>;
using Tenarr = Eigen::Tensor<float, 4, Eigen::ColMajor, int>;
void Conv2D_WGrad(Tenarr& ker, const Tensor& in, const Tensor& out, int pad_w, int pad_h) {
	alignas(64) float tmp[4 * 4096] = {};
	int ker_mem = ker.dimension(0) * sizeof(float);
	for(int i = 0; i < ker.dimension(2); i++) {     
		for(int j = 0; j < ker.dimension(1); j++) {
			memset(tmp, 0, ker_mem);
			int in_i = i - pad_h;
			int in_j = j - pad_w;
			int clip_k = std::max(0, in_i - in.dimension(2) + out.dimension(2));
			int clip_l = std::max(0, in_j - in.dimension(1) + out.dimension(1));
			for(int k = std::max(-in_i, 0); k < out.dimension(2) - clip_k; k++) {
				for(int l = std::max(-in_j, 0); l < out.dimension(1) - clip_l; l++) {
					for(int ich = 0; ich < in.dimension(0); ich++) {
						for(int och = 0; och < out.dimension(0); och++) {
							tmp[och + ich * out.dimension(0)] += in(ich, in_j + l, in_i + k) * out(och, l, k);
						}
					}
				}
			}
			memcpy(&ker(0, 0, j, i), tmp, ker_mem);
		}
	}
}
```
Thank you for your consideration.
```


### Relevant log output

_No response_",2024-02-07 22:39:28+00:00,"stat:awaiting response, type:performance, comp:core, TF 2.15"
Machine learning from Edx,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info/logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2024-02-08 20:15:03+00:00,"stat:awaiting response, stale, TFLiteConverter"
"discuss.tensorflow.org site produces ""Slow down, too many requests from this IP address.""","Not really sure where to post this issue since I can't get to discuss.tensorflow.org.  Over the past day I've been receiving the subject issue  after restarting my browser and system.  Was in a conversation over a topic but now can't get back to it.  

I'd appreciate if someone would provide some guidance as to whether this is an issue on my side or the server.  Tried different systems and browsers.  Thanks!",2024-02-27 17:21:26+00:00,"stat:awaiting response, type:others"
AIML.MHW,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2024-02-28 15:24:29+00:00,"stat:awaiting response, comp:lite, invalid, TFLiteConverter"
TFC,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2024-03-01 07:10:07+00:00,"stat:awaiting response, invalid, TFLiteConverter"
Tensorflow memory leak during inference,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.13.0-17-gf841394b1b7 2.13.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?



I'm running the following code and noticing a never-ending increase in RAM usage. Eventually, the script terminates with an out-of-memory error. I can't understand what the issue is. I also tried using tf.keras.backend.clear_session() once every 10,000 iterations, but it didn't help. I monitor the specific RAM usage of the PID script. Tensorflow ver is 2.13.1. I would appreciate any insights.


### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf
import numpy as np
import cv2
import time

main_script_pid = os.getpid()
print(""PID of the main script's process:"", main_script_pid)

model_path = '.../Models/model_Ch_0_trt'

dummy_frame = np.random.randint(0, 255, size=(128,128, 3), dtype=np.uint8)

img = cv2.cvtColor(dummy_frame, cv2.COLOR_BGR2GRAY)
img = np.expand_dims(img, axis=0)
img = np.expand_dims(img, axis=-1)
img = img / 255.0  # Normalize pixel values to [0, 1]

trt_saved_model = tf.saved_model.load(model_path)
inference_function = trt_saved_model.signatures[""serving_default""]
input_tensor_name = list(inference_function.structured_input_signature[1].keys())[0]
output_tensor_name = list(inference_function.structured_outputs.keys())[0]

while True:

    predictions = inference_function(**{input_tensor_name: tf.constant(img, dtype=tf.float32)})[output_tensor_name].numpy()
```


### Relevant log output

_No response_",2024-03-04 09:22:05+00:00,"stat:awaiting response, type:bug, stale, type:performance, TF 2.13"
Enormous memory usage after batched forward passes with TensorFlow 2.16.1,"The following minimal example reproduces the effect. (No GPU, just CPU.)

```python3
import numpy as np
import psutil
import tensorflow as tf

model = tf.keras.applications.ResNet152V2()

images = np.zeros([20, 224, 224, 3], dtype=np.uint8)

for run in range(10):
    memory_usage_in_MiB = psutil.Process().memory_info().rss / (1024 * 1024)
    print(f""Memory usage after {run} run(s) (in MiB): {memory_usage_in_MiB:.3f}"", flush=True)
    model(images)
```

```
Memory usage after 0 run(s) (in MiB): 790.715
Memory usage after 1 run(s) (in MiB): 5969.941
Memory usage after 2 run(s) (in MiB): 7112.559
Memory usage after 3 run(s) (in MiB): 7787.625
Memory usage after 4 run(s) (in MiB): 7827.727
Memory usage after 5 run(s) (in MiB): 6785.316
Memory usage after 6 run(s) (in MiB): 5971.680
Memory usage after 7 run(s) (in MiB): 6615.105
Memory usage after 8 run(s) (in MiB): 7151.496
Memory usage after 9 run(s) (in MiB): 6679.250
```

([Dockerfile](https://gist.github.com/Dobiasd/4e6747f3c057b939cb2672d142f612fc) to reproduce)

And it's not just `tf.keras.applications.ResNet152V2()`. It also happens (for example) with `tf.keras.applications.inception_resnet_v2.InceptionResNetV2` and `tf.keras.applications.inception_v3.InceptionV3`. And it also happens when using Python `3.12.2` instead of `3.11.8`.

With TensorFlow `2.15.1` (instead of `2.16.1`), however, the memory usage does not explode:

```
Memory usage after 0 run(s) (in MiB): 1038.891
Memory usage after 1 run(s) (in MiB): 1154.062
Memory usage after 2 run(s) (in MiB): 1154.508
Memory usage after 3 run(s) (in MiB): 1154.684
Memory usage after 4 run(s) (in MiB): 1215.840
Memory usage after 5 run(s) (in MiB): 1154.664
Memory usage after 6 run(s) (in MiB): 1154.816
Memory usage after 7 run(s) (in MiB): 1155.059
Memory usage after 8 run(s) (in MiB): 1154.715
Memory usage after 9 run(s) (in MiB): 1155.367
```

([Dockerfile](https://gist.github.com/Dobiasd/0acf434d521cabf7fe3102b551bf36b0) to reproduce)",2024-03-14 09:00:19+00:00,"stat:awaiting tensorflower, comp:runtime, type:performance, TF 2.16"
Too slow while fetching @llvm-raw repos while building tensorflow from Source,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

ubuntu 22.04.1 on aarch64 

### Mobile device

_No response_

### Python version

python 3.10.12

### Bazel version

7.1.1

### GCC/compiler version

clang 17.0.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It is taking too much time to fetch repo llvm-raw while building tensorflow from source on aarch64 machine with 32 cores. I have tried using all possible number of cores, from low to high. using  ex: --local_cpu_resources=4. The process gets upto this point
`Loading: 
    Fetching repository @llvm-raw; starting 964s
    Fetching ...c40c12e4c7/external/llvm-raw; Extracting 8697bbe2d4aed109520e83c6beab52196ec5b702.tar.gz 962s
`
and continues like this.
Please help me to solve this issue.
Commands used :
`export ONEDNN_VERBOSE=1`
`export TF_ENABLE_ONEDNN_OPTS=1`
`export CC=""/usr/local/bin/clang""`
`export CXX=""/usr/local/bin/clang++""`
`export TF_PYTHON_VERSION=3.10` 
` bazel build -s --config=mkl_aarch64 --features=-layering_check --copt=-O3 --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256  --local_cpu_resources=16 //tensorflow/tools/pip_package:build_pip_package --verbose_failures `

### Standalone code to reproduce the issue

```shell
Commands used :
`export ONEDNN_VERBOSE=1`
`export TF_ENABLE_ONEDNN_OPTS=1`
`export CC=""/usr/local/bin/clang""`
`export CXX=""/usr/local/bin/clang++""`
`export TF_PYTHON_VERSION=3.10` 
` bazel build -s --config=mkl_aarch64 --features=-layering_check --copt=-O3 --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256  --local_cpu_resources=16 //tensorflow/tools/pip_package:build_pip_package --verbose_failures `
```


### Relevant log output

```shell
`Loading: 
    Fetching repository @llvm-raw; starting 964s
    Fetching ...c40c12e4c7/external/llvm-raw; Extracting 8697bbe2d4aed109520e83c6beab52196ec5b702.tar.gz 962s`
```
",2024-04-02 06:27:58+00:00,"stat:awaiting response, type:build/install, stale, subtype: ubuntu/linux, TF 2.16"
 ValueError: Invalid dtype: str768,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0-dev20240407

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

None

### GCC/compiler version

None

### CUDA/cuDNN version

_No response_

### GPU model and memory

None

### Current behavior?

Tensorflow doesn't support arrays where the dtype is numpy.str_.

pred_text = ""how are you doing today?""
pred_text = np.array([[pred_text]])

It returns ValueError: Invalid dtype: str768 when I use it to train/predict with my model:
model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary())+1,
        output_dim=64,
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])


model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

The problem is that the data is stored in a 2D array numpy.array([[]]), so converting each data to str or int slows down the process.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1ymbplqo6wK1ZUVn-vP8jnBMsh6dccgZP#scrollTo=DhMNlE-AOzH9
```


### Relevant log output

_No response_",2024-04-08 15:28:07+00:00,"stat:awaiting response, type:feature, comp:apis"
Memory leak when jit compiling,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a function that two functions that are jit compiled. I call them in my training loop function setp (which is not jit compiled, simply in graph mode). With these two functions jit compiled I get a registers spilled warning as follows:

`2024-04-24 12:58:52.550355: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function '__cuda_sm20_div_rn_f64_full', 8 bytes spill stores, 8 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__cuda_sm20_div_rn_f64_full', 8 bytes spill stores, 8 bytes spill loads`

This makes the training much slower.
Without jit compiling those two functions I don't get this warning.

My problem is similar to the one in [this forum thread](https://discuss.tensorflow.org/t/xla-jit-compile-causing-memory-leak/22657).  In this thread some user redirects to this open issue [`#56423`](https://github.com/tensorflow/tensorflow/issues/56423). However this one relates this problem to the distribution strategy. In my case I am just using one GPU. In this issue someone, mentioned that if they performed the optimizer step outside of the jit function it worked. In my case, as I said above, my training step function is not jit compiled and so the optimizer step is already not inside a jit compiled function.



### Standalone code to reproduce the issue

```shell
Unfortunately, I cannot build a MWE at the moment.
```


### Relevant log output

_No response_",2024-04-25 09:50:59+00:00,"stat:awaiting response, type:bug, stale, comp:ops, TF 2.16"
Tensorflow lite GPU delegate can't work well and NNAPI can't make the model faster,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow-lite-task-text:0.3.0 tensorflow-lite-gpu-delegate-plugin:0.4.4 tensorflow-lite-gpu:2.13.0 tensorflow-lite:2.14.0 tensorflow-lite-support:0.4.4

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

Vivo x100: cpu: MediaTek Dimensity 9300    GPU:Immortalis-G720

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I used TensorFlow Lite Model Maker to fine-tune BERT to get a text classification model. I want to use the GPU to speed up my model. I tried two methods. One is to use NNAPI and the other is to use GPU delegate. But it turned out that NNAPI didn't make my model faster, it was much slower. Before using GPU delegate for acceleration, I used compatList.isDelegateSupportedOnThisDevice() to detect if there is a supported GPU on the device, and the result is no. But in fact there is.

### Standalone code to reproduce the issue

```shell
fun classifyGpu(text: String){
        val baseOptionsBuilder = BaseOptions.builder()
        lateinit var bertClassifierGpu: BertNLClassifier
        
        baseOptionsBuilder.useNnapi()
   
        val baseOptions = baseOptionsBuilder.build()
        
        val options = BertNLClassifier.BertNLClassifierOptions
                .builder()
                .setBaseOptions(baseOptions)
                .build()

        bertClassifierGpu = BertNLClassifier.createFromFileAndOptions(
                context,
                MOBILEBERT,
                options)

        results = bertClassifierGpu.classify(text)
 
    }
```


### Relevant log output

_No response_",2024-04-30 12:02:37+00:00,"stat:awaiting response, type:bug, type:support, stale, comp:lite, TFLiteGpuDelegate"
XLA related ptxas version error when changing batch size,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100 40GB

### Current behavior?

I have a custom training loop that calls functions that are jit compiled. I got this error message you see below when using a batch size of 512. However, if I change the batch size to 256 for example (or 128), I no longer get this error. This is very weird, because the error about the ptxas version (which from my understanding is related with the CUDA toolkit version) has nothing to do with the batch size. So, I think the batch size of 512 may be causing another error (possibly some memory issue?) and the wrong error is being thrown ...  I am not sure, but let me know what you think.

Thanks and sorry for not being able to provide  MWE.

### Standalone code to reproduce the issue

```shell
Cannot build a MWE unfortunately.
```


### Relevant log output

```shell
2024-04-30 16:37:32.740441: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:580 : INTERNAL: XLA requires ptxas version 11.8 or higher
2024-04-30 16:37:32.740516: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INTERNAL: XLA requires ptxas version 11.8 or higher
	 [[{{node PartitionedCall}}]]
Traceback (most recent call last):
  File ""/home/ids/afreitas/april/cnn_test/train_traj.py"", line 281, in <module>
    loss = training_loop(ic, gt, msteps_sched[j])    
  File ""/home/ids/afreitas/my_tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ids/afreitas/my_tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node PartitionedCall defined at (most recent call last):
  File ""/home/ids/afreitas/april/cnn_test/train_traj.py"", line 281, in <module>

  File ""/home/ids/afreitas/april/cnn_test/train_traj.py"", line 191, in training_loop

  File ""/home/ids/afreitas/april/cnn_test/train_traj.py"", line 192, in training_loop

XLA requires ptxas version 11.8 or higher
	 [[{{node PartitionedCall}}]] [Op:__inference_training_loop_26507]
```
",2024-04-30 17:20:16+00:00,"type:bug, comp:xla, TF 2.16"
2.12.0: memory leak in TFLite's tflite::Interpreter::Invoke(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0 

### Custom code

Yes

### OS platform and distribution

Cross-build from 'Windows:x86_64' to 'Android:armv8'

### Mobile device

Android with Snapdragon 820

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

 CXX compiler identification is Clang 14.0.7

### CUDA/cuDNN version

no

### GPU model and memory

Snapdragon 820 with Adreno 530

### Current behavior?

Running the invoke for a tflite model using the gpu delegate, with opencl backend. 
It goes fast and well, the problem is that exist a memory leak, that is increasing, not sure how to fix it. Not sure if it's an error on the opencl implementation, on the drivers of the adreno gpu or in the delegate implementation.
![image](https://github.com/tensorflow/tensorflow/assets/4903831/6313b041-bda0-4ecc-9926-c9a7a5d35b8c)


### Standalone code to reproduce the issue

```shell
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/delegates/gpu/delegate.h>
#include <tensorflow/lite/c/common.h>


std::unique_ptr<tflite::FlatBufferModel> m_model;
std::unique_ptr<tflite::Interpreter> m_interpreter;

m_model = tflite::FlatBufferModel::BuildFromBuffer(m_modelName, m_bufferSize);

tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder(*m_model.get(), resolver)(&m_interpreter);

auto delegategpu = tflite::Interpreter::TfLiteDelegatePtr(TfLiteGpuDelegateV2Create(&gpu_options), &TfLiteGpuDelegateV2Delete);

m_interpreter->ModifyGraphWithDelegate(std::move(delegategpu))


m_interpreter->AllocateTensors()
for(int i = 0; i< 1000 ;i ++)
{
m_interpreter->Invoke() != TfLiteStatus::kTfLiteOk)
}
```


### Relevant log output

_No response_",2024-04-30 21:44:09+00:00,"type:bug, comp:lite, TFLiteGpuDelegate, TF 2.12"
problem with converted custom model at TensorFlow Lite,"I created a tflite model from yolov5n  [export.py -- weights mymodel.pt -- include tflite] for the learning model I learned from yolov5n. By the way, /home/pi1/Desktop/project3/examples/lite/examples/object_detection/raspberry_pi. It works fine when I run the existing detect.py , but when I use the above model, I get the following error. 

(0507) pi1@raspberrypi:~/Desktop/project3/examples/lite/examples/object_detection/raspberry_pi $ python detect.py --model mymodel.tflite
Traceback (most recent call last):
  File ""detect.py"", line 15, in <module>
    import tensorflow_io as tfio
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/__init__.py"", line 17, in <module>
    from tensorflow_io.python.api import *  # pylint: disable=wildcard-import
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/api/__init__.py"", line 19, in <module>
    from tensorflow_io.python.ops.io_dataset import IODataset
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/__init__.py"", line 24, in <module>
    import tensorflow as tf
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 28, in <module>
    from tensorflow.core.framework import function_pb2
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/core/framework/function_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 42, in <module>
    serialized_options=None, file=DESCRIPTOR),
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/google/protobuf/descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates



When I downgrade protobuf, I get the following error. Please solve the problem.

(0507) pi1@raspberrypi:~/Desktop/project3/examples/lite/examples/object_detection/raspberry_pi $ python detect.py --model mymodel.tflite
/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']
caused by: ['/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN10tensorflow8internal15LogMessageFatalC1EPKci']
  warnings.warn(f""unable to load libtensorflow_io_plugins.so: {e}"")
/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']
caused by: ['/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZN10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFNS_8StatusOrISt10unique_ptrIS1_NS_4core15RefCountDeleterEEEEvEE']
  warnings.warn(f""file system plugins are not loaded: {e}"")
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Traceback (most recent call last):
  File ""detect.py"", line 152, in <module>
    main()
  File ""detect.py"", line 148, in main
    int(args.numThreads), bool(args.enableEdgeTPU))
  File ""detect.py"", line 65, in run
    detector = vision.ObjectDetector.create_from_options(options)
  File ""/home/pi1/miniconda3/envs/0507/lib/python3.7/site-packages/tensorflow_lite_support/python/task/vision/object_detector.py"", line 91, in create_from_options
    options.base_options.to_pb2(), options.detection_options.to_pb2())
RuntimeError: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.
",2024-05-07 10:10:13+00:00,"stat:awaiting response, type:support, stale, comp:lite, TFLiteConverter"
tf.data filter dataset too slow,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Similar to issue #53169, I have observed that the ""filter before batch"" approach is significantly slow. Filtering the dataset alone takes 430ms, whereas the ""batch+map"" method only requires 20ms.
In theory, the computation of filter and map should be similar, but ""filter before batch"" consumes excessive time.
I attempted to filter after batching, but encountered a limitation where the filter predicate must return a scalar boolean value. Unfortunately, it does not support filtering batched elements.

My question is:
Is there a potential optimization for this performance issue? I aim to develop a customized operation that can filter batched elements (accepting [M,] shaped tensors as input and producing [N,] tensors as output). Is there a more efficient approach available?

```
import time

import tensorflow as tf
fast_dataset = tf.data.Dataset.range(10000)


def fast_benchmark(dataset, name, num_epochs=2):
    start_time = time.perf_counter()
    for _ in tf.data.Dataset.range(num_epochs):
        for _ in dataset:
            pass
    tf.print(""Test"", name, ""Execution time(ms):"", 1000 * (time.perf_counter() - start_time))


def increment(x):
    return x+1


def filter_fn(x):
  return tf.math.equal(tf.math.mod(x, 2), 1)


if __name__ == '__main__':
  fast_benchmark(
    fast_dataset
    .map(increment)
    .batch(256)
    ,
    ""map+batch""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .map(increment)
    ,
    ""batch+map""
  )
  fast_benchmark(
    fast_dataset
    .map(increment)
    .batch(256)
    .prefetch(tf.data.AUTOTUNE)
    ,
    ""map+batch+prefetch""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .map(increment)
    .prefetch(tf.data.AUTOTUNE)
    ,
    ""batch+map+prefetch""
  )
  fast_benchmark(
    fast_dataset
    .prefetch(tf.data.AUTOTUNE)
    .batch(256)
    .map(increment)
    ,
    ""prefetch+batch+map""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .prefetch(tf.data.AUTOTUNE)
    .map(increment)
    ,
    ""batch+prefetch+map""
  )
  fast_benchmark(
    fast_dataset
    .filter(filter_fn)
    .batch(256)
    ,
    ""filter+batch""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .filter(filter_fn)
    ,
    ""batch+filter""
  )
```

result:
![image](https://github.com/tensorflow/tensorflow/assets/14976749/0833e2b3-1850-412f-b9a8-92f701b307f9)


### Standalone code to reproduce the issue

```shell
import time

import tensorflow as tf
fast_dataset = tf.data.Dataset.range(10000)


def fast_benchmark(dataset, name, num_epochs=2):
    start_time = time.perf_counter()
    for _ in tf.data.Dataset.range(num_epochs):
        for _ in dataset:
            pass
    tf.print(""Test"", name, ""Execution time(ms):"", 1000 * (time.perf_counter() - start_time))


def increment(x):
    return x+1


def filter_fn(x):
  return tf.math.equal(tf.math.mod(x, 2), 1)


if __name__ == '__main__':
  fast_benchmark(
    fast_dataset
    .map(increment)
    .batch(256)
    ,
    ""map+batch""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .map(increment)
    ,
    ""batch+map""
  )
  fast_benchmark(
    fast_dataset
    .map(increment)
    .batch(256)
    .prefetch(tf.data.AUTOTUNE)
    ,
    ""map+batch+prefetch""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .map(increment)
    .prefetch(tf.data.AUTOTUNE)
    ,
    ""batch+map+prefetch""
  )
  fast_benchmark(
    fast_dataset
    .prefetch(tf.data.AUTOTUNE)
    .batch(256)
    .map(increment)
    ,
    ""prefetch+batch+map""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .prefetch(tf.data.AUTOTUNE)
    .map(increment)
    ,
    ""batch+prefetch+map""
  )
  fast_benchmark(
    fast_dataset
    .filter(filter_fn)
    .batch(256)
    ,
    ""filter+batch""
  )
  fast_benchmark(
    fast_dataset
    .batch(256)
    .filter(filter_fn)
    ,
    ""batch+filter""
  )
```


### Relevant log output

```shell
Test map+batch Execution time(ms): 585.0009880959988
Test batch+map Execution time(ms): 23.16068299114704
Test map+batch+prefetch Execution time(ms): 503.9997957646847
Test batch+map+prefetch Execution time(ms): 19.63987946510315
Test prefetch+batch+map Execution time(ms): 54.23441715538502
Test batch+prefetch+map Execution time(ms): 16.469698399305344
Test filter+batch Execution time(ms): 282.77427703142166
```
",2024-05-10 11:02:11+00:00,"stat:awaiting response, stale, comp:data, type:performance, TF 2.4"
Android Tflite model fails to load on GPU Delegate: CL_OUT_OF_HOST_MEMORY,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

org.tensorflow:tensorflow-lite:2.16.1

### Custom code

Yes

### OS platform and distribution

Android

### Mobile device

Samsung S23

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Currently trying to get a larger model to load on an S23 but I am running into OOM errors. When initializing an Interpreter using a GPUDelegate with factory options grabbed from CompatibilityList.getBestOptionsForThisDevice(), the Interpreter crashes with `Failed to apply delegate: Failed to build program executable - Out of host memoryError: Program not built!`.  This seems to pop up from an OpenCL error that is parsed with: https://github.com/tensorflow/tensorflow/blob/dd5c42638ac3c19c7facffb4c3cdadd2524cc6a5/tensorflow/lite/delegates/gpu/cl/util.cc#L42

My best guess is that this is due to hitting the Dalvik-heap memory limit of 512mb found on my device with Runtime.maxMemory(). I profiled the memory usage and it seems to crash around the 450mb  mark. Does Tflite on android not use native memory to get around this? I seem to recall people getting 1gb+ models running on their devices. I guess this could possibly be a build step that is going over the limits, but once built it would be offloaded to native?

Note: I am using pyjnius to do this which might be causing problems, but I feel like that isn't the cause. 

### Standalone code to reproduce the issue

```shell
Not sure how useful.
```


### Relevant log output

```shell
05-22 16:31:08.869 23806 23859 I python  :  jnius.jnius.JavaException: JVM exception occurred: Internal error: Failed to apply delegate: Failed to build program executable - Out of host memoryError: Program not built!
05-22 16:31:08.869 23806 23859 I python  :  Falling back to OpenGL
05-22 16:31:08.869 23806 23859 I python  :  TfLiteGpuDelegate Init: No shader implementation for transpose
05-22 16:31:08.869 23806 23859 I python  :  TfLiteGpuDelegate Prepare: delegate is not initialized
05-22 16:31:08.869 23806 23859 I python  :  Node number 2612 (TfLiteGpuDelegateV2) failed to prepare.
```
",2024-05-22 21:10:26+00:00,"stat:awaiting response, type:bug, stale, comp:lite, type:performance, TFLiteGpuDelegate, Android, TF 2.16"
Immediate Assistance Required: Issue with Converting Keras Model to TFLite,"### 1. System information

- OS Platform and Distribution : Windows 10 / flutter 3.19.5 /dart 3.3.3 / python 3.12.4
- TensorFlow installation : pip package
- TensorFlow library : 2.16.1

### 2. Code
#### Option B: Paste your code here or provide a link to a custom end-to-end colab
import os
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_hub as hub
import warnings

# 경고 메시지 무시
warnings.filterwarnings(""ignore"", category=DeprecationWarning)
warnings.filterwarnings(""ignore"", category=FutureWarning)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TensorFlow의 INFO 및 WARNING 메시지 무시

# 데이터 로드 및 전처리 함수
def load_data():
    def preprocess(image, label):
        try:
            image = tf.image.resize(image, (224, 224))
            image = image / 255.0  # 정규화
        except Exception as e:
            print(f""Error in preprocessing: {e}"")
            return None, None
        return image, label

    train_dataset, test_dataset = tfds.load('cifar10', split=['train', 'test'], as_supervised=True)
    train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    train_dataset = train_dataset.filter(lambda x, y: x is not None)  # 예외 처리된 데이터 제외
    train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)

    test_dataset = test_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    test_dataset = test_dataset.filter(lambda x, y: x is not None)  # 예외 처리된 데이터 제외
    test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)

    return train_dataset, test_dataset

# 모델 클래스 정의
class EfficientNetLiteModel(tf.keras.layers.Layer):
    def __init__(self):
        super(EfficientNetLiteModel, self).__init__()
        effnet_lite_url = ""https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2""
        self.effnet_lite_layer = hub.KerasLayer(effnet_lite_url, trainable=False)
        self.dense_layer = tf.keras.layers.Dense(10)  # softmax 활성화 함수 제거

    def call(self, inputs):
        outputs = self.effnet_lite_layer(inputs)
        outputs = self.dense_layer(outputs)
        return outputs

# 모델 생성 함수
def build_model():
    model = tf.keras.Sequential([
        EfficientNetLiteModel()
    ])
    model.build(input_shape=(None, 224, 224, 3))
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model


# 모델 학습 함수
def train_model(model_name):
    train_dataset, test_dataset = load_data()
    model = build_model()

    model.fit(train_dataset, epochs=3, validation_data=test_dataset)

    # 여러 최적화 옵션으로 모델 저장
    export_quantized_model(model, model_name, tf.lite.Optimize.DEFAULT, ""default"")
    export_quantized_model(model, model_name, tf.lite.Optimize.OPTIMIZE_FOR_LATENCY, ""latency"")
    export_quantized_model(model, model_name, tf.lite.Optimize.OPTIMIZE_FOR_SIZE, ""size"")

    # Keras 모델 저장
    model.save(f""{model_name}.h5"")

# 정량화된 TFLite 모델 내보내기
def export_quantized_model(model, model_name, optimization, suffix):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [optimization]
    
    # 입력 형상 지정을 위한 대표 데이터셋 생성
    def representative_dataset():
        for images, _ in load_data()[0].take(100):
            data = tf.cast(images, tf.float32)
            yield [data]

    converter.representative_dataset = representative_dataset

    # 대표 데이터셋 생성자
    def representative_dataset_gen():
        for images, _ in load_data()[0].take(100):
            yield [images]
            
    converter.representative_dataset = representative_dataset_gen
    tflite_quant_model = converter.convert()

    tflite_model_file = f""{model_name}_quant_{suffix}.tflite""
    with open(tflite_model_file, 'wb') as f:
        f.write(tflite_quant_model)


# TFLite 모델 평가 함수
def evaluate_tflite_model(tflite_model_path, test_dataset):
    # TFLite 모델 로드
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
    interpreter.allocate_tensors()

    input_index = interpreter.get_input_details()[0]['index']
    output_index = interpreter.get_output_details()[0]['index']

    # 정확도 평가
    total_seen = 0
    num_correct = 0

    for images, labels in test_dataset:
        interpreter.set_tensor(input_index, images)
        interpreter.invoke()
        predictions = interpreter.get_tensor(output_index)

        predicted_labels = tf.argmax(predictions, axis=1)
        num_correct += tf.reduce_sum(tf.cast(predicted_labels == labels, tf.int32)).numpy()
        total_seen += images.shape[0]

    accuracy = num_correct / total_seen
    print(f""TFLite 모델 ({tflite_model_path}) 정확도: {accuracy:.4f}"")

# 메인 함수
def main():
    model_name = ""efficientnet-lite0""
    train_model(model_name)  # 여러 최적화된 모델 생성
    
    # 학습된 TFLite 모델 평가
    _, test_dataset = load_data()
    evaluate_tflite_model(f""{model_name}.tflite"", test_dataset)
    evaluate_tflite_model(f""{model_name}_quant_default.tflite"", test_dataset)
    evaluate_tflite_model(f""{model_name}_quant_latency.tflite"", test_dataset)
    evaluate_tflite_model(f""{model_name}_quant_size.tflite"", test_dataset)

if __name__ == ""__main__"":
    main()

### 3. Failure after conversion
2024-06-15 07:33:32.135707: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-15 07:33:33.475807: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From D:\getxgallery\myenv\Lib\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

Epoch 3/3
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 0s 324ms/step - accuracy: 0.9128 - loss: 0.25622024-06-15 08:03:13.894773: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
         [[{{node IteratorGetNext}}]]
2024-06-15 08:05:09.110888: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
         [[{{node IteratorGetNext}}]]
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 623s 398ms/step - accuracy: 0.9128 - loss: 0.2562 - val_accuracy: 0.8921 - val_loss: 0.3063
Traceback (most recent call last):
  File ""D:\getxgallery\model_training\gallery_model_training\test.py"", line 138, in <module>
    main()
  File ""D:\getxgallery\model_training\gallery_model_training\test.py"", line 128, in main
    train_model(model_name)  # 여러 최적화된 모델 생성
    ^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\model_training\gallery_model_training\test.py"", line 67, in train_model
    export_quantized_model(model, model_name, tf.lite.Optimize.DEFAULT, ""default"")
  File ""D:\getxgallery\model_training\gallery_model_training\test.py"", line 93, in export_quantized_model
    tflite_quant_model = converter.convert()
                         ^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1175, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1129, in _convert_and_export_metrics  
    result = convert_func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1641, in convert
    self._freeze_keras_model()
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
    ^^^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\lite.py"", line 1582, in _freeze_keras_model
    input_signature = _model_input_signature(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\getxgallery\myenv\Lib\site-packages\tensorflow\lite\python\tflite_keras_util.py"", line 84, in model_input_signature
    input_specs = model._get_save_spec(  # pylint: disable=protected-access
                  ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Sequential' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?",2024-06-15 01:14:41+00:00,"stat:awaiting response, type:bug, stale, comp:lite, TFLiteConverter, TF 2.16"
TFJS basic operations are extremely slow in comparison to Native JS,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tfjs

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was just shocked when I saw that even basic operations like passing an array to `tf.tensor` is 60% slower than allocating a `tf.buffer`, setting each value one by one and calling `toTensor()`. When it comes to `tf.randomNormal`, which I believe is called many many times, there is a major slow down, even though the same Box-Muller algorithm is used.

### Standalone code to reproduce the issue

```shell
<!DOCTYPE html>
<html>
<head>
    <title>Random Number Benchmark</title>
    <script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs""></script>
</head>
<body>
    <script>
        async function benchmarkTfjsRandomNormal(numSamples) {
            const start = performance.now();

            // Generate random floats using TensorFlow.js
            const randomFloats = tf.randomNormal([numSamples]);

            // Ensure the tensors are evaluated
            await randomFloats.data();

            const end = performance.now();
            tf.dispose([randomFloats]); // Clean up tensors
            return end - start;
        }

        function generateBoxMullerNormal() {
            let u = 0, v = 0;
            while (u === 0) u = Math.random(); // Converting [0,1) to (0,1)
            while (v === 0) v = Math.random();
            return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);
        }

        function benchmarkNativeRandomNormal(numSamples) {
            const start = performance.now();

            const randomFloats = new Float32Array(numSamples);
            for (let i = 0; i < numSamples; i++) {
                randomFloats[i] = generateBoxMullerNormal();
            }

            const end = performance.now();
            return end - start;
        }

        async function runBenchmarks() {
            const sampleSizes = [1000, 10000, 100000, 1000000];
            for (const numSamples of sampleSizes) {
                const tfjsTime = await benchmarkTfjsRandomNormal(numSamples);
                console.log(`TensorFlow.js (${numSamples} samples): ${tfjsTime.toFixed(2)} ms`);

                const nativeTime = benchmarkNativeRandomNormal(numSamples);
                console.log(`Native JavaScript (${numSamples} samples): ${nativeTime.toFixed(2)} ms`);

                const speedup = nativeTime / tfjsTime;
                console.log(`Speedup: ${speedup}`);
                console.log('');
            }
        }

        // Run the benchmarks and log the results
        runBenchmarks();
    </script>
</body>
</html>
```


### Relevant log output

```shell
TensorFlow.js (1000 samples): 11.00 ms
test.html:49 Native JavaScript (1000 samples): 0.40 ms
test.html:52 Speedup: 0.03636363690549677
test.html:53 
test.html:46 TensorFlow.js (10000 samples): 2.40 ms
test.html:49 Native JavaScript (10000 samples): 0.90 ms
test.html:52 Speedup: 0.3750000015522043
test.html:53 
test.html:46 TensorFlow.js (100000 samples): 5.90 ms
test.html:49 Native JavaScript (100000 samples): 5.20 ms
test.html:52 Speedup: 0.8813559292925052
test.html:53 
test.html:46 TensorFlow.js (1000000 samples): 64.80 ms
test.html:49 Native JavaScript (1000000 samples): 58.20 ms
test.html:52 Speedup: 0.8981481482120248
test.html:53 
test.html:46 TensorFlow.js (10000000 samples): 648.90 ms
test.html:49 Native JavaScript (10000000 samples): 591.00 ms
test.html:52 Speedup: 0.9107720758122536
```
",2024-06-22 23:51:03+00:00,"stat:awaiting response, stale, comp:apis, type:performance"
รายงานปัญหาระหว่างการแปลงโมเดลเป็น TFLite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2024-06-23 17:45:59+00:00,"stat:awaiting response, comp:lite, TFLiteConverter"
TensorFlow Lite Converter Issue,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2024-06-24 22:33:45+00:00,"comp:lite, TFLiteConverter"
No dashboards are active for the current data set.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.1.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA RTX

### Current behavior?

I am training a model for PPO training using RL with CARLA simulator. While training events files are generated but when I want to visualize the data the browser shows no data to show. I want to show the graph regarding reward and model training.

### Standalone code to reproduce the issue

```shell
#!/bin/env python
import gym
import macad_gym  # noqa F401
import argparse
import os
from pprint import pprint

import cv2
import ray
import ray.tune as tune
from gym.spaces import Box, Discrete
from macad_agents.rllib.env_wrappers import wrap_deepmind
from macad_agents.rllib.models import register_mnih15_net

from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy #0.8.5
from ray.rllib.models.catalog import ModelCatalog
from ray.rllib.models.preprocessors import Preprocessor
from ray.tune import register_env
import time
import tensorflow as tf
from tensorboardX import SummaryWriter

# from tensorflow.compat.v1 import ConfigProto
# from tensorflow.compat.v1 import InteractiveSession
# config = tf.ConfigProto()
# config.gpu_options.allow_growth = True
# session = InteractiveSession(config=config)
# config = tf.ConfigProto()
# config.gpu_options.per_process_gpu_memory_fraction = 0.7
# tf.keras.backend.set_session(tf.Session(config=config));

parser = argparse.ArgumentParser()
parser.add_argument(
    ""--env"",
    default=""PongNoFrameskip-v4"",
    help=""Name Gym env. Used only in debug mode. Default=PongNoFrameskip-v4"")
parser.add_argument(
    ""--disable-comet"",
    action=""store_true"",
    help=""Disables comet logging. Used for local smoke tests"")
parser.add_argument(
    ""--num-workers"",
    default=1, #2 #fix
    type=int,
    help=""Num workers (CPU cores) to use"")
parser.add_argument(
    ""--num-gpus"", default=1, type=int, help=""Number of gpus to use. Default=2"")
parser.add_argument(
    ""--sample-bs-per-worker"", #one iteration
    default=1024,
    type=int,
    help=""Number of samples in a batch per worker. Default=50"")
parser.add_argument(
    ""--train-bs"",
    default=128,
    type=int,
    help=""Train batch size. Use as per available GPU mem. Default=500"")
parser.add_argument(
    ""--envs-per-worker"",
    default=1,
    type=int,
    help=""Number of env instances per worker. Default=10"")
parser.add_argument(
    ""--notes"",
    default=None,
    help=""Custom experiment description to be added to comet logs"")
parser.add_argument(
    ""--model-arch"",
    default=""mnih15"",
    help=""Model architecture to use. Default=mnih15"")
parser.add_argument(
    ""--num-steps"",
    default=4000000,
    type=int,
    help=""Number of steps to train. Default=20M"")
parser.add_argument(
    ""--num-iters"",
    default=300,
    type=int,
    help=""Number of training iterations. Default=20"")
parser.add_argument(
    ""--log-graph"",
    action=""store_true"",
    help=""Write TF graph on Tensorboard for debugging"",default=True)
parser.add_argument(
    ""--num-framestack"",
    type=int,
    default=4,
    help=""Number of obs frames to stack"")
parser.add_argument(
    ""--debug"", action=""store_true"", help=""Run in debug-friendly mode"", default=False)
parser.add_argument(
    ""--redis-address"",
    default=None,
    help=""Address of ray head node. Be sure to start ray with""
    ""ray start --redis-address <...> --num-gpus<.> before running this script"")
parser.add_argument(
    ""--use-lstm"", action=""store_true"", help=""Append a LSTM cell to the model"",default=True)



args = parser.parse_args()

model_name = args.model_arch
if model_name == ""mnih15"":
    register_mnih15_net()  # Registers mnih15
else:
    print(""Unsupported model arch. Using default"")
    register_mnih15_net()
    model_name = ""mnih15""

# Used only in debug mode
env_name = ""HomoNcomIndePOIntrxMASS3CTWN3-v0""
env = gym.make(env_name)
# print (env.spec.max_episode_steps,""-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+"")
# env.spec.max_episode_steps=1024
# print (env.spec.max_episode_steps,""-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+"")

env_actor_configs = env.configs

num_framestack = args.num_framestack
# env_config[""env""][""render""] = False


def env_creator(env_config):
    
    import macad_gym
    env = gym.make(""HomoNcomIndePOIntrxMASS3CTWN3-v0"")

    # Apply wrappers to: convert to Grayscale, resize to 84 x 84,
    # stack frames & some more op
    env = wrap_deepmind(env, dim=84, num_framestack=num_framestack)
    return env


register_env(env_name, lambda config: env_creator(config))

# Placeholder to enable use of a custom pre-processor
class ImagePreproc(Preprocessor):
    def _init_shape(self, obs_space, options):
        self.shape = (84, 84, 3)  # Adjust third dim if stacking frames
        return self.shape

    def transform(self, observation):
        observation = cv2.resize(observation, (self.shape[0], self.shape[1]))
        # cv2.imshow(""observation"",observation)
        return observation
def transform(self, observation):
        observation = cv2.resize(observation, (self.shape[0], self.shape[1]))
        return observation

ModelCatalog.register_custom_preprocessor(""sq_im_84"", ImagePreproc)


if args.redis_address is not None:
    # num_gpus (& num_cpus) must not be provided when connecting to an
    # existing cluster
    ray.init(redis_address=args.redis_address,object_store_memory=10**10,log_to_driver=False)
else:
    ray.init(num_gpus=args.num_gpus,object_store_memory=10**10,log_to_driver=False)

config = {
    # Model and preprocessor options.
    ""model"": {
        ""custom_model"": model_name,
        ""custom_options"": {
            # Custom notes for the experiment
            ""notes"": {
                ""args"": vars(args)
            },
        },
        # NOTE:Wrappers are applied by RLlib if custom_preproc is NOT specified
        ""custom_preprocessor"": ""sq_im_84"",
        ""dim"": 84,
        ""free_log_std"": False,  # if args.discrete_actions else True,
        ""grayscale"": True,
        # conv_filters to be used with the custom CNN model.
        # ""conv_filters"": [[16, [4, 4], 2], [32, [3, 3], 2], [16, [3, 3], 2]]
    },
    # preproc_pref is ignored if custom_preproc is specified
    # ""preprocessor_pref"": ""deepmind"",

    # env_config to be passed to env_creator
    
    ""env_config"": env_actor_configs
}

def default_policy():
    env_actor_configs[""env""][""render""] = True

    config = {
    # Model and preprocessor options.
    ""model"": {
        ""custom_model"": model_name,
        ""custom_options"": {
            # Custom notes for the experiment
            ""notes"": {
                ""args"": vars(args)
            },
        },
        # NOTE:Wrappers are applied by RLlib if custom_preproc is NOT specified
        ""custom_preprocessor"": ""sq_im_84"",
        ""dim"": 84,
        ""free_log_std"": False,  # if args.discrete_actions else True,
        ""grayscale"": True,
        # conv_filters to be used with the custom CNN model.
        # ""conv_filters"": [[16, [4, 4], 2], [32, [3, 3], 2], [16, [3, 3], 2]]
    },


    # Should use a critic as a baseline (otherwise don't use value baseline;
    # required for using GAE).
    ""use_critic"": True,
    # If true, use the Generalized Advantage Estimator (GAE)
    # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
    ""use_gae"": True,
    # The GAE(lambda) parameter.
    ""lambda"": 1.0,
    # Initial coefficient for KL divergence.
    ""kl_coeff"": 0.3,
    # Size of batches collected from each worker.
    ""rollout_fragment_length"": 128,
    # Number of timesteps collected for each SGD round. This defines the size
    # of each SGD epoch.
    # ""train_batch_size"": 4000,
    # Total SGD batch size across all devices for SGD. This defines the
    # minibatch size within each epoch.
    ""sgd_minibatch_size"": 64,
    # Whether to shuffle sequences in the batch when training (recommended).
    ""shuffle_sequences"": True,
    # Number of SGD iterations in each outer loop (i.e., number of epochs to
    # execute per train batch).
    ""num_sgd_iter"": 8,
    # Stepsize of SGD.
    ""lr"": 5e-5,
    # Learning rate schedule.
    # ""lr_schedule"": None,
    # Share layers for value function. If you set this to True, it's important
    # to tune vf_loss_coeff.
    ""vf_share_layers"": False,
    # Coefficient of the value function loss. IMPORTANT: you must tune this if
    # you set vf_share_layers: True.
    ""vf_loss_coeff"": 1.0,
    # Coefficient of the entropy regularizer.
    ""entropy_coeff"": 0.1,
    # Decay schedule for the entropy regularizer.
    ""entropy_coeff_schedule"": None,
    # PPO clip parameter.
    ""clip_param"": 0.3,
    # Clip param for the value function. Note that this is sensitive to the
    # scale of the rewards. If your expected V is large, increase this.
    ""vf_clip_param"": 10.0,
    # If specified, clip the global norm of gradients by this amount.
    ""grad_clip"": None,
    # Target value for KL divergence.
    ""kl_target"": 0.03,
    # Whether to rollout ""complete_episodes"" or ""truncate_episodes"".
    ""batch_mode"": ""complete_episodes"",
    # Which observation filter to apply to the observation.
    ""observation_filter"": ""NoFilter"",
    # Uses the sync samples optimizer instead of the multi-gpu one. This is
    # usually slower, but you might want to try it if you run into issues with
    # the default optimizer.
    ""simple_optimizer"": False,
    # Use PyTorch as framework?
    ""use_pytorch"": False,

    # Discount factor of the MDP.
    ""gamma"": 0.99,
    # Number of steps after which the episode is forced to terminate. Defaults
    # to `env.spec.max_episode_steps` (if present) for Gym envs.
    ""horizon"": 512,
    # Calculate rewards but don't reset the environment when the horizon is
    # hit. This allows value estimation and RNN state to span across logical
    # episodes denoted by horizon. This only has an effect if horizon != inf.
    ""soft_horizon"": True,
    # Don't set 'done' at the end of the episode. Note that you still need to
    # set this if soft_horizon=True, unless your env is actually running
    # forever without returning done=True.
    ""no_done_at_end"": True,
    ""monitor"": True,




    # System params.
    # Should be divisible by num_envs_per_worker
    ""sample_batch_size"":
     args.sample_bs_per_worker,
    ""train_batch_size"":
    args.train_bs,
    # ""rollout_fragment_length"": 128,
    ""num_workers"":
    args.num_workers,
    # Number of environments to evaluate vectorwise per worker.
    ""num_envs_per_worker"":
    args.envs_per_worker,
    ""num_cpus_per_worker"":
    1,
    ""num_gpus_per_worker"":
    1,
    # ""eager_tracing"": True,

    # # Learning params.
    # ""grad_clip"":
    # 40.0,
    # ""clip_rewards"":
    # True,
    # either ""adam"" or ""rmsprop""
    ""opt_type"":
    ""adam"",
    # ""lr"":
    # 0.003,
    ""lr_schedule"": [
        [0, 0.0006],
        [20000000, 0.000000000001],  # Anneal linearly to 0 from start 2 end
    ],
    # rmsprop considered
    ""decay"":
    0.5,
    ""momentum"":
    0.0,
    ""epsilon"":
    0.1,
    # # balancing the three losses
    # ""vf_loss_coeff"":
    # 0.5,  # Baseline loss scaling
    # ""entropy_coeff"":
    # -0.01,

    # preproc_pref is ignored if custom_preproc is specified
    # ""preprocessor_pref"": ""deepmind"",
   # ""gamma"": 0.99,

    ""use_lstm"": args.use_lstm,
    # env_config to be passed to env_creator
    ""env"":{
        ""render"": True
    },
    # ""in_evaluation"": True,
    # ""evaluation_num_episodes"": 1,
    ""env_config"": env_actor_configs
    }






    # pprint (config)
    return (PPOTFPolicy, Box(0.0, 255.0, shape=(84, 84, 3)), Discrete(9),config)

# pprint (args.checkpoint_path)
# pprint(os.path.isfile(args.checkpoint_path))


if args.debug:
    # For checkpoint loading and retraining (not used in this script)
    experiment_spec = tune.Experiment(
        ""multi-carla/"" + args.model_arch,
        ""PPO"",
        # restore=args.checkpoint_path,
        # timesteps_total is init with None (not 0) which causes issue
        # stop={""timesteps_total"": args.num_steps},
        stop={""timesteps_since_restore"": args.num_steps},
        config=config,
        # checkpoint_freq=1000, #1000
        # checkpoint_at_end=True,
        resources_per_trial={
            ""cpu"": 1,
            ""gpu"": 1
        })

    experiment_spec = tune.run_experiments({
            ""MA-Inde-PPO-SSUI3CCARLA"": {
                ""run"": ""PPO"",
                ""env"": env_name,
                ""stop"": {
                    
                    ""training_iteration"": args.num_iters,
                    ""timesteps_total"": args.num_steps,
                    ""episodes_total"": 1024,
                },
                # ""restore"":args.checkpoint_path,   
                ""config"": {

                    ""log_level"": ""DEBUG"",
                   # ""num_sgd_iter"": 10,  # Enables Experience Replay
                    ""multiagent"": {
                        ""policies"": {
                            id: default_policy()
                            for id in env_actor_configs[""actors""].keys()
                                # print()
                        },
                        ""policy_mapping_fn"":
                        tune.function(lambda agent_id: agent_id),
                        ""policies_to_train"": [""car2"",""car3""],
                    },
                    ""env_config"": env_actor_configs,
                    ""num_workers"": args.num_workers,
                    ""num_envs_per_worker"": args.envs_per_worker,
                    ""sample_batch_size"": args.sample_bs_per_worker,
                    ""train_batch_size"": args.train_bs,
                    ""horizon"": 512,

                },
                ""checkpoint_freq"": 5,
                ""checkpoint_at_end"": True,


            }
        })

  

else:

    experiment_spec = tune.Experiment(
        ""multi-carla/"" + args.model_arch,
        ""PPO"",
        stop={""timesteps_since_restore"": args.num_steps},
        config=config,
        resources_per_trial={
            ""cpu"": 1,
            ""gpu"": 1
        })

    experiment_spec = tune.run_experiments({
            ""MA-Inde-PPO-SSUI3CCARLA"": {
                ""run"": ""PPO"",
                ""env"": env_name,
                ""stop"": {
                    
                    ""training_iteration"": args.num_iters,
                    ""timesteps_total"": args.num_steps,
                    ""episodes_total"": 1024,
                    
                },

                ""config"": {

                    ""log_level"": ""DEBUG"",
                   # ""num_sgd_iter"": 10,  # Enables Experience Replay
                    ""multiagent"": {
                        ""policies"": {
                            id: default_policy()
                            for id in env_actor_configs[""actors""].keys()
                        },
                        ""policy_mapping_fn"":
                        tune.function(lambda agent_id: agent_id),
                        ""policies_to_train"": [""car2"",""car3""], 
                    },
                    ""env_config"": env_actor_configs,
                    ""num_workers"": args.num_workers,
                    ""num_envs_per_worker"": args.envs_per_worker,
                    ""sample_batch_size"": args.sample_bs_per_worker,
                    ""train_batch_size"": args.train_bs,
                    #""horizon"": 512, #yet to be fixed

                },
                ""checkpoint_freq"": 5,
                ""checkpoint_at_end"": True,


            }
        })


ray.shutdown()
```


### Relevant log output

_No response_",2024-07-17 08:41:16+00:00,"stat:awaiting response, type:support, TF 2.1"
Can not provide a sparse tensor to calculate ctc loss in TnesorFlow2,"When I use TensorFlow2 to train a CTC model like this:

`ctc_loss = tf.nn.ctc_loss(labels=ys, logits=ctc_logits, label_length=ylen, logit_length=xlen, blank_index=-1)`

It's ok, but the train speed is very slow compared with TensorFlow1 environment with the same train batch size (ablout 2~3 times slower），what's more，the GPU memory usage increase so huge that the train batch size must be set to very small. So, I try to replace the input dense label with sparse tensor like this:

`ys_sparse = tf.sparse.from_dense(ys)`
`ctc_loss = tf.nn.ctc_loss(labels=ys_sparse, logits=ctc_logits, label_length=None, logit_length=xlen, blank_index=-1)`

The train speed is much faster and the GPU memory usage will significant decreased. But I find another problem, in this condition, the normal training bath size can only be set to 1, when the batch size increase to 2, the training network will deteriorate rapidly，and the output logits will become nan，the ctc_loss will become nan too，and the training can't continue anymore. What's wrong？Is there any solution？

![1722323201749](https://github.com/user-attachments/assets/e45ecce2-7583-46cf-85df-73ae4a2664ca)

![image](https://github.com/user-attachments/assets/8064c23b-43c4-40c2-a29e-1d7e398fb523)
",2024-07-30 07:10:50+00:00,"stat:awaiting response, type:support, stale, comp:ops"
"Regarding lstm's full integer quantisation, does it quantise its intermediate output? The lstm full integer quantisation I get has only one input and one output quantisation parameter (minus the arguments)","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
![Uploading 企业微信截图_20240808180440.png…]()

- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2024-08-08 10:05:27+00:00,"stat:awaiting response, type:support, stale, comp:lite, TFLiteConverter, 2.17"
AttributeError: 'Sequential' object has no attribute '_get_save_spec',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",2024-08-11 10:55:46+00:00,"stat:awaiting response, stale, comp:lite, TFLiteConverter"
EffienceNet .keras model with base model trainable has poor performance in TensorFlow 2.17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Debian 

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am training an EfficientNet model with a custom head using TensorFlow and Keras, saving the model to a `.keras` format. If the base model `trainable` flag is set to False, such that I only train the head, then when I later load the `.keras` model and evaluate it on a dataset, I get the expected good performance. When I set the trainable flag to True and train a model (which converges well), then when I later load the model and evaluate it on the same dataset the performance has degraded significantly. (I am evaluating the model on the same dataset using the code both at the end of training, and later on in a separate notebook. It is in this separate notebook where the performance is bad, where again the same dataset is being used and the same code is being used in both evaluation places.)

Saving to a `.h5` model does not have this issue, and the performance of the saved model is good. I have spent the day trying different `trainable` and `training` flag values in various places to no improvement, thinking originally that it was something to do with the BatchNorm layers in the model. Recompiling the model has not helped.

When I switch back to an older TensorFlow version (2.15.0.post1) I do not see this issue. Both the trained `.keras` and `.h5` models perform well when later loaded and evaluated on my dataset of interest.

This seems like a bug to me, though I also acknowledge that perhaps I have missed something in the TF updates. I have searched the TensorFlow API docs for the various methods to no success. If it is the latter I would be very grateful for any advice, thank you.

### Standalone code to reproduce the issue

```shell
Code is relatively straightforward, I will provide some simple snippets here, though I cannot provide the full files.

Training code:

    model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=args.epochs,
        callbacks=callbacks,
        verbose=2
    )

Model:

        base_model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))
        base_model.trainable = True

        # Add a fully connected dense layer
        x = tf.keras.layers.Dense(512, activation='relu')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(rate=0.5)(x)
        
        # Another fully connected dense layer
        x = tf.keras.layers.Dense(256, activation='relu')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(rate=0.5)(x)

        x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)

        # Create a new model with the modified head
        model = tf.keras.Model(inputs=base_model.input, outputs=x)

Evaluation code:

model = tf.keras.models.load_model('./models/quickevaltest_oldertfversion_img96_ep10_bs32_lr0.0001_20240820.keras')

    for batch in tqdm.tqdm(imgs):
        predictions.append(np.array(model.predict(batch, verbose=0))) # model.call(batch) or model(call) is faster (actually maybe not) but predict is a little easier to work with
```


### Relevant log output

_No response_",2024-08-21 00:45:32+00:00,"stat:awaiting response, type:bug, comp:keras, 2.17"
TextVectorization returns 'int64' vs 'float32' in TF 2.7 / nightly + Training simple unigram/bigram models much slower than in 2.15,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.7 / nightly

### Custom code

No

### OS platform and distribution

Ubuntu (Colab) 22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2  

### GPU model and memory

Nvidia T4

### Current behavior?

Hi there,

I'm running into issues when proofreading some teaching materials based on Chollet's *Deep Learning With Python*. 

Running [this Colab notebook](https://drive.google.com/file/d/130cVhEsJT6J-z6f180I3S6DbOb5W8kAa/view?usp=sharing), (which is chapter 11, first part, based on [his repo](https://github.com/fchollet/deep-learning-with-python-notebooks), but fleshed out for teaching, this should be runnable out of the box) is *much* slower in the current versions of TensorFlow (2.17, or nightly), than in 2.15. I am unsure as to where this comes from, as the nets used in this are just fully-connected layers on unigrams/bigrams (XLA compilation seems to play a role, although that seems a lot). The slowness is particularly striking during the `evaluate()` step, where the XLA acceleration happening after the second epoch of training does not happen. I'm wondering if there's something obvious that's wrong in the code, or in the update...

Also, I notice that the `TextVectorization` layer seems to be returning `int64` in TF 2.17 and nightly, as opposed to `float32` previously. I did not find any mention of this in the documentation. Is this the desired behaviour?

### Standalone code to reproduce the issue

```shell
https://drive.google.com/file/d/130cVhEsJT6J-z6f180I3S6DbOb5W8kAa/view?usp=sharing
```


### Relevant log output

_No response_",2024-09-09 17:47:19+00:00,"stat:awaiting response, comp:keras, type:performance, 2.17"
